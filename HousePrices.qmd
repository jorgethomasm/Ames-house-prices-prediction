---
title: "Prediction of House Prices in Ames"
subtitle: "A commented and visual walk-through advance regression techniques." # only for html output
author: "Jorge A. Thomas"
date: "`r Sys.Date()`"
format:    
    html:
      self-contained: true
      code-fold: true
      df-print: tibble
      code-summary: "Show the code"
      grid: 
        margin-width: 350px
execute: 
  echo: fenced
reference-location: margin # margin
citation-location: document
bibliography: HousePrices.bib
# Template: https://quarto-dev.github.io/quarto-gallery/page-layout/tufte.html
---

```{r}
#| label: setup
#| message: false
#| echo: false

library(tidyverse) # ETL and EDA tools
source("./tools/jthomfuncs.r")
theme_set(jthomggtheme)
```

::: {#fig-intro layout-ncol=2}
![Houses](./imgs/ames_gai_16x9.jpeg){width=100%}

![Location](./imgs/Ames_map.png){width=100%}

Ames, Iowa - USA.
:::

# Introduction

In this classic Kaggle dataset [@house-prices-advanced-regression-techniques] you'll follow my workflow developing pipelines on the ETL phase of the DS cycle, as well as a tidy approach on the EDA substantialy supported on Visual Analytics. I'll be feeding some tips on the right margin along the analysis.

The Ames House dataset was prepared to improve on the older classic Boston Houses dataset and, given its open competition nature, it comprises two files:

1. One file contains features and labels or what's the same predictors and responses. Here this is the *training dataset*. It's all I have to split the ***Data Budget***.

2. Another file contains only features (predictors) to make predictions for the final submissions. Here this is the *test dataset*.

The goal of the competition is to achieve minimum RMSE.

I will keep updating this notebook, making improvements and reporting the rank obtained with my submitted predictions.

# PHASE I: ETL / EDA

## Extract Data

I checked beforehands that there are no missing values, here `NA`, in the target variable `SalePrice`. 
Therefore, I will write a pipeline to read and concatenate both datasets (bind rows), adding and extra column `dataset` to label as "train" and "test" for further easy splitting[^1] (subset or filter).

[^1]: The whole Feature Transformation pipeline most be always the same for all predictors in both datasets.

```{r}
#| label: Load Data
#| warning: false

ames_train_raw <- read_csv("./data/raw/train.csv") # Train, validattion and test dataset
print("Dimensions of training dataset")
dim(ames_train_raw)

ames_test_raw <- read_csv("./data/raw/test.csv")  # Features for submission dataset
print("Dimensions of test dataset containing only Feats.")
dim(ames_test_raw) 

# Add Target column with NA so both DFs can be concatenated:.id
ames_test_raw <- 
  ames_test_raw |> 
    mutate(SalePrice = NA)

# Binding and adding identifier column "dataset" 
ames_all <- 
  bind_rows(list(train = ames_train_raw, test = ames_test_raw), .id = "dataset")

print("Available variables:")
names(ames_all)
```

In order to organise my **Data Budget** I count on the **train dataset** with 1460 Observations of 79 predictors (one column is only an `Id` per house) and the Response, which is the `SalePrice` of the houses at the end of the list above.

Note that `Id` and the just added `dataset` columns are not predictors. Hence, there are **79 features** to play  with. Also rememeber that the number of features for modelling will vary. Some will be discarded and some will be created along the analysis.

## Impute Missing Values

First things first, read the full [data description](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data)[^2]. There you'll find that for several columns, missing values `NA` means actually *"None"*. The physical absence of a determined feature in a house is a category exposing the lack of such quality that can have a significant impact on the response `SalePrice`. Later it is most probably that binary features indicating the existance of some installations should be created.

Therefore, I will fill the empty `NA` fields of the indicated columns of both datasets with the string *"None"*.

[^2]: **Print a copy**, make notes and study it. This is the first step to get into domain knowledge, in this case Real Estate, for later *Feature Engineering*.

```{r}
#| label: easy replace_na

cols_NA_to_none <- list(
  Alley = "None",
  BsmtQual = "None", BsmtCond = "None", BsmtExposure = "None", BsmtFinType1 = "None", BsmtFinType2 = "None", 
  FireplaceQu = "None", 
  GarageType = "None", GarageFinish = "None", GarageQual = "None", GarageCond = "None", 
  PoolQC = "None",
  Fence = "None", 
  MiscFeature = "None")

ames_all <- ames_all |>
  replace_na(cols_NA_to_none)   
```

One of the early and recurrent steps of the EDA is to check the **completeness of data**. Let's search for missing values, after filling indicated fields[^3]. 
For this case I wrote the function `count_na()` that generates the table displayed on the right margin.

[^3]: **Write a function to quantify missing values** depending on the language and labelling system used. 

```{r}
#| label: Counting NAs
#| code-fold: false
#| column: margin

# Remaining NA count leaving out target SalePrice
ames_all |> 
  select(-SalePrice) |>
  count_na() |>  
  knitr::kable(caption = 'Ames dataset')
```

I'll fill the remaining missing values as follow:

- The `LotFrontage` column refers to the linear feet of street connected to the house. This feature is not well documented and the missing percentage related to other variables makes it not only unreliable, but very low in variance. Therefore, I will delete the feature.

- Everytime a Garage doesn't exist, i.e., `GarageType = "None"`, there is the corresponding missing value in Garage Year Built `GarageYrBlt = NA`. I will **engineer a new feature** called `GarageNew` with three ordinal categories: None, No, Yes. This based on the delta of `YearBuilt - GarageYrBlt`; I expect that given the house price, the algorithm will learn that "None" is worse than "No" and so on. Then I will remove the `GarageYrBlt` predictor.

- For the rest of variables with a 1 % or less missing values `NA`, I'll calculate the **median** (if numerical) and the **mode** (if string) in order to fill them with it.

Here's my pipeline for the whole dataset:

```{r}
#| label: terminating NAs

# "replace_na_with_median" is a custom function

ames_all <- ames_all |>
  mutate(LotFrontage = NULL) |> # Removing LotFrontage
  mutate(GarageNew = if_else(YearBuilt - GarageYrBlt > 0, "Yes", "No")) |> # New Feat.
  replace_na(list(GarageNew = "None")) |>
  mutate(GarageNew = factor(GarageNew, levels = c("None", "No", "Yes"))) |> # 3 levels
  mutate(GarageYrBlt = NULL) |> # Removing old Feat.
  mutate_if(is.numeric, replace_na_with_median)
```

Let's get a list with the mode of each remaining columns containing missing values `NA`.

```{r }
#| label: Mode for NAs

# Get the mode for reamaining columns with NAs:
# "find_mode()" is a custom function.

# Good, old-fashioned code: apply(ames_all, 2, find_mode)

list_na_mode <- ames_all |> 
  select(MasVnrType, MasVnrArea, MSZoning, Electrical, Utilities,	BsmtFullBath,	BsmtHalfBath,	Functional,	Exterior1st,	Exterior2nd,	BsmtFinSF1,	BsmtFinSF2,	BsmtUnfSF,	TotalBsmtSF,	KitchenQual,GarageCars,	GarageArea,	SaleType) |>
  map(find_mode)

# map returns a list 

# Replace with the created named-lists
ames_all <- ames_all |>
  replace_na(list_na_mode) 

# Sanity check of missing values:
print("Full Ames dataset (train and test)")
ames_all |>
  select(-SalePrice) |>
  count_na()
```

**"The data is complete!"** Let's think about predictors.

## Feature Engineering

I have already started with the creation of a new predictor in the previous step, this to substitute the problematic variable `GarageYrBlt` that had a lot of missing values.

Feature creation and transformations are not sequential, i.e., this is not the only step where *Feature Engineering* is applied. 
Think about the next *modelling phase*, where you want to reduce features, e.g., where new Principal Components are the new aggregators of original features.

In this part I will establish which variables are categorical and numerical; this is not always as obvious as it seems[^4].

[^4]: Integer variables like calendar *Years* can be treated as categories for the analysis.

The first easy thing that comes to mind is to add the **total area of the property** as a new column named `TotAreaSF`; SF means *Squared Feet*.

After that, I check the [data documentation](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data) to see which variables can be thought as categoricals (factors); this can be tedeous if Real Estate is not your domain of expertise. 

After 1.5 hours of reading, googling, understanding other notebooks and deliberating (yes, it was tedeous), I can summarise four types of ways to engineer new features depending on the nature of the variables:

1. Aggregation to a total of **continous-numerical** surface area (Square Feet), e.g. new `TotInteriorArea` Feat. summing basement and floors, etc. Same for a new `TotExteriorArea`. Then deleting the addends variables.

2. Aggregation to a total of **numerical counts** of an available installation of a determined type, e.g. new `TotBaths`. Then deleting the addends.

3. Categorisation with ascending quality levels of the originally numerical variables `OverallQual` and `OverallCond`. 

4. Creation of new **binary** features that state if a determined installation exists or doesn't.

Here's the code that complies with the four previous steps:

```{r}
#| label: Feature Engineering

# --- Total Aggregations ----
ames_all <- ames_all |>

  # Continuous numerical ----
  # NEW TotalInteriorArea
  mutate(TotInteriorArea = BsmtFinSF1 + BsmtFinSF2 + `1stFlrSF` + `2ndFlrSF`) |>
  select(-c(BsmtFinSF1, BsmtFinSF2, `1stFlrSF`))|> # Bye bye...

  # NEW HouseAge
  # mutate(HouseAge = YrSold - YearBuilt) |>  # Same as YearBuilt

  # NEW TotalExteriorArea
  mutate(TotExteriorArea = WoodDeckSF + OpenPorchSF + EnclosedPorch + `3SsnPorch` + ScreenPorch) |>
  select(-c(WoodDeckSF, OpenPorchSF, EnclosedPorch, `3SsnPorch`, ScreenPorch)) |># Bye bye...

  # Counting numerical ----
  # NEW TotalBaths
  mutate(TotBaths = BsmtFullBath + 0.5 * BsmtHalfBath + FullBath + 0.5 * HalfBath) |> 
  select(-c(BsmtFullBath, BsmtHalfBath, FullBath, HalfBath)) # Bye bye...


# ---- Ordered Categorical Features ----
ames_all <- ames_all |>
  mutate(OverallQual = factor(OverallQual)) |>
  mutate(OverallCond = factor(OverallCond))  # Check level orders 1, ..., 10 (BEST)


# ---- Binary Features ----
# Does the installation or Feature exist? 

ames_all <- ames_all |>
  # CentralAir
  mutate(CentralAir = if_else(CentralAir == "N", "No", "Yes")) |>
  mutate(CentralAir = fct_relevel(CentralAir, "No", "Yes"))|>

  # NEW IsRemodelled (it will replace YearRemodAdd)
  mutate(IsRemodelled = if_else(YearRemodAdd - YearBuilt > 0, "Yes", "No")) |>
  mutate(IsRemodelled = factor(IsRemodelled, levels = c("No", "Yes"))) |>
  select(-YearRemodAdd) |> # Bye YearRemodAdd

  # NEW ExistPool  
  # mutate(ExistPool = if_else(PoolArea == 0, "No", "Yes")) |>
  # mutate(ExistPool = fct_relevel(ExistPool, "No", "Yes"))|>
  # a dummy of PoolQC will make this Feat. redundant!

  # NEW ExistGarage
  mutate(ExistGarage = if_else(GarageArea == 0, "No", "Yes")) |>
  mutate(ExistGarage = fct_relevel(ExistGarage, "No", "Yes"))|>

  # NEW ExistBsmt
  mutate(ExistBsmt = if_else(TotalBsmtSF == 0, "No", "Yes")) |>
  mutate(ExistBsmt = fct_relevel(ExistBsmt, "No", "Yes"))|>

  # NEW Exist2ndFloor
  mutate(Exist2ndFloor = if_else(`2ndFlrSF` == 0, "No", "Yes")) |>
  select(-`2ndFlrSF`) |> # Bye, bye... 
  mutate(Exist2ndFloor = fct_relevel(Exist2ndFloor, "No", "Yes")) |>

  # NEW ExistFireplace
  mutate(ExistFireplace = if_else(Fireplaces == 0, "No", "Yes")) |>
  mutate(ExistFireplace = fct_relevel(ExistFireplace, "No", "Yes")) |>

  # NEW!!! Exist MasVnrArea: Masonry veneer area
  mutate(ExistMasVeneer = if_else(MasVnrArea > 0, "Yes", "No")) |>
  mutate(ExistMasVeneer = factor(ExistMasVeneer, levels = c("No", "Yes")))
 

# ---- The rest are factors ----

# I'm going to assume that the rest are Categorical variables 
ames_all <- ames_all |>
  mutate(across(where(is.character), factor)) |>
  mutate(YrSold = factor(YrSold))  # Convert to factor


# ---- Numerical Features ----

# BedroomabvGr (count) left numerical 
# Kitchens (count) left numerical 
# TotRmsAbvGrd (count) left numerical
# Fireplaces (count) left numerical
# GarageCars (count) left numerical
# GarageArea (continous) left numerical 
# MiscVal left numerical 
# MasVnrArea (continous) left numeric


# ---- Remove 'LowQualFinSF' ----

# Low quality finished square feet (all floors)
# Most of the values are 0 -> almost 0 variance. 
ames_all$LowQualFinSF <- NULL
ames_all$PoolArea <- NULL
```

Note that I replaced `YearRemodAdd` to a simpler binary `IsRemodelled` variable, similar to what I did with the new `GarageNew` column.

OK! Let's move on. 

### Searching for simple correlations

Although **nothing is linear**, is good to have an idea of simple correlations between all variables; it gives me an idea of what kind of plots to make. After going through the previous steps, it's obvious that there are correlations. Think about the number of cars that fit in a garage `GarageCars` and the area of the garage `GarageArea`.

```{r}
#| label: fig-num-corr
#| fig-width: 8
#| fig-height: 8
#| fig-cap: Pearson's correlation of numeric features and response. High multicollinearity.
#| warning: false
#| message: false

library(corrplot)

ames_all |>
  filter(dataset == "train") |>
  select(-c(dataset, Id)) |>
  select(where(is.numeric)) |>  
  cor(method = "pearson") |>
  corrplot(type = "lower", method = "circle", insig = 'blank', order = "hclust", diag = TRUE,
  tl.col="black", tl.cex = 0.8, addCoef.col = 'black', number.cex = 0.6)

```

According to the matrix above, `GrLivArea` has the strongest linear relationship with the target `SalePrice`, followed by `TotInteriorArea`, this given the collinearity with `GrLivArea`, and then `GarageArea` or `GarageCars`, both heavily correlated too, like commented before. 

I'm leaving blank, i.e., without a circle, the insignificant correlations for better visualisation and focus. Now I'm checking the categorical variables and the response.

```{r}
#| label: fig-cat-corr
#| fig-width: 10
#| fig-height: 10
#| fig-cap: Pearson's correlation of categorical features and response.
#| warning: false
#| message: false

ames_all |>
  filter(dataset == "train") |>
  select(-dataset) |>
  select(where(is.factor) | contains("Price")) |>  
  mutate_if(is.factor, as.integer) |> 
  cor(method = "pearson") |>
  corrplot(type = "lower", order="hclust", diag = TRUE, insig = 'blank',
  tl.col="black", tl.cex = 0.8, number.cex = 0.6)

```

The best linear association with `SalePrice` is given by `OverallQual` factor, followed by everything else that has to do with *quality* ratings. 

Here, the highest collinearity is showed between `PoolQC` and the new `ExistPool` variables. This is because they share a majority ammount of "None" and "No" values respectivelly in the exact same positions, therefore making the information redundant. In case of one-hot enconding there would be two repeated colums.

Overall, one can see clearly the **multicollinearity** clusters, which suggest the use of advance **Feature Selection** techniques.

### Visual Exploration

Now with the previous information in hand, I'm going to visualise separatelly the two most important quasi-linear relationsships: `GrLivArea` and `OverallQual` vs. `SalePrice`.

```{r}
#| label: fig-cont_linear
#| message: false
#| warning: false
#| column: body
#| fig-width: 6
#| fig-height: 6
#| fig-cap: 'GrvLivArea Vs. SalePrice - The most "linear" realationship between continuous variables. One can clearly notice the increase of variance as both, Living Area and Price increase, i.e., heteroskedasticity.'
#| cap-location: margin
#| fig-subcap: 
#|   - "Scatter Plot"
#|   - "Hexbin Plot"
#| layout-ncol: 2

ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = GrLivArea, y = SalePrice/1000)) +  
  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +  
  labs(x = expression("Ground Living Area ["~ft^2~"]"), 
       y = "Sale\nPrice\n['000 $]") +  
  geom_point(aes(color = OverallQual), alpha = 0.5) + theme(axis.text.y = element_blank()) +
  geom_smooth(method = "lm", formula =  y ~ splines::bs(x, 3), color = "black", size = 1.5, alpha = 0.5) +
  theme(legend.position = "bottom")

# install.packages("hexbin")
ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = GrLivArea, y = SalePrice/1000)) +  
  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +    
  geom_hex(bins = 35, colour = "seagreen") + 
  labs(x = expression("Ground Living Area ["~ft^2~"]")) +    
  theme(axis.title.y = element_blank(), legend.position = "bottom")

```

The hexbin plot reveals three cells of price and size where houses are more common in the training dataset. **There are more than 50 houses for each bin in the range of 130K, 140K and 180K USD** aproximately.

On the other hand, note how **problematic are the outliers** with the spline fit! This visual allows me to *remove outliers*, i.e., the biggest and cheap houses (right data points).

```{r}
#| label: fig-RemoveOutliers
#| fig-cap: Removing two outliers from the training dataset part the spline fit is dramatically improved!
#| column: margin

ames_all |>
  filter(dataset == "train") |>
  select(c(SalePrice, GrLivArea, OverallQual, YrSold)) |>
  filter(GrLivArea > 4500) |>
  glimpse()

idx_remove <- which(ames_all$dataset == "train" & ames_all$GrLivArea > 4500)

ames_all <-
  ames_all |> 
    filter(!row_number() %in% idx_remove)

ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = GrLivArea, y = SalePrice/1000)) +  
  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +  
  labs(x = expression("Ground Living Area ["~ft^2~"]"), 
       y = "Sale Price ['000 $]") +  
  geom_point(aes(color = OverallQual), alpha = 0.5) + theme(axis.text.y = element_blank()) +
  geom_smooth(method = "lm", 
              formula =  y ~ splines::bs(x, 3), 
              color = "black", 
              size = 1.5, 
              alpha = 0.5) +
  theme(legend.position = "none", axis.title.y = element_text(angle = 90))

```

The **two big houses above were like super offers!** Or did the housing bubble exploded?

In the same fashion, I want to see the effect of the factor `OverallQual`.

```{r}
#| label: fig-cat_linear
#| message: false
#| warning: false
#| fig-width: 7
#| fig-height: 8
#| fig-cap: 'OverallQual Vs. SalePrice - The most "linear" realationship with a categorical predictor. The raincloud plot shows how variances of factors are very different.'

# install.packages("ggrain")
library(ggrain)

ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = OverallQual, y = SalePrice/1000, colour = OverallQual, fill = OverallQual)) +     
  geom_rain(alpha = 0.3) +  
  scale_y_continuous(breaks = seq(0, 800, 100)) +  
  labs(x = "Overall\nQuality", y = "Sale Price ['000 $]") +
  coord_flip() + 
  theme(legend.position = "none")

```

#### I'm wondering in which price range is the most popular quality?

```{r}
#| label: fig-count_overallQual
#| cap-location: margin
#| fig-cap: 'Houses with mid-quality levels are most frequent in the train dataset. I will reduce the categories to nine (9), grouping or lumping 1 and 2 as "Other".'
#| column: margin
#| code-fold: false

ames_all |>
  filter(dataset == "train") |>
  count(OverallQual, name = "count") |>
  ggplot(aes(y = fct_reorder(OverallQual, count), x = count)) + 
  geom_col(aes(fill = OverallQual)) +
  geom_text(aes(label = count)) +
  labs(y = "Overall Quality") + 
  theme(legend.position = "none", axis.title.y = element_text(angle = 90))
```

It seems that most  popular houses, with an overall qualilty of 5 (mid-quality level), are the ones between 120K and 145K USD. Of course, this was back in the years of 2006 - 2010.

Now I'll group or lump into one category `OverallQual = 1` and `OverallQual = 2`.

```{r}
#| label: lumping

ames_all <- 
  ames_all |>
  mutate(OverallQual = fct_lump(OverallQual, n = 8)) |>
  mutate(OverallQual = fct_relevel(OverallQual, "Other")) 
  
```

#### How the sale price varies depending on the living area by house styles?

```{r}
#| label: fig-houseStyles
#| fig-column: page-right
#| fig-width: 8
#| fig-height: 8
#| fig-cap: "One and two story houses are the most common within the train dataset."

ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = GrLivArea, y = SalePrice/1000)) +  
  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +  
  labs(x = expression("Ground Living Area ["~ft^2~"]"), 
       y = "Sale\nPrice\n['000 $]") +  
  geom_point(aes(color = OverallQual), alpha = 0.3) +
  geom_smooth(method = "lm", 
              formula =  y ~ splines::bs(x, 3), 
              color = "black",
              se = FALSE,
              size = 0.7, 
              alpha = 0.5) +
  facet_wrap(~HouseStyle, nrow = 2) +
  theme(legend.position = "bottom")

```


### Analysis of response SalePrice

How well-behaved is the target?

```{r}
#| label: fig-Y
#| message: false
#| warning: false
#| column: body
#| fig-width: 6
#| fig-height: 6
#| fig-cap: "Analysis of the target variable. All the assumptions of OLS and linear regression are broken, as usual."
#| fig-subcap: 
#|   - "Histogram"
#|   - "Q-Q Plot"
#| layout-ncol: 2

ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = SalePrice/1000)) +  
  geom_histogram(bins = 50, color = "seagreen", fill = "seagreen", alpha = 0.5) +
  scale_x_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +  
  labs(x = "Sale Price ['000 $]")

ames_all |>
  filter(dataset == "train") |> 
  ggplot(aes(sample = SalePrice/1000)) + 
  stat_qq(color = "seagreen", alpha = 0.5) + 
  stat_qq_line(color = "seagreen") +  
  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +
  labs(y = "Sale\nPrice\n['000 $]",
       x = "Theoretical Quantiles (Norm. Dist.)")

```

Clearly, the target variable (Y) is not well-bahaved. It shows a log-normal distribution heavily skewed, to right of course. In order to achieve better predictions, a transformation might be needed.

#### Transformation of the response

For what I've seen, processes that have to do with *money*, e.g., sale prices, costs of products, salaries, they are most of the time **Log-normal**. 
Rarely there is something free, so you don't have zeros or with negativ value; think about the top 1% of reach people, or anything that becomes **premium** the higher the price. These are long and sparsed right tails.

The logarithmic transformation works when there are few large values, like in this case, there are only a few expensive hauses. The logarithm scale will shrink that right tail[^5], making the distribution more normal-like.

[^5]: Think of a **Logarithmic transformation** like pressing an **accordeon** only from your right arm.

Nevertheless, the moment the first data transformation is performed, we start to loose interpretability very quickly. Nowadays, the most accurate predictions need more than one transformation in order to work best[^6]. 

[^6]: Forget about linearity and interpretability if you want the most accurate results.  

Because we have powerful computers and libraries with written functions to perform variable transformations, I will use the Box-Cox transformation, which is a family of functions that includes the logarithmic one depending on the value of a parameter *lambda*. This parameter is automatically assessed.

::: column-margin
*Family Box-Cox transformations*:

<br/>

$y(\lambda) = \begin{cases} \frac{y^{-1}-1}{\lambda}, & \text{if } \lambda \neq 0 \\ \log y, & \text{if } \lambda = 0 \end{cases}$
:::

```{r}
#| label: fig-boxcox
#| fig-cap: "Best parameter lambda = -0.05050505. It is almost a pure logarithmic transformation."
#| column: margin
#| code-fold: false
#| warning: false
#| message: false

boxcox_trans <- MASS::boxcox(lm(SalePrice ~ 1, 
                                data = subset(ames_all, dataset == "train")), 
                             lambda = seq(-1, 1), 
                             plotit = TRUE)

boxcox_lambda <- as.numeric(boxcox_trans$x[which.max(boxcox_trans$y)])

# New transformed response variable
ames_all <- ames_all |>
  mutate(SalePrice_bc = (SalePrice^boxcox_lambda - 1) / boxcox_lambda)

```

The new mutated `SalePrice` should be better behaved...

```{r}
#| label: fig-Y-trans
#| message: false
#| warning: false
#| column: body
#| fig-width: 6
#| fig-height: 6
#| fig-cap: "The transformed response shows a more bell-like shape."
#| fig-subcap: 
#|   - "Histogram"
#|   - "Q-Q Plot"
#| layout-ncol: 2

ames_all |>
  filter(dataset == "train") |>  
  ggplot(aes(x = SalePrice_bc)) +  
  geom_histogram(bins = 50, color = "seagreen", fill = "seagreen", alpha = 0.5) +  
  labs(x = "Sale Price [Box-Cox]")

ames_all |>
  filter(dataset == "train") |>  
  ggplot(aes(sample = SalePrice_bc)) + 
  stat_qq(color = "seagreen", alpha = 0.5) + 
  stat_qq_line(color = "seagreen") +   
  labs(y = "Sale\nPrice\n[Box-Cox]", x = "Theoretical Quantiles (Norm. Dist.)")
```

... and it is, indeed. Well, except for the tails, now the transformed values are more normal-like at the core.

[**Do not include the transformation of the response as a step within the recipe.** It will cause an [error](https://github.com/tidymodels/recipes/issues/1033) with the *predict* function trying to locate `SalePrice` in the test dataset.]{.column-margin}

# PHASE 2: MODELLING

&emsp; *"This is where the fun begins."* 

One of the motivations to develop this project is because I'm switching to the **Tidymodels** framework. Therefore, it's time to load the library to train a simple reference model.

## Resampling Technique for Test Error Estimation
This is a critical step for final model selection. Depending on the chosen resample technique to estimate the Goodness-of-Prediction or test error, in this case metrics like RMSE can be slightly optimistic or conservative. Take a look at the pristine explanation by [@resamplingML].

- **Simple validation set?** 1 train fold and 1 test fold: NO, it's old and innacurate.
- **v-folds Cross-Validation?** Analysis (training folds), Assessment (validation folds) and test: it's *small data*, I can choose something even more powerful.
- **The Bootstrap?** YES.

By today's standards this dataset is small, therefore I will choose Bootstraping to keep the chosen samples the same size as the training set. The probability of an observation of being sample at least once is 0.63, therefore the complementary training set (around 37%) will become the *assessment set*. Although performance metrics will be slightly conservative or pesimistic, I prefer it.

I will generate 1000 bootstrap-samples. 

## Expending the data budget

Normally with v-fold CV I expend my data budget, but here doing resampling with replacement is kind of printing money... Here's the code to generate the bootstraps and therefore split the data budget:

```{r}
#| label: DataBudget
#| warning: false
#| fold: false

library(tidymodels)
tidymodels_prefer()
# conflicted::conflicts_prefer(scales::discard)

# Select the pre-processed training dataset
ames_train <- 
  ames_all |> 
  filter(dataset == "train")
 

# ---- v-folds Cross Validation ----

# v-folds CV repeated 10 times (v x 10 models)
set.seed(1982)
ames_folds <- vfold_cv(ames_train, v = 10, repeats = 10, strata = SalePrice_bc) # v = 10 folds are default

# ---- The Bootstrap ----

set.seed(1982)
ames_boots <- bootstraps(ames_train, times = 1000, strata = SalePrice_bc)
ames_boots  
```

## Base Worflows Set

My final objective is to stack (combine / ensemble) several models of different nature. However, for a first model I'd like to keep interpretability. 

### Linear Models

**Where to start?** In order to have a set of refence good models, I will proceed as follows:

- Cherry-picking predictors with linear influence from correlograms @fig-num-corr and @fig-cat-corr.
- Establish a ***workflow set*** of linear models:
  - Fit the simplest OLS univariate lm `GrLivArea` vs. `SalePrice`.
  - Cherry-pick predictors and fit a linear hyper-plane: multivariate lm.
  - Add some flexibility on top fitting a Spline GAM.

For the sake of RMSE metric comparison I will always scale numeric predictors and use the Box-Cox transformed response `SalePrice_bc`, this given the improvement shown in @fig-Y-trans. 

The following code shows preprocessing steps enclosed in a pipeline or ***recipe***. The list of available *step functions* can be found [here](https://recipes.tidymodels.org/reference/index.html).

```{r}
#| label: fig-cherry_picking_feats
#| fig-cap: "Correlated numerical predictors are skewed and contain zero values."

ames_train |>  
  select(c(GrLivArea, TotBaths, GarageArea, TotalBsmtSF, YearBuilt)) |> 
  pivot_longer(GrLivArea:YearBuilt) |>
  ggplot(aes(x = value)) +  
  geom_histogram(bins = 50, color = "lightgrey", fill = "lightgrey") +  
  labs(x = "") + 
  facet_wrap(~name, scales = "free")
```

Given the sensitivity to tails and outliers of linear models, remember @fig-cont_linear-1, it will be convienent to try to normalise the distribution of these predictors. However, Box-Cox transformations work only for positive data. The alternative will be the more general **Yeo-Johnson transformation**, which works for both, positive and negative values.

[Note that `SalePrice_bc` is the transformed outcome. Click [here](https://www.tmwr.org/recipes.html#skip-equals-true) for more info.]{.column-margin}

```{r}
#| label: cherry-picking_lm

# ---------- Preprocessing Pipelines ----------

# Uni-lm (simplest reasonable model)
simplest_rec <-
  recipe(SalePrice_bc ~ GrLivArea, data = ames_train) |>  
  step_YeoJohnson(GrLivArea) |>
  step_scale(all_numeric_predictors())

# Sanity check for warnings:
# simplest_rec |> prep() |>  bake(new_data = NULL)


# Multi-lm 
multi_lm_rec <-
  recipe(SalePrice_bc ~ GrLivArea + TotBaths + GarageArea + TotalBsmtSF + YearBuilt + OverallQual + KitchenQual + Foundation + ExistFireplace + HeatingQC, data = ames_train) |>  
  step_YeoJohnson(all_numeric_predictors()) |>
  step_scale(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors())

# Sanity check for warnings:
# multi_lm_rec |> prep() |> bake(new_data = NULL)

# GAM: Splines 
multi_gam_rec <-
  multi_lm_rec |>
  step_bs(all_numeric_predictors())  # GAM basis fun expansion.


# ---------- Model specification ----------
lm_spec <- 
  linear_reg() |>
  set_mode("regression") |>
  set_engine("lm")


# ---------- Set of Workflows ----------  
my_lm_recipes <- list(simplest_rec, multi_lm_rec, multi_gam_rec)
my_lms <- list(lm_spec)

# With a workflow set I can combine a list of pre-processors with a list of models:
ames_wfset <- workflow_set(my_lm_recipes, my_lms, cross = TRUE, case_weights = NULL)
ames_wfset


# ---------- Fit Bootstraps ----------
doParallel::registerDoParallel()
set.seed(1982)

ames_lms_res <- workflow_map(ames_wfset, "fit_resamples", resamples = ames_boots)

ames_lms_res |>  
  collect_metrics(summarize = TRUE) |>
  filter(.metric == "rmse")
```

OK! We can see the power of adding good predictors in the test RMSE estimation

My reference is the **linear GAM with Splines**, showing a mean RMSE = 0.0812. The result was transformed in the pipeline, therefore no units are displayed.

[I got slighlty better results using one-hot encoded categoricals. Moreover, applying the Yeo-Johnson transformation to numeric Feats. helped too! They were skewed, see @fig-cherry_picking_feats.]{.column-margin}

In order to see the feature importance, I'll train and extract the workflow of the simple, second model, i.e., the multivariate lm:

```{r}
#| label: lm_best_feats
#| column: margin

ames_lms_res |>
  extract_workflow("recipe_2_linear_reg") |>
  fit(ames_train) |>  
  tidy() |> 
  select(-statistic) |>
  arrange(-abs(estimate)) |>
  slice_head(n = 15)
```

Here is the code to extract the best model workflow, retrain with the whole training set, predicting and writing the submission .csv file:

```{r}
#| label: first_submission
#| warning: false

# Get best model and fit with whole training set
fit_lm_bsplines <- 
  extract_workflow(ames_lms_res, "recipe_3_linear_reg") |>
  fit(ames_train)

# Predicting on the test set
ames_test <- 
  ames_all |>
  select(-SalePrice) |>
  filter(dataset == "test")

res_1_bsplines <- 
  predict(fit_lm_bsplines, ames_test) |>
  mutate(SalePrice_pred = inv_boxcox(.pred, boxcox_lambda))  # orignal values in USD

# .csv for submission
sub_1 <- data.frame(Id = ames_test$Id, SalePrice = res_1_bsplines$SalePrice_pred)
# write.csv(sub_1, "./data/submissions/sub_1_splines.csv", quote = FALSE, row.names = FALSE)
```

#### First Kaggle Submission and Result

Ok! I got on the **top 59% (2825/4819)** cherry picking predictors and a GAM of splines. Not great as expected but an important referece.

Now that I have some reference, I'll proceed with more complexity and involving all predictors in order to extract more signal. 

## Advanced Workflows

Having high multicollinearty it's reasonable to think of some sort of penalisation or shrinkage method. Hence, I'll go with a regularised (shrinkage) Generalised Linear Model, [GLM](https://glmnet.stanford.edu/articles/glmnet.html). In this manner I can have a first look at feature importance.

I'll choose Elastic Net regression. Features must be scaled! I don't want to over-penalised non-scaled variables using shrinkage methods.

```{r}
#| label: Preprocessing

allfeats_rec <-
  recipe(SalePrice_bc ~ ., data = ames_train) |>
  update_role(Id, new_role = "Id variabe") |>
  update_role(SalePrice, new_role = "original outcome") |>
  update_role(dataset, new_role = "splitting variable") |>
  step_YeoJohnson(all_numeric_predictors()) |>
  step_scale(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors()) |>    
  step_zv(all_predictors())

# allfeats_rec |> prep() |> bake(new_data = NULL) 
# shrinkage_spline_rec <-
#   shrinkage_rec |>
#   step_bs(all_numeric_predictors())  

```

### Assessment with Elastic-Net

#### Specifications and Tuning

```{r}
#| label: Elastic_net-set-tune
#| eval: false	

library(usemodels)  # CheatSheets

# Model: Elastic Net -------------
glmnet_spec <-
  linear_reg(penalty = tune(), mixture = tune()) |> 
  set_engine("glmnet") |>
  set_mode("regression")
  # translate()

glmnet_wf <-
  workflow() |>
  add_recipe(allfeats_rec) |>
  add_model(glmnet_spec)

# penalty = lambda; mixture = alpha
glmnet_grid <- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20), 
                               mixture = c(0.05, 0.2, 0.4, 0.6, 0.8, 1))

set.seed(1982)
doParallel::registerDoParallel()

glmnet_tune <- tune_grid(glmnet_wf, resamples = ames_folds, grid = glmnet_grid)

# Explore Results 
show_best(glmnet_tune, metric = "rmse")

# Set best parameters
final_glmnet <- glmnet_wf |>
  finalize_workflow(select_best(glmnet_tune, metric = "rmse"))

final_glmnet

save(final_glmnet, file = "./models/final_models/final_glmnet.Rdata")
# final_glmnet <- load("./models/final_models/final_glmnet.Rdata")

```

Load Elastic-Net workflow with best parameters:

```{r}
#| label: final_glmnet
#| code-fold: false

final_glmnet <- load("./models/final_models/final_glmnet.Rdata")
final_glmnet

# glmnet_boot_fit <- 
#   final_glmnet |> 
#   fit_resamples(resamples = ames_boots)
  
# glmnet_boot_fit |>
#   collect_metrics(summarize = TRUE)

```

A very low penalty of $\lambda = 0.00264$ with a mixture of $\alpha = 0.6$ were the best parameters. What a pitty such a low penalty, almost a full linear model.

For curiosity, I fitted the 1000 bootstraps to the **elastic net model** (code above) obtaining a mean RMSE = 0.0652. Best so far...

### Assessment with Random Forest

#### Specifications and Tuning

```{r}
#| label: Random_Forest-set-tune
#| eval: false

rf_spec <-
  rand_forest(mtry = tune(), trees = 1000, min_n = tune()) |>
  set_engine("ranger") |>
  set_mode("regression")

rf_wf <- workflow() |>
  add_recipe(allfeats_rec) |>
  add_model(rf_spec)

# Hyperparameters tuning ---
set.seed(1982)
doParallel::registerDoParallel()

rf_tune <- tune_grid(rf_wf,                     
                    resamples = ames_folds, 
                    grid = 10)

# Explore Results
show_best(rf_tune, metric = "rmse")

#  26 minutes Aprox. ---

# Set best parameters
final_rf <- rf_wf |>
  finalize_workflow(select_best(rf_tune, metric = "rmse"))

final_rf

save(final_rf, file = "./models/final_models/final_rf.Rdata")
# final_rf <- load("./models/final_models/final_rf.Rdata")
```

Load Random Forest workflow with best parameters:

```{r}
#| label: final_rf
#| code-fold: false

final_rf <- load("./models/final_models/final_rf.Rdata")
final_rf

# Test error estimation using the bootstraps

# doParallel::registerDoParallel()
# set.seed(1982)

# final_rf |> 
#   fit_resamples(resamples = ames_boots) |>
#   collect_metrics(summarize = TRUE)

```

For curiosity, I fitted the 1000 bootstraps to the **best rf candidate** (code above) obtaining a mean RMSE = 0.0772


```{r}
#| label: GoF

# install.packages("doParallel")

# doParallel::registerDoParallel()
# set.seed(1982)

# ames_results <- workflow_map(ames_wfset, "fit_resamples", resamples = ames_boots)

# ames_results |>  
#   collect_metrics(summarize = TRUE) |>
#   filter(.metric == "rmse")


# save(ames_results, file = "./models/ames_results_wf.Rdata")

# ames_results <- load("./models/ames_results_wf.Rdata")

```


```{r}


# ames_wf <- workflow() |>
#   add_model(spec_lm) |>
#   add_formula(SalePrice ~ GrLivArea)

# # Fit models to bootstrapped sets

# trained_models <- fit_resamples(object = ames_wf, resamples = ames_boots)

# # See estimated performance
# trained_models |>  collect_metrics(summarize = TRUE) # TRUE gets Avg. performance 

```



## Arbitrary Full Width Content

::: column-page-right
*R is free software and comes with ABSOLUTELY NO WARRANTY.* You are welcome to redistribute it under the terms of the GNU General Public License versions 2 or 3. For more information about these matters see <https://www.gnu.org/licenses/>.
:::


# Sidenotes

cite R here [@R-base].

::: {.callout-note appearance="simple"}
This feature depends upon `link-citations` to locate and place references in the margin. This is enabled by default, but if you disable `link-citations` then references in the HTML output will be placed at the end of the output document as they normally are.
:::


::: {.callout-tip}
## Using R within Chunk Options
If you wish to use raw R expressions as part of the chunk options (like above), then you need to define those in the `tag=value` format within the curly brackets `{r label, tag=value}` instead of the `tag: value` YAML syntax on a new line starting with the hashpipe `#|`. The former approach is documented on [knitr's website](https://yihui.org/knitr/options/) while the latter is explained in [Quarto's documentation](https://quarto.org/docs/reference/cells/cells-knitr.html).
:::
