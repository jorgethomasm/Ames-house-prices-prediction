---
title: "Prediction of House Prices in Ames"
subtitle: "A commented and visual walk-through advance regression techniques." # only for html output
author: "Jorge A. Thomas"
date: "`r Sys.Date()`"
format:    
    html:
      self-contained: true
      code-fold: true
      df-print: tibble
      code-summary: "Show the code"
      grid: 
        margin-width: 350px
execute: 
  echo: fenced
reference-location: margin # margin
citation-location: document
bibliography: HousePrices.bib
# Template: https://quarto-dev.github.io/quarto-gallery/page-layout/tufte.html
---

```{r}
#| label: setup
#| message: false
#| echo: false

library(tidyverse) # ETL and EDA tools
source("./tools/jthomfuncs.r")
theme_set(jthomggtheme)
```

::: {#fig-intro layout-ncol=2}
![Houses](./imgs/ames_gai_16x9.jpeg){width=100%}

![Location](./imgs/Ames_map.png){width=100%}

Ames, Iowa - USA.
:::

# Introduction

In this classic Kaggle dataset [@house-prices-advanced-regression-techniques] you'll follow my workflow developing pipelines on the ETL phase of the DS cycle, as well as a tidy approach on the EDA substantialy supported on Visual Analytics. I'll be feeding some tips on the right margin along the analysis.

The Ames House dataset was prepared to improve on the older classic Boston Houses dataset and, given its open competition nature, it comprises two files:

1. One file contains features and labels or what's the same predictors and responses. Here this is the *training dataset*. It's all I have to split the ***Data Budget***.

2. Another file contains only features (predictors) to make predictions for the final submissions. Here this is the *test dataset*.

The goal of the competition is to achieve minimum RMSE.

I will keep updating this notebook, making improvements and reporting the rank obtained with my submitted predictions.

# PHASE I: ETL / EDA

## Extract Data

I checked beforehands that there are no missing values, here `NA`, in the target variable `SalePrice`. 
Therefore, I will write a pipeline to read and concatenate both datasets (bind rows), adding and extra column `dataset` to label as "train" and "test" for further easy splitting[^1] (subset or filter).

[^1]: The whole Feature Transformation pipeline most be always the same for all predictors in both datasets.

```{r}
#| label: Load Data
#| warning: false

ames_train_raw <- read_csv("./data/raw/train.csv") # Train, validattion and test dataset
print("Dimensions of training dataset")
dim(ames_train_raw)

ames_test_raw <- read_csv("./data/raw/test.csv")  # Features for submission dataset
print("Dimensions of test dataset containing only Feats.")
dim(ames_test_raw) 

# Add Target column with NA so both DFs can be concatenated:.id
ames_test_raw <- ames_test_raw |> mutate(SalePrice = NA)

# Binding and adding identifier column "dataset" 
ames_all <- bind_rows(list(train = ames_train_raw, test = ames_test_raw), .id = "dataset")

print("Available variables:")
names(ames_all)
```

In order to organise my **Data Budget** I count on the **train dataset** with 1460 Observations of 79 predictors (one column is only an `Id` per house) and the Response, which is the `SalePrice` of the houses at the end of the list above.

Note that `Id` and the just added `dataset` columns are not predictors. Hence, there are **79 features** to play  with. Also rememeber that the number of features for modelling will vary. Some will be discarded and some will be created along the analysis.

## Impute Missing Values

First things first, read the full [data description](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data)[^2]. There you'll find that for several columns, missing values `NA` means actually *"None"*. The physical absence of a determined feature in a house is a category exposing the lack of such quality that can have a significant impact on the response `SalePrice`. Later it is most probably that binary features indicating the existance of some installations should be created.

Therefore, I will fill the empty `NA` fields of the indicated columns of both datasets with the string *"None"*.

[^2]: **Print a copy**, make notes and study it. This is the first step to get into domain knowledge, in this case Real Estate, for later *Feature Engineering*.

```{r}
#| label: easy replace_na

cols_NA_to_none <- list(
  Alley = "None",
  BsmtQual = "None", BsmtCond = "None", BsmtExposure = "None", BsmtFinType1 = "None", BsmtFinType2 = "None", 
  FireplaceQu = "None", 
  GarageType = "None", GarageFinish = "None", GarageQual = "None", GarageCond = "None", 
  PoolQC = "None",
  Fence = "None", 
  MiscFeature = "None")

ames_all <- ames_all |>
  replace_na(cols_NA_to_none)   
```

One of the early and recurrent steps of the EDA is to check the **completeness of data**. Let's search for missing values, after filling indicated fields[^3]. 
For this case I wrote the function `count_na()` that generates the table displayed on the right margin.

[^3]: **Write a function to quantify missing values** depending on the language and labelling system used. 

```{r}
#| label: Counting NAs
#| code-fold: false
#| column: margin

# Remaining NA count leaving out target SalePrice
ames_all |> 
  select(-SalePrice) |>
  count_na() |>  
  knitr::kable(caption = 'Ames dataset')
```

I'll fill the remaining missing values as follow:

- The `LotFrontage` column refers to the linear feet of street connected to the house. This feature is not well documented and the missing percentage related to other variables makes it not only unreliable, but very low in variance. Therefore, I will delete the feature.

- Everytime a Garage doesn't exist, i.e., `GarageType = "None"`, there is the corresponding missing value in Garage Year Built `GarageYrBlt = NA`. I will **engineer a new feature** called `GarageNew` with three ordinal categories: None, No, Yes. This based on the delta of `YearBuilt - GarageYrBlt`; I expect that given the house price, the algorithm will learn that "None" is worse than "No" and so on. Then I will remove the `GarageYrBlt` predictor.

- For the rest of variables with a 1 % or less missing values `NA`, I'll calculate the **median** (if numerical) and the **mode** (if string) in order to fill them with it.

Here's my pipeline for the whole dataset:

```{r}
#| label: terminating NAs

# "replace_na_with_median" is a custom function

ames_all <- ames_all |>
  mutate(LotFrontage = NULL) |> # Removing LotFrontage
  mutate(GarageNew = if_else(YearBuilt - GarageYrBlt > 0, "Yes", "No")) |> # New Feat.
  replace_na(list(GarageNew = "None")) |>
  mutate(GarageNew = factor(GarageNew, levels = c("None", "No", "Yes"))) |> # 3 levels
  mutate(GarageYrBlt = NULL) |> # Removing old Feat.
  mutate_if(is.numeric, replace_na_with_median)
```

Let's get a list with the mode of each remaining columns containing missing values `NA`.

```{r }
#| label: Mode for NAs

# Get the mode for reamaining columns with NAs:
# "find_mode()" is a custom function.

# Good, old-fashioned code: apply(ames_all, 2, find_mode)

list_na_mode <- ames_all |> 
  select(MasVnrType, MasVnrArea, MSZoning, Electrical, Utilities,	BsmtFullBath,	BsmtHalfBath,	Functional,	Exterior1st,	Exterior2nd,	BsmtFinSF1,	BsmtFinSF2,	BsmtUnfSF,	TotalBsmtSF,	KitchenQual,GarageCars,	GarageArea,	SaleType) |>
  map(find_mode)

# map returns a list 

# Replace with the created named-lists
ames_all <- ames_all |>
  replace_na(list_na_mode) 

# Sanity check of missing values:
print("Full Ames dataset (train and test)")
ames_all |>
  select(-SalePrice) |>
  count_na()
```

**"The data is complete!"** Let's think about predictors.

## Feature Engineering

I have already started with the creation of a new predictor in the previous step, this to substitute the problematic variable `GarageYrBlt` that had a lot of missing values.

Feature creation and transformations are not sequential, i.e., this is not the only step where *Feature Engineering* is applied. 
Think about the next *modelling phase*, where you want to reduce features, e.g., where new Principal Components are the new aggregators of original features.

In this part I will establish which variables are categorical and numerical; this is not always as obvious as it seems[^4].

[^4]: Integer variables like calendar *Years* can be treated as categories for the analysis.

The first easy thing that comes to mind is to add the **total area of the property** as a new column named `TotAreaSF`; SF means *Squared Feet*.

After that, I check the [data documentation](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data) to see which variables can be thought as categoricals (factors); this can be tedeous if Real Estate is not your domain of expertise. 

After 1.5 hours of reading, googling, understanding other notebooks and deliberating (yes, it was tedeous), I can summarise four types of ways to engineer new features depending on the nature of the variables:

1. Aggregation to a total of **continous-numerical** surface area (Square Feet), e.g. new `TotInteriorArea` Feat. summing basement and floors, etc. Same for a new `TotExteriorArea`. Then deleting the addends variables.

2. Aggregation to a total of **numerical counts** of an available installation of a determined type, e.g. new `TotBaths`. Then deleting the addends.

3. Categorisation with ascending quality levels of the originally numerical variables `OverallQual` and `OverallCond`. 

4. Creation of new **binary** features that state if a determined installation exists or doesn't.

Here's the code that complies with the four previous steps:

```{r}
#| label: Feature Engineering

# --- Total Aggregations ----
ames_all <- ames_all |>

  # Continuous numerical ----
  # NEW TotalInteriorArea
  mutate(TotInteriorArea = BsmtFinSF1 + BsmtFinSF2 + `1stFlrSF` + `2ndFlrSF`) |>
  select(-c(BsmtFinSF1, BsmtFinSF2, `1stFlrSF`))|> # Bye bye...

  # NEW HouseAge
  # mutate(HouseAge = YrSold - YearBuilt) |>  # Same as YearBuilt

  # NEW TotalExteriorArea
  mutate(TotExteriorArea = WoodDeckSF + OpenPorchSF + EnclosedPorch + `3SsnPorch` + ScreenPorch) |>
  select(-c(WoodDeckSF, OpenPorchSF, EnclosedPorch, `3SsnPorch`, ScreenPorch)) |># Bye bye...

  # Counting numerical ----
  # NEW TotalBaths
  mutate(TotBaths = BsmtFullBath + 0.5 * BsmtHalfBath + FullBath + 0.5 * HalfBath) |> 
  select(-c(BsmtFullBath, BsmtHalfBath, FullBath, HalfBath)) # Bye bye...


# ---- Ordered Categorical Features ----
ames_all <- ames_all |>
  mutate(OverallQual = factor(OverallQual)) |>
  mutate(OverallCond = factor(OverallCond))  # Check level orders 1, ..., 10 (BEST)


# ---- Binary Features ----
# Does the installation or Feature exist? 

ames_all <- ames_all |>
  # CentralAir
  mutate(CentralAir = if_else(CentralAir == "N", "No", "Yes")) |>
  mutate(CentralAir = fct_relevel(CentralAir, "No", "Yes"))|>

  # NEW IsRemodelled (it will replace YearRemodAdd)
  mutate(IsRemodelled = if_else(YearRemodAdd - YearBuilt > 0, "Yes", "No")) |>
  mutate(IsRemodelled = factor(IsRemodelled, levels = c("No", "Yes"))) |>
  select(-YearRemodAdd) |> # Bye YearRemodAdd

  # NEW ExistPool  
  # mutate(ExistPool = if_else(PoolArea == 0, "No", "Yes")) |>
  # mutate(ExistPool = fct_relevel(ExistPool, "No", "Yes"))|>
  # a dummy of PoolQC will make this Feat. redundant!

  # NEW ExistGarage
  mutate(ExistGarage = if_else(GarageArea == 0, "No", "Yes")) |>
  mutate(ExistGarage = fct_relevel(ExistGarage, "No", "Yes"))|>

  # NEW ExistBsmt
  mutate(ExistBsmt = if_else(TotalBsmtSF == 0, "No", "Yes")) |>
  mutate(ExistBsmt = fct_relevel(ExistBsmt, "No", "Yes"))|>

  # NEW Exist2ndFloor
  mutate(Exist2ndFloor = if_else(`2ndFlrSF` == 0, "No", "Yes")) |>
  select(-`2ndFlrSF`) |> # Bye, bye... 
  mutate(Exist2ndFloor = fct_relevel(Exist2ndFloor, "No", "Yes")) |>

  # NEW ExistFireplace
  mutate(ExistFireplace = if_else(Fireplaces == 0, "No", "Yes")) |>
  mutate(ExistFireplace = fct_relevel(ExistFireplace, "No", "Yes")) |>

  # NEW!!! Exist MasVnrArea: Masonry veneer area
  mutate(ExistMasVeneer = if_else(MasVnrArea > 0, "Yes", "No")) |>
  mutate(ExistMasVeneer = factor(ExistMasVeneer, levels = c("No", "Yes")))
 

# ---- The rest are factors ----

# I'm going to assume that the rest are Categorical variables 
ames_all <- ames_all |>
  mutate(across(where(is.character), factor)) |>
  mutate(YrSold = factor(YrSold))  # Convert to factor


# ---- Numerical Features ----

# BedroomabvGr (count) left numerical 
# Kitchens (count) left numerical 
# TotRmsAbvGrd (count) left numerical
# Fireplaces (count) left numerical
# GarageCars (count) left numerical
# GarageArea (continous) left numerical 
# MiscVal left numerical 
# MasVnrArea (continous) left numeric


# ---- Remove 'LowQualFinSF' ----

# Low quality finished square feet (all floors)
# Most of the values are 0 -> almost 0 variance. 
ames_all$LowQualFinSF <- NULL
ames_all$PoolArea <- NULL
```

Note that I replaced `YearRemodAdd` to a simpler binary `IsRemodelled` variable, similar to what I did with the new `GarageNew` column.

OK! Let's move on. 

### Searching for simple correlations

Although **nothing is linear**, is good to have an idea of simple correlations between all variables; it gives me an idea of what kind of plots to make. After going through the previous steps, it's obvious that there are correlations. Think about the number of cars that fit in a garage `GarageCars` and the area of the garage `GarageArea`.

```{r}
#| label: fig-num-corr
#| fig-width: 8
#| fig-height: 8
#| fig-cap: Pearson's correlation of numeric features and response. High multicollinearity.
#| warning: false
#| message: false

library(corrplot)

ames_all |>
  filter(dataset == "train") |>
  select(-c(dataset, Id)) |>
  select(where(is.numeric)) |>  
  cor(method = "pearson") |>
  corrplot(type = "lower", method = "circle", insig = 'blank', order = "hclust", diag = TRUE,
  tl.col="black", tl.cex = 0.8, addCoef.col = 'black', number.cex = 0.6)

```

According to the matrix above, `GrLivArea` has the strongest linear relationship with the target `SalePrice`, followed by `TotInteriorArea`, this given the collinearity with `GrLivArea`, and then `GarageArea` or `GarageCars`, both heavily correlated too, like commented before. 

I'm leaving blank, i.e., without a circle, the insignificant correlations for better visualisation and focus. Now I'm checking the categorical variables and the response.

```{r}
#| label: fig-cat-corr
#| fig-width: 10
#| fig-height: 10
#| fig-cap: Pearson's correlation of categorical features and response.
#| warning: false
#| message: false

ames_all |>
  filter(dataset == "train") |>
  select(-dataset) |>
  select(where(is.factor) | contains("Price")) |>  
  mutate_if(is.factor, as.integer) |> 
  cor(method = "pearson") |>
  corrplot(type = "lower", order="hclust", diag = TRUE, insig = 'blank',
  tl.col="black", tl.cex = 0.8, number.cex = 0.6)

```

The best linear association with `SalePrice` is given by `OverallQual` factor, followed by everything else that has to do with *quality* ratings. 

Here, the highest collinearity is showed between `PoolQC` and the new `ExistPool` variables. This is because they share a majority ammount of "None" and "No" values respectivelly in the exact same positions, therefore making the information redundant. In case of one-hot enconding there would be two repeated colums.

Overall, one can see clearly the **multicollinearity** clusters, which suggest the use of advance **Feature Selection** techniques.

### Visual Exploration

Now with the previous information in hand, I'm going to visualise separatelly the two most important quasi-linear relationsships: `GrLivArea` and `OverallQual` vs. `SalePrice`.

```{r}
#| label: fig-cont_linear
#| message: false
#| warning: false
#| column: body
#| fig-width: 6
#| fig-height: 6
#| fig-cap: 'GrvLivArea Vs. SalePrice - The most "linear" realationship between continuous variables. One can clearly notice the increase of variance as both, Living Area and Price increase, i.e., heteroskedasticity.'
#| cap-location: margin
#| fig-subcap: 
#|   - "Scatter Plot"
#|   - "Hexbin Plot"
#| layout-ncol: 2

ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = GrLivArea, y = SalePrice/1000)) +  
  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +  
  labs(x = expression("Ground Living Area ["~ft^2~"]"), 
       y = "Sale\nPrice\n['000 $]") +  
  geom_point(aes(color = OverallQual), alpha = 0.5) + theme(axis.text.y = element_blank()) +
  geom_smooth(method = "lm", formula =  y ~ splines::bs(x, 3), color = "black", size = 1.5, alpha = 0.5) +
  theme(legend.position = "bottom")

# install.packages("hexbin")
ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = GrLivArea, y = SalePrice/1000)) +  
  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +    
  geom_hex(bins = 35, colour = "seagreen") + 
  labs(x = expression("Ground Living Area ["~ft^2~"]")) +    
  theme(axis.title.y = element_blank(), legend.position = "bottom")

```

The hexbin plot reveals three cells of price and size where houses are more common in the training dataset. **There are more than 50 houses for each bin in the range of 130K, 140K and 180K USD** aproximately.

On the other hand, note how **problematic are the outliers** with the spline fit! This visual allows me to *remove outliers*, i.e., the biggest and cheap houses (right data points).

```{r}
#| label: fig-RemoveOutliers
#| fig-cap: Removing three outliers the spline fit is dramatically improved!
#| column: margin

ames_all |>
  select(c(SalePrice, GrLivArea, OverallQual, YrSold)) |>
  filter(GrLivArea > 4500) |>
  glimpse()

ames_all <- ames_all |> filter(GrLivArea < 4500)

ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = GrLivArea, y = SalePrice/1000)) +  
  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +  
  labs(x = expression("Ground Living Area ["~ft^2~"]"), 
       y = "Sale Price ['000 $]") +  
  geom_point(aes(color = OverallQual), alpha = 0.5) + theme(axis.text.y = element_blank()) +
  geom_smooth(method = "lm", 
              formula =  y ~ splines::bs(x, 3), 
              color = "black", 
              size = 1.5, 
              alpha = 0.5) +
  theme(legend.position = "none", axis.title.y = element_text(angle = 90))

```

The **three big houses above were like super offers!** Or did the housing bubble exploded?

In the same fashion, I want to see the effect of the factor `OverallQual`.

```{r}
#| label: fig-cat_linear
#| message: false
#| warning: false
#| fig-width: 7
#| fig-height: 8
#| fig-cap: 'OverallQual Vs. SalePrice - The most "linear" realationship with a categorical predictor. The raincloud plot shows how variances of factors are very different.'

# install.packages("ggrain")
library(ggrain)

ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = OverallQual, y = SalePrice/1000, colour = OverallQual, fill = OverallQual)) +     
  geom_rain(alpha = 0.3) +  
  scale_y_continuous(breaks = seq(0, 800, 100)) +  
  labs(x = "Overall\nQuality", y = "Sale Price ['000 $]") +
  coord_flip() + 
  theme(legend.position = "none")

```

#### I'm wondering in which price range is the most popular quality?

```{r}
#| label: fig-count_overallQual
#| cap-location: margin
#| fig-cap: 'Houses with mid-quality levels are most frequent in the train dataset. I will reduce the categories to nine (9), grouping or lumping 1 and 2 as "Other".'
#| column: margin
#| code-fold: false

ames_all |>
  filter(dataset == "train") |>
  count(OverallQual, name = "count") |>
  ggplot(aes(y = fct_reorder(OverallQual, count), x = count)) + 
  geom_col(aes(fill = OverallQual)) +
  geom_text(aes(label = count)) +
  labs(y = "Overall Quality") + 
  theme(legend.position = "none", axis.title.y = element_text(angle = 90))
```

It seems that most  popular houses, with an overall qualilty of 5 (mid-quality level), are the ones between 120K and 145K USD. Of course, this was back in the years of 2006 - 2010.

Now I'll group or lump into one category `OverallQual = 1` and `OverallQual = 2`.

```{r}
#| label: lumping

ames_all <- 
  ames_all |>
  mutate(OverallQual = fct_lump(OverallQual, n = 8)) |>
  mutate(OverallQual = fct_relevel(OverallQual, "Other")) 
  
```

#### How the sale price varies depending on the living area by house styles?

```{r}
#| label: fig-houseStyles
#| fig-column: page-right
#| fig-width: 8
#| fig-height: 8
#| fig-cap: "One and two story houses are the most common within the train dataset."

ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = GrLivArea, y = SalePrice/1000)) +  
  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +  
  labs(x = expression("Ground Living Area ["~ft^2~"]"), 
       y = "Sale\nPrice\n['000 $]") +  
  geom_point(aes(color = OverallQual), alpha = 0.3) +
  geom_smooth(method = "lm", 
              formula =  y ~ splines::bs(x, 3), 
              color = "black",
              se = FALSE,
              size = 0.7, 
              alpha = 0.5) +
  facet_wrap(~HouseStyle, nrow = 2) +
  theme(legend.position = "bottom")

```


### Analysis of response SalePrice

How well-behaved is the target?

```{r}
#| label: fig-Y
#| message: false
#| warning: false
#| column: body
#| fig-width: 6
#| fig-height: 6
#| fig-cap: "Analysis of the target variable. All the assumptions of OLS and linear regression are broken, as usual."
#| fig-subcap: 
#|   - "Histogram"
#|   - "Q-Q Plot"
#| layout-ncol: 2

ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = SalePrice/1000)) +  
  geom_histogram(bins = 50, color = "seagreen", fill = "seagreen", alpha = 0.5) +
  scale_x_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +  
  labs(x = "Sale Price ['000 $]")

ames_all |>
  filter(dataset == "train") |> 
  ggplot(aes(sample = SalePrice/1000)) + 
  stat_qq(color = "seagreen", alpha = 0.5) + 
  stat_qq_line(color = "seagreen") +  
  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +
  labs(y = "Sale\nPrice\n['000 $]",
       x = "Theoretical Quantiles (Norm. Dist.)")

```

Clearly, the target variable (Y) is not well-bahaved. It shows a log-normal distribution heavily skewed, to right of course. In order to achieve better predictions, a transformation might be needed.

#### Transformation of the response

For what I've seen, processes that have to do with *money*, e.g., sale prices, costs of products, salaries, they are most of the time **Log-normal**. 
Rarely there is something free, so you don't have zeros or with negativ value; think about the top 1% of reach people, or anything that becomes **premium** the higher the price. These are long and sparsed right tails.

The logarithmic transformation works when there are few large values, like in this case, there are only a few expensive hauses. The logarithm scale will shrink that right tail[^5], making the distribution more normal-like.

[^5]: Think of a **Logarithmic transformation** like pressing an **accordeon** only from your right arm.

Nevertheless, the moment the first data transformation is performed, we start to loose interpretability very quickly. Nowadays, the most accurate predictions need more than one transformation in order to work best[^6]. 

[^6]: Forget about linearity and interpretability if you want the most accurate results.  

Because we have powerful computers and libraries with written functions to perform variable transformations, I will use the Box-Cox transformation, which is a family of functions that includes the logarithmic one depending on the value of a parameter *lambda*. This parameter is automatically assessed.

::: column-margin
*Family Box-Cox transformations*:

<br/>

$y(\lambda) = \begin{cases} \frac{y^{-1}-1}{\lambda}, & \text{if } \lambda \neq 0 \\ \log y, & \text{if } \lambda = 0 \end{cases}$
:::

```{r}
#| label: fig-boxcox
#| fig-cap: "Best parameter lambda = -0.05050505. It is almost a pure logarithmic transformation."
#| column: margin
#| code-fold: false
#| warning: false
#| message: false

boxcox_trans <- MASS::boxcox(lm(SalePrice ~ 1, 
                                data = subset(ames_all, dataset == "train")), 
                             lambda = seq(-1, 1), 
                             plotit = TRUE)

boxcox_lambda <- as.numeric(boxcox_trans$x[which.max(boxcox_trans$y)])

```

The new temporal column (only for visualisation) `SalePrice_bc` should be better behaved...

```{r}
#| label: fig-Y-trans
#| message: false
#| warning: false
#| column: body
#| fig-width: 6
#| fig-height: 6
#| fig-cap: "The transformed response shows a more bell-like shape."
#| fig-subcap: 
#|   - "Histogram"
#|   - "Q-Q Plot"
#| layout-ncol: 2

ames_all |>
  filter(dataset == "train") |>
  mutate(SalePrice_bc = (SalePrice^boxcox_lambda - 1) / boxcox_lambda) |> # New transformed Y'
  ggplot(aes(x = SalePrice_bc)) +  
  geom_histogram(bins = 50, color = "seagreen", fill = "seagreen", alpha = 0.5) +  
  labs(x = "Sale Price [Box-Cox]")

ames_all |>
  filter(dataset == "train") |> 
  mutate(SalePrice_bc = (SalePrice^boxcox_lambda - 1) / boxcox_lambda) |> # New transformed Y'
  ggplot(aes(sample = SalePrice_bc)) + 
  stat_qq(color = "seagreen", alpha = 0.5) + 
  stat_qq_line(color = "seagreen") +   
  labs(y = "Sale\nPrice\n[Box-Cox]", x = "Theoretical Quantiles (Norm. Dist.)")
```

... and it is, indeed. Well, except for the tails, now the transformed values are more normal-like at the core.

**I will include the *transformation of the response* as a step within a recipe in the modelling phase.**

# PHASE 2: MODELLING

&emsp; *"This is where the fun begins."* 

One of the motivations to develop this project is because I'm switching to the **Tidymodels** framework. Therefore, it's time to load the library to train a simple reference model.

## Resampling Technique for Test Error Estimation
This is a critical step for final model selection. Depending on the chosen resample technique to estimate the Goodness-of-Prediction or test error, in this case metrics like RMSE can be slightly optimistic or conservative. Take a look at the pristine explanation by [@resamplingML].

- **Simple validation set?** 1 train fold and 1 test fold: NO, it's old and innacurate.
- **v-folds Cross-Validation?** Analysis (training folds), Assessment (validation folds) and test: it's *small data*, I can choose something even more powerful.
- **The Bootstrap?** YES.

By today's standards this dataset is small, therefore I will choose Bootstraping to keep the chosen samples the same size as the training set. The probability of an observation of being sample at least once is 0.63, therefore the complementary training set (around 37%) will become the *assessment set*. Although performance metrics will be slightly conservative or pesimistic, I prefer it.

I will generate 200 bootstrap-samples. 

### Expending the data budget

Normally with v-fold CV I expend my data budget, but here doing resampling with replacement is kind of printing money... Here's the code to generate the bootstraps and therefore split the data budget:

```{r}
#| label: DataBudget
#| warning: false
#| fold: false

# install.packages("tidymodels")
library(tidymodels)
tidymodels_prefer()
# conflicted::conflicts_prefer(scales::discard)

# Select the pre-processed training dataset
ames_train <- 
  ames_all |> 
  filter(dataset == "train")
 

# ---- v-folds Cross Validation ----

# v-folds CV repeated 10 times (v x 10 models)
# set.seed(1982)
# ames_folds <- vfold_cv(ames_train, v = 8, repeats = 10, strata = SalePrice) # v = 10 folds are default

# ---- The Bootstrap ----

set.seed(1982)
ames_boots <- bootstraps(ames_train, times = 1000, strata = SalePrice)
ames_boots  
```

## Base Models

My final objective is to stack (combine or ensemble) several models of different nature. However, for a first model I'd like to keep interpretability. 

**Where to start?** In order to have a set of refence good models, I will proceed as follows:

- Cherry-picking predictors with linear influence from correlograms @fig-num-corr and @fig-cat-corr.
- Establish a ***workflow set*** of linear models:
  - Simplest OLS univariate lm `GrLivArea` vs. `SalePrice`.
  - Cherry-pick predictors and fit a linear hyper-plane: multivariate lm.
  - Add some flexibility on top with a Spline GAM.

For the sake of RMSE metric comparison I will always scale numeric predictors and apply the Box-Cox transformation to the outcome `SalePrice`, this given the improvement shown in @fig-Y-trans. 

The following code shows preprocessing steps enclosed in a pipeline or ***recipe***. The list of available *step functions* can be found [here](https://recipes.tidymodels.org/reference/index.html).

```{r}
#| label: fig-cherry_picking_feats
#| fig-cap: "Correlated numerical predictors are skewed and contain zero values."

ames_train |>  
  select(c(GrLivArea, TotBaths, GarageArea, TotalBsmtSF, YearBuilt)) |> 
  pivot_longer(GrLivArea:YearBuilt) |>
  ggplot(aes(x = value)) +  
  geom_histogram(bins = 50, color = "lightgrey", fill = "lightgrey") +  
  labs(x = "") + 
  facet_wrap(~name, scales = "free_x")
```

Given the sensitivity to tails and outliers of linear models, remember @fig-cont_linear-1, it will be convienent to try to normalise the distribution of these predictors. However, Box-Cox transformations work only for positive data. The alternative will be the more general **Yeo-Johnson transformation**, which works for both, positive and negative values.

```{r}
#| label: cherry-picking_lm

# ---------- Preprocessing Pipelines ----------

# Uni-lm (simplest reasonable model)
simplest_rec <-
  recipe(SalePrice ~ GrLivArea, data = ames_train) |>
  step_BoxCox(SalePrice) |> 
  step_YeoJohnson(GrLivArea) |>
  step_scale(all_numeric_predictors())

# Sanity check for warnings:
# simplest_rec |> prep() |>  bake(new_data = NULL)


# Multi-lm 
multi_lm_rec <-
  recipe(SalePrice ~ GrLivArea + TotBaths + GarageArea + TotalBsmtSF + YearBuilt + OverallQual + KitchenQual + Foundation + ExistFireplace + HeatingQC, data = ames_train) |>
  step_BoxCox(SalePrice) |> 
  step_YeoJohnson(all_numeric_predictors()) |>
  step_scale(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors())

# Sanity check for warnings:
# multi_lm_rec |> prep() |> bake(new_data = NULL)

# GAM: Splines 
multi_gam_rec <-
  multi_lm_rec |>
  step_bs(all_numeric_predictors())  # GAM basis fun expansion.


# ---------- Model specification ----------

lm_spec <- 
  linear_reg() |>
  set_mode("regression") |>
  set_engine("lm")


# ---------- Set of Workflows ----------  

my_lm_recipes <- list(simplest_rec, multi_lm_rec, multi_gam_rec)
my_lms <- list(lm_spec)

# With a workflow set I can combine a list of pre-processors with a list of models:
ames_wfset <- workflow_set(my_lm_recipes, my_lms, cross = TRUE, case_weights = NULL)
ames_wfset


# ---------- Fit Bootstraps ----------

doParallel::registerDoParallel()
set.seed(1982)

ames_lms_res <- workflow_map(ames_wfset, "fit_resamples", resamples = ames_boots)

ames_lms_res |>  
  collect_metrics(summarize = TRUE) |>
  filter(.metric == "rmse")
```

OK! We can see the power of adding good predictors.

My reference is the **linear GAM with Splines**, showing a mean RMSE = 0.0756. The result was transformed in the pipeline, therefore no units are displayed.

[I got slighlty better results using one-hot encoded categoricals. Moreover, applying the Yeo-Johnson transformation to numeric Feats. helped too! They were skewed, see @fig-cherry_picking_feats.]{.column-margin}

## Predicting for First Kaggle Submission


```{r}
#| label: first_submission

# Train Selected Model on the whole training set


# Predicting on the test set



# .csv for submission




```

Now that I have some reference RMSEs, I proceed to involve all predictors. Having high multicollinearty it's reasonable to think of some sort of penalisation or shrinkage method. Hence, I'll go with a regularised (shrinkage) Generalised Linear Model, [GLM](https://glmnet.stanford.edu/articles/glmnet.html). In this manner I can have a first look at feature importance.

I'll choose Elastic net regression. Features must be scaled! I don't want to over-penalised non-scaled variables using shrinkage methods.

```{r}
#| label: Preprocessing

# shrinkage_rec <-
#   recipe(SalePrice ~ ., data = ames_train) |>
#   update_role(Id, new_role = "Id variabe") |>
#   update_role(dataset, new_role = "splitting variable") |>
#   step_BoxCox(SalePrice, limits = c(-1, 1)) |>  
#   step_scale(all_numeric_predictors()) |>
#   step_dummy(all_nominal_predictors()) |>  
#   step_zv(all_predictors())

# shrinkage_spline_rec <-
#   shrinkage_rec |>
#   step_bs(all_numeric_predictors())  



# Recipe for trees  
# dummy_recipe <-
#   base_recipe |>
#   step_dummy(all_nominal_predictors())

# Recipe for splines
# spline_recipe <-
#   dummy_recipe |>
#   step_bs(GrLivArea)
```


```{r}
#| label: ElasticNetSpecs
#| code-fold: false

# install.packages(c("ranger", "earth", "glmnet"))

# Linear Model
# lm_spec <- 
#   linear_reg() |>
#   set_mode("regression") |>
#   set_engine("lm")


# Linear Model penalised
# elnet_spec <-
#   linear_reg(penalty = tune(), mixture = tune()) |> # penalty = lambda; mixture = alpha
#   set_mode("regression") |>
#   set_engine("glmnet")

# elnet_grid <- expand_grid(penalty = seq(0, 100, by = 10),
#                           mixture = seq(0, 1, by = 0.2) )


# doParallel::registerDoParallel()
# set.seed(1982)

# elnet_tune_res <- tune_grid(elnet_spec, 
#                             preprocessor = shrinkage_spline_rec, 
#                             grid = elnet_grid, 
#                             resamples = ames_boots)

# elnet_tune_res |>
#   collect_metrics() |>
#   filter(.metric == "rmse") |>
#   arrange(mean)


# elnet_tune_res |>
#   collect_metrics() |>
#   filter(.metric == "rmse") |>
#   ggplot(aes(penalty, mean, color = factor(mixture), group = factor(mixture))) + 
#   geom_point() + 
#   geom_line() + 
#   labs(y = "RMSE")
 
# elnet_spec <-
#   linear_reg(penalty = 10, mixture = 0) |> # penalty = lambda; mixture = alpha
#   set_mode("regression") |>
#   set_engine("glmnet")

```

```{r}
#| label: OtherModelsSpecs

# linear_reg(mixture = .1) %>% 
#   set_engine("glmnet") %>% 
#   translate()


# # Model 2 Setup
# spec_lm <- 
#   linear_reg() |>
#   set_engine("lm") |>
#   set_mode("regression")

# # Model 3
# spec_rf <-
#   rand_forest(trees = 1000) |>
#   set_engine("ranger") |>
#   set_mode("regression")
  
# # Model 4
# spec_mars <-
#   mars() |>
#   set_engine("earth") |>
#   set_mode("regression")

```




```{r}

# Workflow set

# Allows combine different pre-processors and models fit them all at once and compare results

# my_recipes <- list(base_recipe, dummy_recipe, spline_recipe)
# my_models <- list(spec_rf, spec_mars, spec_lm)

# my_recipes <- list(num_uni_lm_rec, num_mul_lm_rec, num_mul_gam_rec)
# my_models <- list(lm_spec, lm_spec, lm_spec)

# ames_wfset <- 
#   workflow_set(
#     my_recipes, 
#     my_models, 
#     cross = FALSE, 
#     case_weights = NULL)

# ames_wfset
```


```{r}
#| label: GoF

# install.packages("doParallel")

# doParallel::registerDoParallel()
# set.seed(1982)

# ames_results <- workflow_map(ames_wfset, "fit_resamples", resamples = ames_boots)

# ames_results |>  
#   collect_metrics(summarize = TRUE) |>
#   filter(.metric == "rmse")



# save(ames_results, file = "./models/ames_results_wf.Rdata")

# ames_results <- load("./models/ames_results_wf.Rdata")

```



```{r}


# ames_wf <- workflow() |>
#   add_model(spec_lm) |>
#   add_formula(SalePrice ~ GrLivArea)

# # Fit models to bootstrapped sets

# trained_models <- fit_resamples(object = ames_wf, resamples = ames_boots)

# # See estimated performance
# trained_models |>  collect_metrics(summarize = TRUE) # TRUE gets Avg. performance 

```


## Full Width Figures

You can arrange for figures to span across the entire page by using the chunk option `fig-column: page-right`.

```{r}
#| label: fig-fullwidth
#| fig-cap: "A full width figure."
#| fig-width: 11
#| fig-height: 3
#| fig-column: page-right
#| warning: false
ggplot(diamonds, aes(carat, price)) + geom_smooth() +
  facet_grid(~ cut)
```

Other chunk options related to figures can still be used, such as `fig-width`, `fig-cap`, and so on. For full width figures, usually `fig-width` is large and `fig-height` is small. In the above example, the plot size is $11 \times 3$.

## Arbitrary Full Width Content

Any content can span to the full width of the page, simply place the element in a `div` and add the class `column-page-right`. For example, the following code will display its contents as full width.

``` md
::: {.fullwidth}
Any _full width_ content here.
:::
```

Below is an example:

::: column-page-right
*R is free software and comes with ABSOLUTELY NO WARRANTY.* You are welcome to redistribute it under the terms of the GNU General Public License versions 2 or 3. For more information about these matters see <https://www.gnu.org/licenses/>.
:::


## Margin Captions

When you include a figure constrained to the main column, you can choose to place the figure's caption in the margin by using the `cap-location` chunk option. For example:

```{r}
#| label: fig-main-margin-cap
#| fig-cap: "A figure with a longer caption. The figure appears in the main column, but the caption is placed in the margin. Captions can even contain elements like a citation such as @R-base."
#| cap-location: margin
ggplot(diamonds, aes(cut, price)) + geom_boxplot()
```

# Sidenotes

[This is a span that has the class `column-margin` which places it in the margin without the sidenote mark.]{.column-margin} If you'd like to place ancillary information in the margin without the sidenote mark (the superscript number), you can use apply the `column-margin` class to the element.

References can be displayed as margin notes for HTML output. For example, we can cite R here [@R-base].

::: {.callout-note appearance="simple"}
This feature depends upon `link-citations` to locate and place references in the margin. This is enabled by default, but if you disable `link-citations` then references in the HTML output will be placed at the end of the output document as they normally are.
:::


Then two plots in separate figure environments (the code is identical to the previous code chunk, but the chunk option is the default `fig-show: asis` now):

```{r fig-two-separate, ref.label='fig-two-together', fig.cap=sprintf("Two plots in separate figure environments (the %s plot).", c("first", "second")), message=FALSE}
#| cap-location: margin
```

You may have noticed that the two figures have different captions, and that is because we used a character vector of length 2 for the chunk option `fig.cap` (something like `fig.cap = c('first plot', 'second plot')`).

::: {.callout-tip}
## Using R within Chunk Options
If you wish to use raw R expressions as part of the chunk options (like above), then you need to define those in the `tag=value` format within the curly brackets `{r label, tag=value}` instead of the `tag: value` YAML syntax on a new line starting with the hashpipe `#|`. The former approach is documented on [knitr's website](https://yihui.org/knitr/options/) while the latter is explained in [Quarto's documentation](https://quarto.org/docs/reference/cells/cells-knitr.html).
:::
