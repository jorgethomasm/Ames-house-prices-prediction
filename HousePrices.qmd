---
title: "Prediction of House Prices in Ames"
subtitle: "A commented and visual walk-through advance regression techniques." # only for html output
author: "Jorge A. Thomas"
date: "`r Sys.Date()`"
format:    
    html:
      self-contained: true
      code-fold: true
      df-print: tibble
      code-summary: "Show the code"
      grid: 
        margin-width: 350px
execute: 
  echo: fenced
reference-location: margin # margin
citation-location: document
bibliography: HousePrices.bib
# Template: https://quarto-dev.github.io/quarto-gallery/page-layout/tufte.html
---

```{r}
#| label: setup
#| message: false
#| echo: false

library(tidyverse) # ETL and EDA tools
source("./tools/jthom_functions.r")
theme_set(jthom_ggtheme)
```

::: {#fig-intro layout-ncol=2}
![Houses](./imgs/ames_gai_16x9.jpeg){width=100%}

![Location](./imgs/Ames_map.png){width=100%}

Ames, Iowa - USA.
:::

# Introduction

In this classic Kaggle dataset [@house-prices-advanced-regression-techniques] you'll follow my workflow developing pipelines on the ETL phase of the DS cycle, as well as a tidy approach on the EDA substantialy supported on Visual Analytics. I'll be feeding some tips on the right margin along the analysis.

The Ames House dataset was prepared to improve on the older classic Boston Houses dataset and, given its open competition nature, it comprises two files:

1. One file contains features and labels (or what's the same predictors and response). Here this is the *training dataset*. It's all I have to split the *Data Budget*.

2. Another file contains only features (predictors) to make predictions for the final submissions. Here this is the *test dataset*.

The goal of the competition is to achieve minimum RMSE.

I will keep updating this notebook, making improvements and reporting the rank obtained with my submitted predictions.

# PHASE I: ETL / EDA

## Extract Data

I checked beforehands that there are no missing values, here `NA`, in the target variable `SalePrice`. 
Therefore, I will write a pipeline to read and concatenate both datasets (bind rows), adding and extra column `dataset` to label as "train" and "test" for further easy splitting[^1] (subset or filter).

[^1]: The whole Feature Transformation pipeline most be always the same for all predictors in both datasets.

```{r}
#| label: Load Data
#| warning: false

ames_train_raw <- read_csv("./data/raw/train.csv") # Train, validattion and test dataset
print("Dimensions of training dataset")
dim(ames_train_raw)

ames_test_raw <- read_csv("./data/raw/test.csv")  # Features for submission dataset
print("Dimensions of test dataset containing only Feats.")
dim(ames_test_raw) 

# Add Target column with NA so both DFs can be concatenated:.id
ames_test_raw <- ames_test_raw |> mutate(SalePrice = NA)

# Binding and adding identifier column "dataset" 
ames_all <- bind_rows(list(train = ames_train_raw, test = ames_test_raw), .id = "dataset")

print("Available variables:")
names(ames_all)
```

In order to organise my **Data Budget** I count on the **train dataset** with 1460 Observations of 79 predictors (one column is only an `Id` per house) and the Response, which is the `SalePrice` of the houses at the end of the list above.

Note that `Id` and the just added `dataset` columns are not predictors. Hence, there are **79 features** to play  with. Also rememeber that the number of features for modelling will vary. Some will be discarded and some will be created along the analysis.

I'll get rid of the `Id` column:
```{r}
#| label: Remove Id

ames_all$Id <- NULL
```

## Impute Missing Values

First things first, read the full [data description](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data)[^2]. There you'll find that for several columns, missing values `NA` means actually *"None"*. The physical absence of a determined feature in a house is a category exposing the lack of such quality that can have a significant impact on the response `SalePrice`. Later it is most probably that binary features indicating the existance of some installations should be created.

Therefore, I will fill the empty `NA` fields of the indicated columns of both datasets with the string *"None"*.

[^2]: **Print a copy**, make notes and study it. This is the first step to get into domain knowledge, in this case Real Estate, for later *Feature Engineering*.

```{r}
#| label: easy replace_na

cols_NA_to_none <- list(
  Alley = "None",
  BsmtQual = "None", BsmtCond = "None", BsmtExposure = "None", BsmtFinType1 = "None", BsmtFinType2 = "None", 
  FireplaceQu = "None", 
  GarageType = "None", GarageFinish = "None", GarageQual = "None", GarageCond = "None", 
  PoolQC = "None",
  Fence = "None", 
  MiscFeature = "None")

ames_all <- ames_all |>
  replace_na(cols_NA_to_none)   
```

One of the early and recurrent steps of the EDA is to check the **completeness of data**. Let's search for missing values, after filling indicated fields[^3]. 
For this case I wrote the function `count_na()` that generates the table displayed on the right margin.

[^3]: **Write a function to quantify missing values** depending on the language and labelling system used. 

```{r}
#| label: Counting NAs
#| code-fold: false
#| column: margin

# Remaining NA count leaving out target SalePrice
ames_all |> 
  select(-SalePrice) |>
  count_na() |>  
  knitr::kable(caption = 'Ames dataset')
```

I'll fill the remaining missing values as follow:

- The `LotFrontage` column refers to the linear feet of street connected to the house. This feature is not well documented and the missing percentage related to other variables makes it not only unreliable, but very low in variance. Therefore, I will delete the feature.

- Everytime a Garage doesn't exist, i.e., `GarageType = "None"`, there is the corresponding missing value in Garage Year Built `GarageYrBlt = NA`. I will **engineer a new feature** called `GarageNew` with three ordinal categories: None, No, Yes. This based on the delta of `YearBuilt - GarageYrBlt`; I expect that given the house price, the algorithm will learn that "None" is worse than "No" and so on. Then I will remove the `GarageYrBlt` predictor.

- For the rest of variables with a 1 % or less missing values `NA`, I'll calculate the **median** (if numerical) and the **mode** (if string) in order to fill them with it.

Here's my pipeline for the whole dataset:

```{r}
#| label: terminating NAs

# "replace_na_with_median" is a custom function

ames_all <- ames_all |>
  mutate(LotFrontage = NULL) |> # Removing LotFrontage
  mutate(GarageNew = if_else(YearBuilt - GarageYrBlt > 0, "Yes", "No")) |> # New Feat.
  replace_na(list(GarageNew = "None")) |>
  mutate(GarageNew = factor(GarageNew, levels = c("None", "No", "Yes"))) |> # 3 levels
  mutate(GarageYrBlt = NULL) |> # Removing old Feat.
  mutate_if(is.numeric, replace_na_with_median)
```

Let's get a list with the mode of each remaining columns containing missing values `NA`.

```{r }
#| label: Mode for NAs

# Get the mode for reamaining columns with NAs:
# "find_mode()" is a custom function.

# Good, old-fashioned code: apply(ames_all, 2, find_mode)

list_na_mode <- ames_all |> 
  select(MasVnrType, MasVnrArea, MSZoning, Electrical, Utilities,	BsmtFullBath,	BsmtHalfBath,	Functional,	Exterior1st,	Exterior2nd,	BsmtFinSF1,	BsmtFinSF2,	BsmtUnfSF,	TotalBsmtSF,	KitchenQual,GarageCars,	GarageArea,	SaleType) |>
  map(find_mode)

# map returns a list 

# Replace with the created named-lists
ames_all <- ames_all |>
  replace_na(list_na_mode) 

# Sanity check of missing values:
print("Full Ames dataset (train and test)")
ames_all |>
  select(-SalePrice) |>
  count_na()
```

**"The data is complete!"** Let's think about predictors.

## Feature Engineering

I have already started with the creation of a new predictor in the previous step, this to substitute the problematic variable `GarageYrBlt` that had a lot of missing values.

Feature creation and transformations are not sequential, i.e., this is not the only step where *Feature Engineering* is applied. 
Think about the next *modelling phase*, where you want to reduce features, e.g., where new Principal Components are the new aggregators of original features.

In this part I will establish which variables are categorical and numerical; this is not always as obvious as it seems[^4].

[^4]: Integer variables like calendar *Years* can be treated as categories for the analysis.

The first easy thing that comes to mind is to add the **total area of the property** as a new column named `TotAreaSF`; SF means *Squared Feet*.

After that, I check the [data documentation](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data) to see which variables can be thought as categoricals (factors); this can be tedeous if Real Estate is not your domain of expertise. 

After 1.5 hours of reading, googling, understanding other notebooks and deliberating (yes, it was tedeous), I can summarise four types of ways to engineer new features depending on the nature of the variables:

1. Aggregation to a total of **continous-numerical** surface area (Square Feet), e.g. new `TotInteriorArea` Feat. summing basement and floors, etc. Same for a new `TotExteriorArea`. Then deleting the addends variables.

2. Aggregation to a total of **numerical counts** of an available installation of a determined type, e.g. new `TotBaths`. Then deleting the addends.

3. Categorisation with ascending quality levels of the originally numerical variables `OverallQual` and `OverallCond`. 

4. Creation of new **binary** features that state if a determined installation exists or doesn't.

Here's the code that complies with the four previous steps:

```{r}
#| label: Feature Engineering

# --- Total Aggregations ----
ames_all <- ames_all |>

  # Continuous numerical ----
  # NEW TotalInteriorArea
  mutate(TotInteriorArea = BsmtFinSF1 + BsmtFinSF2 + `1stFlrSF` + `2ndFlrSF`) |>
  select(-c(BsmtFinSF1, BsmtFinSF2, `1stFlrSF`))|> # Bye bye...

  # NEW TotalExteriorArea
  mutate(TotExteriorArea = WoodDeckSF + OpenPorchSF + EnclosedPorch + `3SsnPorch` + ScreenPorch) |>
  select(-c(WoodDeckSF, OpenPorchSF, EnclosedPorch, `3SsnPorch`, ScreenPorch)) |># Bye bye...

  # Counting numerical ----
  # NEW TotalBaths
  mutate(TotBaths = BsmtFullBath + 0.5 * BsmtHalfBath + FullBath + 0.5 * HalfBath) |> 
  select(-c(BsmtFullBath, BsmtHalfBath, FullBath, HalfBath)) # Bye bye...


# ---- Ordered Categorical Features ----
ames_all <- ames_all |>
  mutate(OverallQual = factor(OverallQual)) |>
  mutate(OverallCond = factor(OverallCond))  # Check level orders 1, ..., 10 (BEST)


# ---- Binary Features ----
# Does the installation exist? 

ames_all <- ames_all |>
  # CentralAir
  mutate(CentralAir = if_else(CentralAir == "N", "No", "Yes")) |>
  mutate(CentralAir = fct_relevel(CentralAir, "No", "Yes"))|>

  # NEW IsRemodelled (it will replace YearRemodAdd)
  mutate(IsRemodelled = if_else(YearRemodAdd - YearBuilt > 0, "Yes", "No")) |>
  mutate(IsRemodelled = factor(IsRemodelled, levels = c("No", "Yes"))) |>
  select(-YearRemodAdd) |> # Bye YearRemodAdd

  # NEW ExistPool
  mutate(ExistPool = if_else(PoolArea == 0, "No", "Yes")) |>
  mutate(ExistPool = fct_relevel(ExistPool, "No", "Yes"))|>

  # NEW ExistGarage
  mutate(ExistGarage = if_else(GarageArea == 0, "No", "Yes")) |>
  mutate(ExistGarage = fct_relevel(ExistGarage, "No", "Yes"))|>

  # NEW ExistBsmt
  mutate(ExistBsmt = if_else(TotalBsmtSF == 0, "No", "Yes")) |>
  mutate(ExistBsmt = fct_relevel(ExistBsmt, "No", "Yes"))|>

  # NEW Exist2ndFloor
  mutate(Exist2ndFloor = if_else(`2ndFlrSF` == 0, "No", "Yes")) |>
  select(-`2ndFlrSF`) |> # Bye, bye... 
  mutate(Exist2ndFloor = fct_relevel(Exist2ndFloor, "No", "Yes")) |>

  # NEW ExistFireplace
  mutate(ExistFireplace = if_else(Fireplaces == 0, "No", "Yes")) |>
  mutate(ExistFireplace = fct_relevel(ExistFireplace, "No", "Yes"))


# ---- The rest are factors ----
# I'm going to assume that the rest are Categorical variables 

ames_all <- ames_all |>
  mutate(across(where(is.character), factor))

# ---- Numerical Features ----

# BedroomabvGr (count) left numerical 
# Kitchens (count) left numerical 
# TotRmsAbvGrd (count) left numerical
# Fireplaces (count) left numerical
# GarageCars (count) left numerical
# GarageArea (continous) left numerical 
# MiscVal left numerical 
# MasVnrArea (continous) left numeric


# ---- Remove 'LowQualFinSF' ----

# Low quality finished square feet (all floors)
# Most of the values are 0 -> almost 0 variance. 
ames_all$LowQualFinSF <- NULL

```

Note that I replaced `YearRemodAdd` to a simpler binary `IsRemodelled` variable, similar to what I did with the new `GarageNew` column.

OK! Let's move on. 

### Searching for simple correlations

Although **nothing is linear**, is good to have an idea of simple correlations between all variables; it gives me an idea of what kind of plots to make. After going through the previous steps, it's obvious that there are correlations. Think about the number of cars that fit in a garage `GarageCars` and the area of the garage `GarageArea`.

```{r}
#| label: fig-num-corr
#| fig-width: 8
#| fig-height: 8
#| fig-cap: Pearson's correlation of numeric features and response. High multicollinearity.
#| cap-location: margin
#| warning: false
#| message: false

library(corrplot)

ames_all |>
  filter(dataset == "train") |>
  select(-dataset) |>
  select(where(is.numeric)) |>  
  cor(method = "pearson") |>
  corrplot(type = "lower", method = "circle", insig = 'blank', order = "hclust", diag = TRUE,
  tl.col="black", tl.cex = 0.8, addCoef.col = 'black', number.cex = 0.6)

```

According to the matrix above, `GrLivArea` has the strongest linear relationship with the target `SalePrice`, followed by `TotInteriorArea`, this given the collinearity with `GrLivArea`, and then `GarageArea` or `GarageCars`, both heavily correlated too, like commented before. 

I'm leaving blank, i.e., without a circle, the insignificant correlations for better visualisation and focus. Now I'm checking the categorical variables and the response.

```{r}
#| label: fig-cat-corr
#| fig-width: 10
#| fig-height: 10
#| fig-cap: Pearson's correlation of categorical features and response.
#| cap-location: margin
#| warning: false
#| message: false

ames_all |>
  filter(dataset == "train") |>
  select(-dataset) |>
  select(where(is.factor) | contains("Price")) |>  
  mutate_if(is.factor, as.integer) |> 
  cor(method = "pearson") |>
  corrplot(type = "lower", order="hclust", diag = TRUE, insig = 'blank',
  tl.col="black", tl.cex = 0.8, number.cex = 0.6)

```

The best linear association with `SalePrice` is given by `OverallQual` factor, followed by everything else that has to do with *quality* ratings. 

Here, the highest collinearity is showed between `PoolQC` and the new `ExistPool` variables. This is because they share a majority ammount of "None" and "No" values respectivelly in the exact same positions, therefore making the information redundant. In case of one-hot enconding there would be two repeated colums.

Overall, one can see clearly the **multicollinearity** clusters, which suggest the use of advance **Feature Selection** techniques.

### Visual Exploration

Now with the previous information in hand, I'm going to visualise separatelly the two most important quasi-linear relationsships: `GrLivArea` and `OverallQual` vs. `SalePrice`.

```{r}
#| label: fig-cont_linear
#| message: false
#| warning: false
#| column: body
#| fig-width: 6
#| fig-height: 6
#| fig-cap: 'GrvLivArea Vs. SalePrice - The most "linear" realationship between continuous variables.'
#| cap-location: margin
#| fig-subcap: 
#|   - "Scatter Plot"
#|   - "Hexbin Plot"
#| layout-ncol: 2

ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = GrLivArea, y = SalePrice/1000, color = OverallQual)) +  
  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +  
  labs(x = expression("Ground Living Area ["~ft^2~"]"), 
       y = "Sale Price\n['000 $]") +  
  geom_point(alpha = 0.5) + theme(axis.text.y = element_blank()) +
  theme(legend.position = "bottom")

# install.packages("hexbin")
ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = GrLivArea, y = SalePrice/1000)) +  
  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +    
  geom_hex(bins = 35, colour = "seagreen") + 
  labs(x = expression("Ground Living Area ["~ft^2~"]")) +    
  theme(axis.title.y = element_blank(), legend.position = "bottom")

```

The hexbin plot reveals three cells of price and size where houses are more common in the training dataset. **There are more than 50 houses for each bin in the range of 130K, 140K and 180K USD** aproximately.

Furthermore, This visual allows me to *remove outliers*, i.e., the biggest and cheap houses (right data points).

```{r}
#| label: RmOutliers

ames_all |>
  select(c(SalePrice, GrLivArea, OverallQual, YrSold)) |>
  filter(GrLivArea > 4500) |>
  glimpse()

ames_all <- ames_all |> filter(GrLivArea < 4500)

```

The **three big houses above were like super offers!** Or did the housing bubble exploded?

In the same fashion, I want to see the effect of the factor `OverallQual`.

```{r}
#| label: fig-cat_linear
#| message: false
#| warning: false
#| fig-width: 7
#| fig-height: 8
#| fig-cap: 'OverallQual Vs. SalePrice - The most "linear" realationship with a categorical predictor. Variances of the factors are different.'
#| cap-location: margin

# install.packages("ggrain")
library(ggrain)

ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = OverallQual, y = SalePrice/1000, colour = OverallQual, fill = OverallQual)) +     
  geom_rain(alpha = 0.3) +  
  scale_y_continuous(breaks = seq(0, 800, 100)) +  
  labs(x = "Overall Quality", y = "Sale Price ['000 $]") +
  coord_flip() + 
  theme(legend.position = "none")

```

I'm wondering in which price range is the most popular quality?

```{r}
#| label: fig-count_overallQual
#| cap-location: margin
#| fig-cap: "Houses with mid-quality levels are most frequent in the train dataset."
#| column: margin
#| code-fold: false

ames_all |>
  filter(dataset == "train") |>
  count(OverallQual, name = "count") |>
  ggplot(aes(y = fct_reorder(OverallQual, count), x = count, fill = OverallQual)) + 
  geom_col() +
  labs(y = "Overall\nQuality") + 
  theme(legend.position = "none")
```

It seems that most  popular houses, with an overall qualilty of 5 (mid-quality level), are the ones between 120K and 145K USD. Of course, this was back in the years of 2006 - 2010.

### Analysis of the response variable: Sale Price

How well-behaved is the target?

```{r}
#| label: fig-Y
#| message: false
#| warning: false
#| column: body
#| fig-width: 6
#| fig-height: 6
#| fig-cap: "Analysis of the target variable. All the assumptions of OLS and linear regression are broken, as usual."
#| cap-location: margin
#| fig-subcap: 
#|   - "Histogram"
#|   - "Q-Q Plot"
#| layout-ncol: 2

ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = SalePrice/1000)) +  
  geom_histogram(bins = 50, color = "seagreen", fill = "seagreen", alpha = 0.5) +
  scale_x_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +  
  labs(x = "Sale Price ['000 $]")

ames_all |>
  filter(dataset == "train") |> 
  ggplot(aes(sample = SalePrice/1000)) + 
  stat_qq(color = "seagreen", alpha = 0.5) + 
  stat_qq_line(color = "seagreen") +  
  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +
  labs(y = "Sale Price\n['000 $]",
       x = "Theoretical Quantiles (Norm. Dist.)")

```

The target variable (Y) is clearly not well-bahaved; it shows a log-normal distribution heavily skewed to right. In order to achieve better predictions, a transformation will be needed.

#### Transformation of the response

For what I've seen, processes that have to do with *money*, e.g., sale prices, costs of products, salaries, they are most of the time **Log-normal**. 
Rarely is something free, so you don't have zeros; think about the top 1% of reach people, or anything that becomes **premium** the higher the price. These are long and sparsed tails.

The moment the first data transformation is performed, we start to loose interpretability very quickly. 
Nowadays, the most accurate predictions need more than one transformation in order to work best[^5]. 

[^5]: Forget about lineaerity and interpretability if you want the most accurate results.  

The logarithmic transformation works when there are few large values, like in this case, there are only a few expensive hauses. The logarithm scale will shrink that right tail[^6], making the distribution more normal-like.

[^6]: Think of a **Logarithmic transformation** like pressing an **accordeon** only from your right arm.

However, because we have a computer and libraries with written functions to perform variable transformations, I will use the Box-Cox transformation, which is a family of transformations (functions) that include the logarithmic one.

```{r}
#| label: fig-boxcox
#| fig-cap: "Best parameter lambda near 0. Almost a pure logarithmic transformation."
#| column: margin
#| code-fold: false
#| warning: false
#| message: false

boxcox_trans <- MASS::boxcox(lm(SalePrice ~ 1, data = subset(ames_all, dataset == "train")), 
                             lambda = seq(-1, 1), 
                             plotit = TRUE)

lambda <- as.numeric(boxcox_trans$x[which.max(boxcox_trans$y)])

# New transformed response variable
ames_all <- ames_all |>
  mutate(SalePrice_bc = (SalePrice^lambda - 1) / lambda)

```

The new column `SalePrice_bc` should be better behaved...

```{r}
#| label: fig-Y-trans
#| message: false
#| warning: false
#| column: body
#| fig-width: 6
#| fig-height: 6
#| fig-cap: "Transformed Response."
#| cap-location: margin
#| fig-subcap: 
#|   - "Histogram"
#|   - "Q-Q Plot"
#| layout-ncol: 2

ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = SalePrice_bc)) +  
  geom_histogram(bins = 50, color = "seagreen", fill = "seagreen", alpha = 0.5) +  
  labs(x = "Sale Price [Box-Cox]")

ames_all |>
  filter(dataset == "train") |> 
  ggplot(aes(sample = SalePrice_bc)) + 
  stat_qq(color = "seagreen", alpha = 0.5) + 
  stat_qq_line(color = "seagreen") +   
  labs(y = "Sale Price\n[Box-Cox]", x = "Theoretical Quantiles (Norm. Dist.)")

```

... and it is, indeed. Well, except for the tails, now the transformed values are more normal-like at the core.

**I will include the *transformation of the response* as a step within a recipe in the modelling phase.**

# PHASE 2: MODELLING

&emsp; *"This is where the fun begins."* 

One of the motivations to develop this project is because I'm switching to the **Tidymodels** framework. Therefore, it's time to load the library to train a simple reference model.

## Reference Model

```{r}
#| label: modelling

# install.packages("tidymodels")



```

## Model Selection



## Arbitrary Margin Content

You can include anything in the margin by places the class `.column-margin` on the element. See an example on the right about the first fundamental theorem of calculus.

::: column-margin
We know from *the first fundamental theorem of calculus* that for $x$ in $[a, b]$:

$$\frac{d}{dx}\left( \int_{a}^{x} f(u)\,du\right)=f(x).$$
:::

## Full Width Figures

You can arrange for figures to span across the entire page by using the chunk option `fig-column: page-right`.

```{r}
#| label: fig-fullwidth
#| fig-cap: "A full width figure."
#| fig-width: 11
#| fig-height: 3
#| fig-column: page-right
#| warning: false
ggplot(diamonds, aes(carat, price)) + geom_smooth() +
  facet_grid(~ cut)
```

Other chunk options related to figures can still be used, such as `fig-width`, `fig-cap`, and so on. For full width figures, usually `fig-width` is large and `fig-height` is small. In the above example, the plot size is $11 \times 3$.

## Arbitrary Full Width Content

Any content can span to the full width of the page, simply place the element in a `div` and add the class `column-page-right`. For example, the following code will display its contents as full width.

``` md
::: {.fullwidth}
Any _full width_ content here.
:::
```

Below is an example:

::: column-page-right
*R is free software and comes with ABSOLUTELY NO WARRANTY.* You are welcome to redistribute it under the terms of the GNU General Public License versions 2 or 3. For more information about these matters see <https://www.gnu.org/licenses/>.
:::

## Main Column Figures

Besides margin and full width figures, you can of course also include figures constrained to the main column. This is the default type of figures in the LaTeX/HTML output.

```{r}
#| label: fig-main
#| fig-cap: "A figure in the main column."
ggplot(diamonds, aes(cut, price)) + geom_boxplot()
```

## Margin Captions

When you include a figure constrained to the main column, you can choose to place the figure's caption in the margin by using the `cap-location` chunk option. For example:

```{r}
#| label: fig-main-margin-cap
#| fig-cap: "A figure with a longer caption. The figure appears in the main column, but the caption is placed in the margin. Captions can even contain elements like a citation such as @R-base."
#| cap-location: margin
ggplot(diamonds, aes(cut, price)) + geom_boxplot()
```

# Sidenotes

One of the most prominent and distinctive features of this style is the extensive use of sidenotes. There is a wide margin to provide ample room for sidenotes and small figures. Any use of a footnote will automatically be converted to a sidenote.

[This is a span that has the class `column-margin` which places it in the margin without the sidenote mark.]{.column-margin} If you'd like to place ancillary information in the margin without the sidenote mark (the superscript number), you can use apply the `column-margin` class to the element.

References can be displayed as margin notes for HTML output. For example, we can cite R here [@R-base].

::: {.callout-note appearance="simple"}
This feature depends upon `link-citations` to locate and place references in the margin. This is enabled by default, but if you disable `link-citations` then references in the HTML output will be placed at the end of the output document as they normally are.
:::


Then two plots in separate figure environments (the code is identical to the previous code chunk, but the chunk option is the default `fig-show: asis` now):

```{r fig-two-separate, ref.label='fig-two-together', fig.cap=sprintf("Two plots in separate figure environments (the %s plot).", c("first", "second")), message=FALSE}
#| cap-location: margin
```

You may have noticed that the two figures have different captions, and that is because we used a character vector of length 2 for the chunk option `fig.cap` (something like `fig.cap = c('first plot', 'second plot')`).

::: {.callout-tip}
## Using R within Chunk Options
If you wish to use raw R expressions as part of the chunk options (like above), then you need to define those in the `tag=value` format within the curly brackets `{r label, tag=value}` instead of the `tag: value` YAML syntax on a new line starting with the hashpipe `#|`. The former approach is documented on [knitr's website](https://yihui.org/knitr/options/) while the latter is explained in [Quarto's documentation](https://quarto.org/docs/reference/cells/cells-knitr.html).
:::


# Some Notes on Page Layout

To see the Quarto markdown source of this example document, you may follow [this link to Github](https://raw.githubusercontent.com/quarto-dev/quarto-gallery/main/page-layout/tufte.qmd).