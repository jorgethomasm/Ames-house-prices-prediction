---
title: "Prediction of House Prices in Ames"
subtitle: "A commented and visual walk-through advance regression techniques." # only for html output
author: "Jorge A. Thomas"
date: "`r Sys.Date()`"
format:    
    html:
      self-contained: true
      code-fold: true
      df-print: tibble
      code-summary: "Show the code"
      grid: 
        margin-width: 350px
execute: 
  echo: fenced
reference-location: margin # margin
citation-location: document
bibliography: HousePrices.bib
# Template: https://quarto-dev.github.io/quarto-gallery/page-layout/tufte.html
---

```{r}
#| label: setup
#| message: false
#| echo: false

library(tidyverse) # ETL and EDA tools
source("./tools/jthomfuncs.r")
theme_set(jthomggtheme)
```

::: {#fig-intro layout-ncol="2"}
![Houses](./imgs/ames_gai_16x9.jpeg){width="100%"}

![Location](./imgs/Ames_map.png){width="100%"}

Ames, Iowa - USA.
:::

# Introduction

In this classic Kaggle dataset [@house-prices-advanced-regression-techniques] you'll follow my workflow developing pipelines on the ETL phase of the DS cycle using the R programming language [@R-base], as well as a tidy approach on the EDA substantially supported on Visual Analytics. I'll be feeding some tips on the right margin along the analysis.

The Ames House dataset was prepared to improve on the older classic Boston Houses dataset and, given its open competition nature, it comprises two files:

1.  One file contains features and labels or what's the same predictors and responses. Here this is the *training dataset*. It's all I have to split the ***Data Budget***.

2.  Another file contains only features (predictors) to make predictions for the final submissions. Here this is the *test dataset*.

The goal of the competition is to achieve minimum RMSE.

I will keep updating this notebook, making improvements and reporting the rank obtained with my submitted predictions.

# PHASE I: ETL / EDA

## Extract Data

I checked beforehand that there are no missing values, here `NA`, in the target variable `SalePrice`. Therefore, I will write a pipeline to read and concatenate both datasets (bind rows), adding and extra column `dataset` to label as "train" and "test" for further easy splitting[^1] (subset or filter).

[^1]: The whole Feature Transformation pipeline most be always the same for all predictors in both datasets.

```{r}
#| label: Load Data
#| warning: false

ames_train_raw <- read_csv("./data/raw/train.csv") # Train, validattion and test dataset
print("Dimensions of training dataset")
dim(ames_train_raw)

ames_test_raw <- read_csv("./data/raw/test.csv")  # Features for submission dataset
print("Dimensions of test dataset containing only Feats.")
dim(ames_test_raw) 

# Add Target column with NA so both DFs can be concatenated:.id
ames_test_raw <- 
  ames_test_raw |> 
    mutate(SalePrice = NA)

# Binding and adding identifier column "dataset" 
ames_all <- 
  bind_rows(list(train = ames_train_raw, test = ames_test_raw), .id = "dataset")

print("Available variables:")
names(ames_all)
```

In order to organise my **Data Budget** I count on the **train dataset** with 1460 Observations. Note that `Id` and the just added `dataset` columns are not predictors. Hence, there are **78 features** to play with. Also remember that the number of features for modelling will vary. Some will be discarded and some will be created along the analysis.

## Impute Missing Values

First things first, read the full [data description](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data)[^2]. There you'll find that for several columns, missing values `NA` means actually *"None"*. The physical absence of a determined feature in a house is a category exposing the lack of such quality that can have a significant impact on the response `SalePrice`. Later it is most probably that binary features indicating the existence of some installations should be created.

[^2]: **Print a copy**, make notes and study it. This is the first step to get into domain knowledge, in this case Real Estate, for later *Feature Engineering*.

Therefore, I will fill the empty `NA` fields of the indicated columns of both datasets with the string *"None"*.

```{r}
#| label: easy replace_na

cols_NA_to_none <- list(
  Alley = "None",
  BsmtQual = "None", BsmtCond = "None", BsmtExposure = "None", BsmtFinType1 = "None", BsmtFinType2 = "None", 
  FireplaceQu = "None", 
  GarageType = "None", GarageFinish = "None", GarageQual = "None", GarageCond = "None", 
  PoolQC = "None",
  Fence = "None", 
  MiscFeature = "None")

ames_all <- ames_all |>
  replace_na(cols_NA_to_none)   
```

One of the early and recurrent steps of the EDA is to check the **completeness of data**. Let's search for missing values, after filling indicated fields[^3]. For this case I wrote the function `count_na()` that generates the table displayed on the right margin.

[^3]: **Write a function to quantify missing values** depending on the language and labelling system used.

```{r}
#| label: Counting NAs
#| code-fold: false
#| column: margin

# Remaining NA count leaving out target SalePrice
ames_all |> 
  select(-SalePrice) |>
  count_na() |>  
  knitr::kable(caption = 'Ames dataset')
```

I'll fill the remaining missing values as follow:

-   The `LotFrontage` column refers to the linear feet of street connected to the house. This feature is not well documented and the missing percentage related to other variables makes it not only unreliable, but very low in variance. Therefore, I will delete the feature.

-   Everytime a Garage doesn't exist, i.e., `GarageType = "None"`, there is the corresponding missing value in Garage Year Built `GarageYrBlt = NA`. I will **engineer a new feature** called `GarageNew` with three ordinal categories: None, No, Yes. This based on the delta of `YearBuilt - GarageYrBlt`; I expect that given the house price, the algorithm will learn that "None" is worse than "No" and so on. Then I will remove the `GarageYrBlt` predictor.

-   For the rest of variables with a 1 % or less missing values `NA`, I'll calculate the **median** (if numerical) and the **mode** (if string) in order to fill them with it.

Here's my pipeline for the whole dataset:

```{r}
#| label: terminating NAs

# "replace_na_with_median" is a function I created

ames_all <- ames_all |>
  mutate(LotFrontage = NULL) |> # Removing LotFrontage
  mutate(GarageNew = if_else(YearBuilt - GarageYrBlt > 0, "Yes", "No")) |> # New Feat.
  replace_na(list(GarageNew = "None")) |>
  mutate(GarageNew = factor(GarageNew, levels = c("None", "No", "Yes"))) |> # 3 levels
  mutate(GarageYrBlt = NULL) |> # Removing old Feat.
  mutate_if(is.numeric, replace_na_with_median)
```

Let's get a list with the mode of each remaining columns containing missing values `NA`.

```{r }
#| label: Mode for NAs

# Get the mode for reamaining columns with NAs:
# "find_mode()" is a custom function.

# Good, old-fashioned code: apply(ames_all, 2, find_mode)

list_na_mode <- ames_all |> 
  select(MasVnrType, MasVnrArea, MSZoning, Electrical, Utilities,	BsmtFullBath,	BsmtHalfBath,	Functional,	Exterior1st,	Exterior2nd,	BsmtFinSF1,	BsmtFinSF2,	BsmtUnfSF,	TotalBsmtSF,	KitchenQual,GarageCars,	GarageArea,	SaleType) |>
  map(find_mode)

# map returns a list 

# Replace with the created named-lists
ames_all <- ames_all |>
  replace_na(list_na_mode) 

# Sanity check of missing values:
print("Full Ames dataset (train and test)")
ames_all |>
  select(-SalePrice) |>
  count_na()
```

**"The data is complete!"** Let's think about predictors.

There are variables (columns) starting with numbers? I'll rename them:

``` {r}
#| label: rename_cols

ames_all <-
  ames_all |>
  rename(FirstFlrSF = "1stFlrSF") |>
  rename(SecondFlrSF = "2ndFlrSF") |>
  rename(threessnporch = "3SsnPorch")

```

### Screening of variable importance

Using the **Mutual Information criterion (MI)**, I'll rank the extent to which knowledge of each original predictor reduces the uncertainty of `SalePrice`.

```{r}
#| label: mutual-info
#| column: margin
#| code-fold: false

library(infotheo)

# "calc_mi" is a function I wrote.

mi_y_X <- ames_all |>
  filter(dataset == "train") |>
  select(-c(Id, dataset)) |>
  calc_mi_score(target = "SalePrice")

mi_y_X |> 
  head(20) |>
  knitr::kable(caption = "Mutual Info")
```

## Feature Engineering

I have already started with the creation of a new predictor in the previous step, this to substitute the problematic variable `GarageYrBlt` that had a lot of missing values.

Feature creation and transformations are not sequential, i.e., this is not the only step where *Feature Engineering* is applied. Think about the next *modelling phase*, where you want to reduce features, e.g., where new Principal Components are the new aggregators of original features.

In this part I will establish which variables are categorical and numerical; this is not always as obvious as it seems[^4].

[^4]: Integer variables like calendar *Years* can be treated as categories for the analysis.

The first easy thing that comes to mind is to add the **total area of the property** as a new column named `TotAreaSF`; SF means *Squared Feet*.

After that, I check the [data documentation](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data) to see which variables can be thought as factors; this can be tedious if Real Estate is not your domain of expertise.

After 1.5 hours of reading, googling, understanding other notebooks and deliberating (yes, it was tedious), I can summarise four types of ways to engineer new features depending on the nature of the variables:

1.  Aggregation to a total of **continuous-numerical** surface area (Square Feet), e.g. new `TotalIntArea` Feat. summing basement and floors, etc. Same for a new `TotalExtArea`. Then deleting the addends variables.

2.  Aggregation to a total of **counts-numerical** of an available installation of a determined type, e.g. new `TotBaths`. Then deleting the addends.

3.  Categorisation with ascending quality levels of the originally numerical variables `OverallQual` and `OverallCond`.

4.  Creation of new **binary** features that state if a determined installation exists or doesn't.

[After my second Kaggle submission (Top 25%) I completed the [Feature Engineering course](https://www.kaggle.com/learn/feature-engineering) that has a lot of tips for this projects.
I included some new features and tricks from there]{.column-margin}

Here's the code that complies with the four previous steps:

```{r}
#| label: Feature Engineering

# --- Total Aggregations ----
ames_all <- ames_all |>

  # Continuous numerical ----
  # NEW TotalInteriorArea
  mutate(TotalIntArea = BsmtFinSF1 + BsmtFinSF2 + FirstFlrSF + SecondFlrSF) |>
  select(-c(BsmtFinSF1, BsmtFinSF2))|> # Bye bye...

  # NEW HouseAge
  # mutate(HouseAge = YrSold - YearBuilt) |>  # Same as YearBuilt

  # NEW TotalExteriorArea
  mutate(TotalExtArea = WoodDeckSF + OpenPorchSF + EnclosedPorch + threessnporch + ScreenPorch) |>
  select(-c(WoodDeckSF, OpenPorchSF, EnclosedPorch, `threessnporch`, ScreenPorch)) |> # Bye bye...

  # Counting numerical ----
  # NEW TotalBaths
  mutate(TotBaths = BsmtFullBath + 0.5 * BsmtHalfBath + FullBath + 0.5 * HalfBath) |> 
  select(-c(BsmtFullBath, BsmtHalfBath, FullBath, HalfBath)) # Bye bye...


# ---- Ordered Categorical Features ----

# Check which columns have "None"
# sapply(ames_all, function(x) sum(x == "None"))

ames_all <- ames_all |>
  # 10 Levels:
  mutate(OverallQual = factor(OverallQual)) |>
  mutate(OverallCond = factor(OverallCond)) |> 

  # 5 Levels + None
  mutate(ExterQual = factor(ExterQual, c("Po", "Fa", "TA", "Gd", "Ex"))) |>
  mutate(ExterCond = factor(ExterCond, c("Po", "Fa", "TA", "Gd", "Ex"))) |>
  mutate(BsmtQual = factor(BsmtQual, c("None", "Po", "Fa", "TA", "Gd", "Ex"))) |>
  mutate(BsmtCond = factor(BsmtCond, c("None", "Po", "Fa", "TA", "Gd", "Ex"))) |>
  mutate(HeatingQC = factor(HeatingQC, c("Po", "Fa", "TA", "Gd", "Ex"))) |>
  mutate(KitchenQual = factor(KitchenQual, c("Po", "Fa", "TA", "Gd", "Ex"))) |>
  mutate(FireplaceQu = factor(FireplaceQu, c("None", "Po", "Fa", "TA", "Gd", "Ex"))) |>
  mutate(GarageQual = factor(GarageQual, c("None", "Po", "Fa", "TA", "Gd", "Ex"))) |>
  mutate(GarageCond = factor(GarageCond, c("None", "Po", "Fa", "TA", "Gd", "Ex"))) |>
  mutate(PoolQC = factor(PoolQC, c("None", "Po", "Fa", "TA", "Gd", "Ex"))) |>

  #  Others:
  mutate(LotShape = factor(LotShape, c("Reg", "IR1", "IR2", "IR3"))) |>
  mutate(LandSlope = factor(LandSlope, c("Sev", "Mod", "Gtl"))) |>
  mutate(BsmtExposure = factor(BsmtExposure, c("None", "No", "Mn", "Av", "Gd"))) |>
  mutate(BsmtFinType1 = factor(BsmtFinType1, c("None", "Unf", "LwQ", "Rec", "BLQ", "ALQ", "GLQ"))) |>
  mutate(BsmtFinType2 = factor(BsmtFinType2, c("None", "Unf", "LwQ", "Rec", "BLQ", "ALQ", "GLQ"))) |>
  mutate(Functional = factor(Functional, c("Sal", "Sev", "Maj1", "Maj2", "Mod", "Min2", "Min1", "Typ"))) |>
  mutate(GarageFinish = factor(GarageFinish, c("None", "Unf", "RFn", "Fin"))) |>
  mutate(PavedDrive = factor(PavedDrive, c("N", "P", "Y"))) |>
  mutate(Utilities = factor(Utilities, c("NoSeWa", "NoSewr", "AllPub"))) |>  
  mutate(Electrical = factor(Electrical, c("Mix", "FuseP", "FuseF", "FuseA", "SBrkr"))) |>
  mutate(Fence = factor(Fence, c("None", "MnWw", "GdWo", "MnPrv", "GdPrv")))


# ---- New Binary Features ----

# Does the installation or Feature exist? 
ames_all <- ames_all |>
  # CentralAir
  mutate(CentralAir = if_else(CentralAir == "N", "No", "Yes")) |>
  mutate(CentralAir = fct_relevel(CentralAir, "No", "Yes"))|>

  # NEW IsRemodelled (it will replace YearRemodAdd)
  mutate(IsRemodelled = if_else(YearRemodAdd - YearBuilt > 0, "Yes", "No")) |>
  mutate(IsRemodelled = factor(IsRemodelled, levels = c("No", "Yes"))) |>
  select(-YearRemodAdd) |> # Bye YearRemodAdd

  # NEW ExistPool  
  # mutate(ExistPool = if_else(PoolArea == 0, "No", "Yes")) |>
  # mutate(ExistPool = fct_relevel(ExistPool, "No", "Yes"))|>
  # a dummy of PoolQC will make this Feat. redundant!

  # NEW ExistGarage
  mutate(ExistGarage = if_else(GarageArea == 0, "No", "Yes")) |>
  mutate(ExistGarage = fct_relevel(ExistGarage, "No", "Yes"))|>

  # NEW ExistBsmt
  mutate(ExistBsmt = if_else(TotalBsmtSF == 0, "No", "Yes")) |>
  mutate(ExistBsmt = fct_relevel(ExistBsmt, "No", "Yes"))|>

  # NEW Exist2ndFloor
  mutate(Exist2ndFloor = if_else(`SecondFlrSF` == 0, "No", "Yes")) |>  
  mutate(Exist2ndFloor = fct_relevel(Exist2ndFloor, "No", "Yes")) |>

  # NEW ExistFireplace
  mutate(ExistFireplace = if_else(Fireplaces == 0, "No", "Yes")) |>
  mutate(ExistFireplace = fct_relevel(ExistFireplace, "No", "Yes")) |>

  # NEW ExistMasVeneer: Masonry veneer area
  mutate(ExistMasVeneer = if_else(MasVnrArea > 0, "Yes", "No")) |>
  mutate(ExistMasVeneer = factor(ExistMasVeneer, levels = c("No", "Yes"))) |>

  # From Kaggle's Feature Engineering course ---- 

  # New ratios
  mutate(LivLotRatio = GrLivArea/LotArea) |>
  mutate(Spaciousness = (FirstFlrSF + SecondFlrSF) / TotRmsAbvGrd) |>

  # New interactions (by me!)
  mutate(OverallQuCoInt = as.numeric(OverallQual) * as.numeric(OverallCond))


 
 

# ---- Unordered factors ----

# Nominative (unordered) categorical Feats
cats_feats <- c("MSSubClass", "MSZoning", "Street", "Alley", "LandContour", "LotConfig", 
                "Neighborhood", "Condition1", "Condition2", "BldgType", "HouseStyle", 
                "RoofStyle", "RoofMatl", "Exterior1st", "Exterior2nd", "MasVnrType", 
                "Foundation", "Heating", "CentralAir", "GarageType", "MiscFeature", 
                "SaleType", "SaleCondition")

ames_all <- ames_all |>
  #mutate(across(where(is.character), factor)) |>
  mutate(across(all_of(cats_feats), factor))
  


# ---- Numerical Features ----

# BedroomabvGr (count) left numerical 
# Kitchens (count) left numerical 
# TotRmsAbvGrd (count) left numerical
# Fireplaces (count) left numerical
# GarageCars (count) left numerical
# GarageArea (continous) left numerical 
# MiscVal left numerical 
# MasVnrArea (continous) left numeric


# ---- Remove 'LowQualFinSF' ----

# Low quality finished square feet (all floors)
# Most of the values are 0 -> almost 0 variance. 

ames_all$LowQualFinSF <- NULL
ames_all$PoolArea <- NULL
```

Note that I replaced `YearRemodAdd` to a simpler binary `IsRemodelled` variable, similar to what I did with the new `GarageNew` column.

### Recheck mutual info

```{r}
#| label: recheck_mi

# "calc_mi" is a function I wrote.

mi_y_X_2 <- ames_all |>
  filter(dataset == "train") |>
  select(-c(Id, dataset)) |>
  calc_mi_score(target = "SalePrice")

mi_y_X_2 |> 
  head(40) |>
  knitr::kable(caption = "Mutual Info")

```

OK! Let's move on.

### Searching for simple correlations

Although **nothing is linear**, is good to have an idea of simple correlations between all variables; it gives me an idea of what kind of plots to make. After going through the previous steps, it's obvious that there are correlations. Think about the number of cars that fit in a garage `GarageCars` and the area of the garage `GarageArea`.

```{r}
#| label: fig-num-corr
#| fig-width: 8
#| fig-height: 8
#| fig-cap: The Pearson's correlation of numeric features and response shows high presence of multicollinearity.
#| warning: false
#| message: false

library(corrplot)

ames_all |>
  filter(dataset == "train") |>
  select(-c(dataset, Id)) |>
  select(where(is.numeric)) |>  
  cor(method = "pearson") |>
  corrplot(type = "lower", method = "circle", insig = 'blank', order = "hclust", diag = TRUE,
  tl.col="black", tl.cex = 0.8, addCoef.col = 'black', number.cex = 0.6)

```

According to the matrix above, `GrLivArea` has the strongest linear relationship with the target `SalePrice`, followed by `TotalIntArea`, this given the collinearity with `GrLivArea`, and then `GarageArea` or `GarageCars`, both heavily correlated too, like commented before.

I'm leaving blank, i.e., without a circle, the insignificant correlations for better visualisation and focus. Now I'm checking the categorical variables and the response.

```{r}
#| label: fig-cat-corr
#| fig-width: 10
#| fig-height: 10
#| fig-cap: Pearson's correlation of categorical features and response.
#| warning: false
#| message: false

ames_all |>
  filter(dataset == "train") |>
  select(-dataset) |>
  select(where(is.factor) | contains("Price")) |>  
  mutate_if(is.factor, as.integer) |> 
  cor(method = "pearson") |>
  corrplot(type = "lower", order="hclust", diag = TRUE, insig = 'blank',
  tl.col="black", tl.cex = 0.8, number.cex = 0.6)

```

The best linear association with `SalePrice` is given by `OverallQual` factor, followed by everything else that has to do with *quality* ratings.

Here, the highest collinearity is showed between `PoolQC` and the new `ExistPool` variables. This is because they share a majority ammount of "None" and "No" values respectively in the exact same positions, therefore making the information redundant. In case of performing one-hot encoding there would be two repeated columns.

Overall, one can see clearly the **multicollinearity** clusters, which suggest the use of advanced **Feature Selection** techniques.

### Visual Exploration

Now with the previous information in hand, I'm going to visualise separately the two most important quasi-linear relationships: `GrLivArea` and `OverallQual` vs. `SalePrice`.

```{r}
#| label: fig-cont_linear
#| message: false
#| warning: false
#| column: body
#| fig-width: 6
#| fig-height: 6
#| fig-cap: 'GrvLivArea Vs. SalePrice - The most "linear" realationship between continuous variables. One can clearly notice the increase of variance as both, Living Area and Price increase, i.e., heteroskedasticity.'
#| cap-location: margin
#| fig-subcap: 
#|   - "Scatter Plot"
#|   - "Hexbin Plot"
#| layout-ncol: 2

ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = GrLivArea, y = SalePrice/1000)) +  
  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +  
  labs(x = expression("Ground Living Area ["~ft^2~"]"), 
       y = "Sale\nPrice\n['000 $]") +  
  geom_point(aes(color = OverallQual), alpha = 0.5) + theme(axis.text.y = element_blank()) +
  geom_smooth(method = "lm", formula =  y ~ splines::bs(x, 3), color = "black", size = 1.5, alpha = 0.5) +
  theme(legend.position = "bottom")

# install.packages("hexbin")
ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = GrLivArea, y = SalePrice/1000)) +  
  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +    
  geom_hex(bins = 35, colour = "seagreen") + 
  labs(x = expression("Ground Living Area ["~ft^2~"]")) +    
  theme(axis.title.y = element_blank(), legend.position = "bottom")

```

The hexbin plot reveals three cells of price and size where houses are more common in the training dataset. **There are more than 50 houses for each bin in the range of 130K, 140K and 180K USD** approximately.

On the other hand, note how **problematic are the outliers** with the spline fit! This visual allows me to *remove outliers*, i.e., the biggest and cheap houses (right data points).

```{r}
#| label: fig-RemoveOutliers
#| fig-cap: Removing two outliers from the training dataset part the spline fit is dramatically improved!
#| column: margin

ames_all |>
  filter(dataset == "train") |>
  select(c(SalePrice, GrLivArea, OverallQual, YrSold)) |>
  filter(GrLivArea > 4500) |>
  glimpse()

idx_remove <- which(ames_all$dataset == "train" & ames_all$GrLivArea > 4500)

ames_all <-
  ames_all |> 
    filter(!row_number() %in% idx_remove)

ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = GrLivArea, y = SalePrice/1000)) +  
  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +  
  labs(x = expression("Ground Living Area ["~ft^2~"]"), 
       y = "Sale Price ['000 $]") +  
  geom_point(aes(color = OverallQual), alpha = 0.5) + theme(axis.text.y = element_blank()) +
  geom_smooth(method = "lm", 
              formula =  y ~ splines::bs(x, 3), 
              color = "black", 
              size = 1.5, 
              alpha = 0.5) +
  theme(legend.position = "none", axis.title.y = element_text(angle = 90))
```

The **two big houses above were super offers!** Or did the housing bubble exploded?

In the same fashion, I want to see the effect of the factor `OverallQual`.

```{r}
#| label: fig-cat_linear
#| message: false
#| warning: false
#| fig-width: 7
#| fig-height: 8
#| fig-cap: 'OverallQual Vs. SalePrice - The most "linear" realationship with a categorical predictor. The raincloud plot shows how variances of factors are very different.'

# install.packages("ggrain")
library(ggrain)

ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = OverallQual, y = SalePrice/1000, colour = OverallQual, fill = OverallQual)) +     
  geom_rain(alpha = 0.3) +  
  scale_y_continuous(breaks = seq(0, 800, 100)) +  
  labs(x = "Overall\nQuality", y = "Sale Price ['000 $]") +
  coord_flip() + 
  theme(legend.position = "none")
```

#### I'm wondering in which price range is the most popular quality?

```{r}
#| label: fig-count_overallQual
#| cap-location: margin
#| fig-cap: 'Houses with mid-quality levels are most frequent in the train dataset. I will reduce the categories to nine (9), grouping or lumping 1 and 2 as "Other".'
#| column: margin
#| code-fold: false

ames_all |>
  filter(dataset == "train") |>
  count(OverallQual, name = "count") |>
  ggplot(aes(y = fct_reorder(OverallQual, count), x = count)) + 
  geom_col(aes(fill = OverallQual)) +
  geom_text(aes(label = count), size = 10) +
  labs(y = "Overall Quality") + 
  theme(legend.position = "none", axis.title.y = element_text(angle = 90))
```

It seems that most popular houses, with an overall quality of 5 (mid-quality level), are the ones between 120K and 145K USD. Of course, this was back in the years of 2006 - 2010.

Now I'll group or lump into one category `OverallQual = 1` and `OverallQual = 2`.

```{r}
#| label: lumping

ames_all <- 
  ames_all |>
  mutate(OverallQual = fct_lump(OverallQual, n = 8)) |>
  mutate(OverallQual = fct_relevel(OverallQual, "Other"))  
```

#### How sale prices vary with the living area and depending on house styles?

```{r}
#| label: fig-houseStyles
#| fig-column: page-right
#| fig-width: 8
#| fig-height: 8
#| fig-cap: "One and two story houses are the most common within the train dataset."

ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = GrLivArea, y = SalePrice/1000)) +  
  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +  
  labs(x = expression("Ground Living Area ["~ft^2~"]"), 
       y = "Sale\nPrice\n['000 $]") +  
  geom_point(aes(color = OverallQual), alpha = 0.3) +
  geom_smooth(method = "lm", 
              formula =  y ~ splines::bs(x, 3), 
              color = "black",
              se = FALSE,
              size = 0.7, 
              alpha = 0.5) +
  facet_wrap(~HouseStyle, nrow = 2) +
  theme(legend.position = "bottom")
```

#### How sale prices vary with the living area and depending on location?

One can hear often that **location is everything**. Let's see it here:

```{r}

#| label: fig-houseLocations
#| fig-column: page-right
#| fig-cap: "There's a lot of variation depending on location, as expected. Northridge (NoRidge), College Creek (CollgCr) and Northridge Heights (NridgHt) seem to be the most expensive neighbourhoods."

ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = GrLivArea, y = SalePrice/1000)) +  
  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +  
  labs(x = expression("Ground Living Area ["~ft^2~"]"), 
       y = "Sale\nPrice\n['000 $]") +  
  geom_point(aes(color = OverallQual), alpha = 0.3) +
  geom_smooth(method = "lm", 
              formula =  y ~ splines::bs(x, 3), 
              color = "black",
              se = FALSE,
              size = 0.7, 
              alpha = 0.5) +
  facet_wrap(~Neighborhood, nrow = 3) +
  theme(legend.position = "bottom")
```

### Analysis of response SalePrice

How well-behaved is the target?

```{r}
#| label: fig-Y
#| message: false
#| warning: false
#| column: body
#| fig-width: 6
#| fig-height: 6
#| fig-cap: "Analysis of the target variable. All the assumptions of OLS and linear regression are broken, as usual."
#| fig-subcap: 
#|   - "Histogram"
#|   - "Q-Q Plot"
#| layout-ncol: 2

ames_all |>
  filter(dataset == "train") |>
  ggplot(aes(x = SalePrice/1000)) +  
  geom_histogram(bins = 50, color = "seagreen", fill = "seagreen", alpha = 0.5) +
  scale_x_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +  
  labs(x = "Sale Price ['000 $]")

ames_all |>
  filter(dataset == "train") |> 
  ggplot(aes(sample = SalePrice/1000)) + 
  stat_qq(color = "seagreen", alpha = 0.5) + 
  stat_qq_line(color = "seagreen") +  
  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +
  labs(y = "Sale\nPrice\n['000 $]",
       x = "Theoretical Quantiles (Norm. Dist.)")
```

Clearly, the target variable is not well-behaved. It shows a log-normal distribution heavily skewed, to right of course. In order to achieve better predictions, a transformation might be needed.

#### Transformation of the response

For what I've seen, processes that have to do with *money*, e.g., sale prices, costs of products, salaries, they are most of the time **Log-normal**. Rarely there is something free, so you don't have zeros or with negative value; think about the top 1% of reach people, or anything that becomes **premium** the higher the price. These are long and sparse right tails.

The logarithmic transformation works when there are few large values, like in this case, there are only a few expensive houses. The logarithm scale will shrink that right tail[^5], making the distribution more normal-like.

[^5]: Think of a **Logarithmic transformation** like pressing an **accordeon** only from your right arm.

Nevertheless, the moment the first data transformation is performed, we start to loose interpretability very quickly. Nowadays, the most accurate predictions need more than one transformation in order to work best[^6].

[^6]: Forget about linearity and interpretability if you want the most accurate results.

Because we have powerful computers and libraries with written functions to perform variable transformations, I will use the Box-Cox transformation, which is a family of functions that includes the logarithmic one depending on the value of a parameter *lambda*. This parameter is automatically assessed.

::: column-margin
*Family Box-Cox transformations*:

<br/>

$y(\lambda) = \begin{cases} \frac{y^{-1}-1}{\lambda}, & \text{if } \lambda \neq 0 \\ \log y, & \text{if } \lambda = 0 \end{cases}$
:::

```{r}
#| label: fig-boxcox
#| fig-cap: "Best parameter lambda = -0.05050505. It is almost a pure logarithmic transformation."
#| column: margin
#| code-fold: false
#| warning: false
#| message: false

boxcox_trans <- MASS::boxcox(lm(SalePrice ~ 1, 
                                data = subset(ames_all, dataset == "train")), 
                             lambda = seq(-1, 1), 
                             plotit = TRUE)

boxcox_lambda <- as.numeric(boxcox_trans$x[which.max(boxcox_trans$y)])

# New transformed response variable
ames_all <- ames_all |>
  mutate(SalePrice_bc = (SalePrice^boxcox_lambda - 1) / boxcox_lambda)

```

The new mutated `SalePrice` should be better behaved...

```{r}
#| label: fig-Y-trans
#| message: false
#| warning: false
#| column: body
#| fig-width: 6
#| fig-height: 6
#| fig-cap: "The transformed response shows a more bell-like shape."
#| fig-subcap: 
#|   - "Histogram"
#|   - "Q-Q Plot"
#| layout-ncol: 2

ames_all |>
  filter(dataset == "train") |>  
  ggplot(aes(x = SalePrice_bc)) +  
  geom_histogram(bins = 50, color = "seagreen", fill = "seagreen", alpha = 0.5) +  
  labs(x = "Sale Price [Box-Cox]")

ames_all |>
  filter(dataset == "train") |>  
  ggplot(aes(sample = SalePrice_bc)) + 
  stat_qq(color = "seagreen", alpha = 0.5) + 
  stat_qq_line(color = "seagreen") +   
  labs(y = "Sale\nPrice\n[Box-Cox]", x = "Theoretical Quantiles (Norm. Dist.)")
```

... and it is, indeed. Well, except for the tails, now the transformed values are more normal-like at the core.

[**Do not include the transformation of the response as a step within the recipe.** It will cause an [error](https://github.com/tidymodels/recipes/issues/1033) with the *predict* function trying to locate `SalePrice` in the test dataset.]{.column-margin}

# PHASE 2: MODELLING

  *"This is where the fun begins."*

One of the motivations to develop this project is because I'm switching to the **Tidymodels** framework. Therefore, it's time to load the library to train a simple reference model.

## Drop no info columns

```{r}
#| label: noinfo_drop
#| code-fold: false

noinfo_cols <- names(which(mi_y_X_2 < 0.06444))

ames_all <-
  ames_all |>
  select(-all_of(noinfo_cols)) 
```


## Resampling Technique for Test Error Estimation

This is a critical step for final model selection. Depending on the chosen resample technique to estimate the Goodness-of-Prediction or test error, in this case metrics like RMSE can be slightly optimistic or conservative. Take a look at the pristine explanation by [@resamplingML].

-   **Simple validation set?** 1 train fold and 1 test fold: NO, it's old and inaccurate.
-   **v-folds Cross-Validation?** Analysis (training folds), Assessment (validation folds) and test: it's *small data*, I can choose something even more powerful.
-   **The Bootstrap?** YES.

By today's standards this dataset is small, therefore I will choose Bootstraping to keep the chosen samples the same size as the training set. The probability of an observation of being sample at least once is 0.63, therefore the complementary training set (around 37%) will become the *assessment set*. Although performance metrics will be slightly conservative or pessimistic, I prefer it.

I will generate 1000 bootstrap-samples.

## Expending the data budget

Normally with v-fold CV I expend my data budget, but here doing resampling with replacement is kind of printing money... Here's the code to generate the bootstraps and therefore split the data budget:

```{r}
#| label: DataBudget
#| warning: false
#| fold: false

library(tidymodels)
tidymodels_prefer()
# conflicted::conflicts_prefer(scales::discard)

# Select the pre-processed training dataset
ames_train <- 
  ames_all |> 
  filter(dataset == "train")
 

# ---- v-folds Cross Validation ----
set.seed(1982)
ames_folds <- vfold_cv(ames_train, v = 10, repeats = 10, strata = SalePrice_bc) # v = 10 folds are default

# ---- The Bootstrap ----
set.seed(1982)
ames_boots <- bootstraps(ames_train, times = 1000, strata = SalePrice_bc)
ames_boots  
```

For the sake of RMSE metric comparison I will always scale numeric predictors and use the Box-Cox transformed response `SalePrice_bc`, this given the improvement shown in @fig-Y-trans.

The following code shows preprocessing steps enclosed in a pipeline or ***recipe***. The list of available *step functions* can be found [here](https://recipes.tidymodels.org/reference/index.html).

```{r}
#| label: skewed_feats
#| fig-cap: "Correlated numerical predictors are skewed and contain zero values."

ames_train |>  
  select(c(GrLivArea, TotBaths, GarageArea, TotalBsmtSF, YearBuilt)) |> 
  pivot_longer(GrLivArea:YearBuilt) |>
  ggplot(aes(x = value)) +  
  geom_histogram(bins = 50, color = "lightgrey", fill = "lightgrey") +  
  labs(x = "") + 
  facet_wrap(~name, scales = "free")
```

Given the sensitivity to tails and outliers of linear models, remember @fig-cont_linear-1, it will be convenient to try to normalise the distribution of these predictors. However, Box-Cox transformations work only for positive data. The alternative will be the more general **Yeo-Johnson transformation**, which works for both, positive and negative values.

[Note that `SalePrice_bc` is the transformed outcome. Click [here](https://www.tmwr.org/recipes.html#skip-equals-true) for more info.]{.column-margin}

```{r}
#| label: cherry-picking_lm
#| eval: false

```

OK! We can see the power of adding good predictors in the test RMSE estimation

My reference is the **linear GAM with Splines**, showing a mean RMSE = 0.0812. The result was transformed in the pipeline, therefore no units are displayed.

[I got slightly better results using dummy encoded factors. Moreover, applying the Yeo-Johnson transformation to numeric Feats. helped too! They were skewed, see @fig-cherry_picking_feats.]{.column-margin}


## Elastic-Net Regression

Having high multicollinearty it's reasonable to think of some sort of penalisation or shrinkage method. Hence, I'll go with a Generalised Linear Model, [GLM](https://glmnet.stanford.edu/articles/glmnet.html). In this manner I can have a first look at feature importance.

I'll choose Elastic Net regression, combining the best of Lasso and Ridge. I must remember to myself that *predictors* must be scaled! I don't want to over-penalised non-scaled variables using shrinkage methods.

```{r}
#| label: Preprocessing

# Elastic Net
glmnet_base_rec <-
  recipe(SalePrice_bc ~ ., data = ames_train) |>
  update_role(Id, new_role = "Id variable") |>
  update_role(SalePrice, new_role = "original outcome") |>
  update_role(dataset, new_role = "splitting variable") |>
  step_dummy(all_nominal_predictors()) |>    
  step_YeoJohnson(all_numeric_predictors()) |>
  step_normalize(all_numeric_predictors()) |>
  step_zv(all_predictors())

# glmnet_base_rec |> prep() |> bake(new_data = NULL) 

```

### Elastic-Net Spec & Tuning

```{r}
#| label: Elastic_net-set-tune

# library(usemodels)  # CheatSheets

# Model: Elastic Net -------------
glmnet_spec <-
  linear_reg(penalty = tune(), mixture = tune()) |> 
  set_engine("glmnet") |>
  set_mode("regression")
  # translate()

glmnet_wf <-
  workflow() |>
  add_recipe(glmnet_base_rec) |>
  add_model(glmnet_spec)

# penalty = lambda; mixture = alpha
glmnet_grid <- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20), 
                               mixture = c(0.05, 0.2, 0.4, 0.6, 0.8, 1))

# Tune with Folds
set.seed(1982)
doParallel::registerDoParallel()

glmnet_tune <- tune_grid(glmnet_wf, 
                         resamples = ames_folds,  # CV is faster!
                         grid = glmnet_grid)

# Explore Results 
show_best(glmnet_tune, metric = "rmse")

# Set best parameters
final_glmnet <- glmnet_wf |>
  finalize_workflow(select_best(glmnet_tune, metric = "rmse"))

final_glmnet

# save(final_glmnet, file = "./models/final_models/final_glmnet.Rdata")

```

### Elastic-Net Test Error Assessment

```{r}
#| label: final_glmnet
#| code-fold: false

# final_glmnet <- load("./models/final_models/final_glmnet.Rdata")
# final_glmnet

glmnet_boot_fit <- 
  final_glmnet |> 
  fit_resamples(resamples = ames_boots)
  
# glmnet_boot_fit |>
#   collect_metrics(summarize = TRUE)

```

A very low penalty of $\lambda = 0.00264$ with a mixture of $\alpha = 0.6$ were the best parameters.

New fit with new feature engineering, mean RMSE = 0.0641 (1000 Bootstraps). **Kaggle's Top 15%**. Best so far...

```{r}
#| label: Elastic-Net_submission
#| eval: false

# Get best model and fit with whole training set
fit_glm_elnet <- 
  final_glmnet |>
  fit(ames_train)

# Predicting on the test set
ames_test <- 
  ames_all |>
  # select(-SalePrice) |>
  filter(dataset == "test")

res_3_elasticnet <- 
  predict(fit_glm_elnet, ames_test) |>
  mutate(SalePrice_pred = inv_boxcox(.pred, boxcox_lambda))  # orignal values in USD

# .csv for submission
sub_3_elasticnet <- data.frame(Id = ames_test$Id, SalePrice = res_3_elasticnet$SalePrice_pred)
write.csv(sub_3_elasticnet, "./data/submissions/jthom_sub_3_elasticnet.csv", quote = FALSE, row.names = FALSE)

```

### XGBoost Regression

#### XGBoost Spec & Tuning

```{r}
#| label: XGB-set-tune
#| eval: false

```

## Models Stacking

To be continue...