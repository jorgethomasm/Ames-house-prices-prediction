[
  {
    "objectID": "HousePrices.html",
    "href": "HousePrices.html",
    "title": "Prediction of House Prices in Ames",
    "section": "",
    "text": "Houses\n\n\n\n\n\n\n\nLocation\n\n\n\n\nFigure 1: Ames, Iowa - USA."
  },
  {
    "objectID": "HousePrices.html#margin-figures",
    "href": "HousePrices.html#margin-figures",
    "title": "Prediction of House Prices in Ames",
    "section": "Margin Figures",
    "text": "Margin Figures\nImages and graphics play an integral role in Tufte’s work. To place figures in the margin you can use the Quarto chunk option column: margin. For example:\n\n\nCode\n```{r}\n#| label: fig-margin\n#| fig-cap: \"MPG vs horsepower, colored by transmission.\"\n#| column: margin\n#| message: false\nlibrary(ggplot2)\nmtcars2 &lt;- mtcars\nmtcars2$am &lt;- factor(\n  mtcars$am, labels = c('automatic', 'manual')\n)\nggplot(mtcars2, aes(hp, mpg, color = am)) +\n  geom_point() + geom_smooth() +\n  theme(legend.position = 'bottom')\n```\n\n\n\n\n\n\nFigure 10: MPG vs horsepower, colored by transmission.\n\n\n\nNote the use of the fig-cap chunk option to provide a figure caption. You can adjust the proportions of figures using the fig-width and fig-height chunk options. These are specified in inches, and will be automatically scaled down to fit within the handout margin."
  },
  {
    "objectID": "HousePrices.html#arbitrary-margin-content",
    "href": "HousePrices.html#arbitrary-margin-content",
    "title": "Prediction of House Prices in Ames",
    "section": "Arbitrary Margin Content",
    "text": "Arbitrary Margin Content\nYou can include anything in the margin by places the class .column-margin on the element. See an example on the right about the first fundamental theorem of calculus.\n\n\nWe know from the first fundamental theorem of calculus that for \\(x\\) in \\([a, b]\\):\n\\[\\frac{d}{dx}\\left( \\int_{a}^{x} f(u)\\,du\\right)=f(x).\\]"
  },
  {
    "objectID": "HousePrices.html#full-width-figures",
    "href": "HousePrices.html#full-width-figures",
    "title": "Prediction of House Prices in Ames",
    "section": "Full Width Figures",
    "text": "Full Width Figures\nYou can arrange for figures to span across the entire page by using the chunk option fig-column: page-right.\n\n\nCode\n```{r}\n#| label: fig-fullwidth\n#| fig-cap: \"A full width figure.\"\n#| fig-width: 11\n#| fig-height: 3\n#| fig-column: page-right\n#| warning: false\nggplot(diamonds, aes(carat, price)) + geom_smooth() +\n  facet_grid(~ cut)\n```\n\n\n\n\n\nFigure 13: A full width figure.\n\n\n\n\nOther chunk options related to figures can still be used, such as fig-width, fig-cap, and so on. For full width figures, usually fig-width is large and fig-height is small. In the above example, the plot size is \\(11 \\times 3\\)."
  },
  {
    "objectID": "HousePrices.html#arbitrary-full-width-content",
    "href": "HousePrices.html#arbitrary-full-width-content",
    "title": "Prediction of House Prices in Ames",
    "section": "Arbitrary Full Width Content",
    "text": "Arbitrary Full Width Content\n\nR is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under the terms of the GNU General Public License versions 2 or 3. For more information about these matters see https://www.gnu.org/licenses/.\n\n\n\n\n\n\n\nThis feature depends upon link-citations to locate and place references in the margin. This is enabled by default, but if you disable link-citations then references in the HTML output will be placed at the end of the output document as they normally are.\n\n\n\n\n\n\n\n\n\nUsing R within Chunk Options\n\n\n\nIf you wish to use raw R expressions as part of the chunk options (like above), then you need to define those in the tag=value format within the curly brackets {r label, tag=value} instead of the tag: value YAML syntax on a new line starting with the hashpipe #|. The former approach is documented on knitr’s website while the latter is explained in Quarto’s documentation."
  },
  {
    "objectID": "HousePrices.html#main-column-figures",
    "href": "HousePrices.html#main-column-figures",
    "title": "Prediction of House Prices in Ames",
    "section": "Main Column Figures",
    "text": "Main Column Figures\nBesides margin and full width figures, you can of course also include figures constrained to the main column. This is the default type of figures in the LaTeX/HTML output.\n\n\nCode\n```{r}\n#| label: fig-main\n#| fig-cap: \"A figure in the main column.\"\nggplot(diamonds, aes(cut, price)) + geom_boxplot()\n```\n\n\n\n\n\nFigure 11: A figure in the main column."
  },
  {
    "objectID": "HousePrices.html#margin-captions",
    "href": "HousePrices.html#margin-captions",
    "title": "Prediction of House Prices in Ames",
    "section": "Margin Captions",
    "text": "Margin Captions\nWhen you include a figure constrained to the main column, you can choose to place the figure’s caption in the margin by using the cap-location chunk option. For example:\n\n\nCode\n```{r}\n#| label: fig-main-margin-cap\n#| fig-cap: \"A figure with a longer caption. The figure appears in the main column, but the caption is placed in the margin. Captions can even contain elements like a citation such as @R-base.\"\n#| cap-location: margin\nggplot(diamonds, aes(cut, price)) + geom_boxplot()\n```\n\n\n\n\n\nFigure 14: A figure with a longer caption. The figure appears in the main column, but the caption is placed in the margin. Captions can even contain elements like a citation such as R Core Team (2023)."
  },
  {
    "objectID": "Tufte_template_instructions.html",
    "href": "Tufte_template_instructions.html",
    "title": "Prediction of House Prices in Ames, Iowa",
    "section": "",
    "text": "This document demonstrates the use of a number of advanced page layout features to produce an attractive and usable document inspired by the Tufte handout style and the use of Tufte’s styles in RMarkdown documents. The Tufte handout style is a style that Edward Tufte uses in his books and handouts. Tufte’s style is known for its extensive use of sidenotes, tight integration of graphics with text, and well-set typography. Quarto1 supports most of the layout techniques that are used in the Tufte handout style for both HTML and LaTeX/PDF output.1 To learn more, you can read more about Quarto or visit Quarto’s Github repository.\n---\ntitle: \"An Example Using the Tufte Style\"\nauthor: \"John Smith\"\nformat:\n  html:\n    grid:\n1      margin-width: 350px\n  pdf: default\n2reference-location: margin\ncitation-location: margin\n---\n\n1\n\nIncreases the width of the margin to make more room for sidenotes and margin figures (HTML only).\n\n2\n\nPlaces footnotes and cited sources in the margin. Other layout options (for example placing a figure in the margin) will be set per element in examples below.\n\n\nThese layout features are designed with two important goals in mind:\n\nTo produce both PDF and HTML output with similar styles from the same Quarto document;\nTo provide simple syntax to write elements of the Tufte style such as side notes and margin figures. If you’d like a figure placed in the margin, just set the option fig-column: margin for your code chunk, and we will take care of the details for you2.\n\n2 You never need to think about \\begin{marginfigure} or &lt;span class=\"marginfigure\"&gt;; the LaTeX and HTML code under the hood may be complicated, but you never need to learn or write such code.If you have any feature requests or find bugs in these capabilities, please do not hesitate to file them to https://github.com/quarto-dev/quarto-cli/issues."
  },
  {
    "objectID": "Tufte_template_instructions.html#margin-figures",
    "href": "Tufte_template_instructions.html#margin-figures",
    "title": "Prediction of House Prices in Ames, Iowa",
    "section": "Margin Figures",
    "text": "Margin Figures\nImages and graphics play an integral role in Tufte’s work. To place figures in the margin you can use the Quarto chunk option column: margin. For example:\n\n\nCode\n```{r}\n#| label: fig-margin\n#| fig-cap: \"MPG vs horsepower, colored by transmission.\"\n#| column: margin\n#| message: false\nlibrary(ggplot2)\nmtcars2 &lt;- mtcars\nmtcars2$am &lt;- factor(\n  mtcars$am, labels = c('automatic', 'manual')\n)\nggplot(mtcars2, aes(hp, mpg, color = am)) +\n  geom_point() + geom_smooth() +\n  theme(legend.position = 'bottom')\n```\n\n\n\n\n\n\nFigure 1: MPG vs horsepower, colored by transmission.\n\n\n\nNote the use of the fig-cap chunk option to provide a figure caption. You can adjust the proportions of figures using the fig-width and fig-height chunk options. These are specified in inches, and will be automatically scaled down to fit within the handout margin."
  },
  {
    "objectID": "Tufte_template_instructions.html#arbitrary-margin-content",
    "href": "Tufte_template_instructions.html#arbitrary-margin-content",
    "title": "Prediction of House Prices in Ames, Iowa",
    "section": "Arbitrary Margin Content",
    "text": "Arbitrary Margin Content\nYou can include anything in the margin by places the class .column-margin on the element. See an example on the right about the first fundamental theorem of calculus.\n\n\nWe know from the first fundamental theorem of calculus that for \\(x\\) in \\([a, b]\\):\n\\[\\frac{d}{dx}\\left( \\int_{a}^{x} f(u)\\,du\\right)=f(x).\\]"
  },
  {
    "objectID": "Tufte_template_instructions.html#full-width-figures",
    "href": "Tufte_template_instructions.html#full-width-figures",
    "title": "Prediction of House Prices in Ames, Iowa",
    "section": "Full Width Figures",
    "text": "Full Width Figures\nYou can arrange for figures to span across the entire page by using the chunk option fig-column: page-right.\n\n\nCode\n```{r}\n#| label: fig-fullwidth\n#| fig-cap: \"A full width figure.\"\n#| fig-width: 11\n#| fig-height: 3\n#| fig-column: page-right\n#| warning: false\nggplot(diamonds, aes(carat, price)) + geom_smooth() +\n  facet_grid(~ cut)\n```\n\n\n\n\n\nFigure 2: A full width figure.\n\n\n\n\nOther chunk options related to figures can still be used, such as fig-width, fig-cap, and so on. For full width figures, usually fig-width is large and fig-height is small. In the above example, the plot size is \\(11 \\times 3\\)."
  },
  {
    "objectID": "Tufte_template_instructions.html#arbitrary-full-width-content",
    "href": "Tufte_template_instructions.html#arbitrary-full-width-content",
    "title": "Prediction of House Prices in Ames, Iowa",
    "section": "Arbitrary Full Width Content",
    "text": "Arbitrary Full Width Content\nAny content can span to the full width of the page, simply place the element in a div and add the class column-page-right. For example, the following code will display its contents as full width.\n::: {.fullwidth}\nAny _full width_ content here.\n:::\nBelow is an example:\n\nR is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under the terms of the GNU General Public License versions 2 or 3. For more information about these matters see https://www.gnu.org/licenses/."
  },
  {
    "objectID": "Tufte_template_instructions.html#main-column-figures",
    "href": "Tufte_template_instructions.html#main-column-figures",
    "title": "Prediction of House Prices in Ames, Iowa",
    "section": "Main Column Figures",
    "text": "Main Column Figures\nBesides margin and full width figures, you can of course also include figures constrained to the main column. This is the default type of figures in the LaTeX/HTML output.\n\n\nCode\n```{r}\n#| label: fig-main\n#| fig-cap: \"A figure in the main column.\"\nggplot(diamonds, aes(cut, price)) + geom_boxplot()\n```\n\n\n\n\n\nFigure 3: A figure in the main column."
  },
  {
    "objectID": "Tufte_template_instructions.html#margin-captions",
    "href": "Tufte_template_instructions.html#margin-captions",
    "title": "Prediction of House Prices in Ames, Iowa",
    "section": "Margin Captions",
    "text": "Margin Captions\nWhen you include a figure constrained to the main column, you can choose to place the figure’s caption in the margin by using the cap-location chunk option. For example:\n\n\nCode\n```{r}\n#| label: fig-main-margin-cap\n#| fig-cap: \"A figure with a longer caption. The figure appears in the main column, but the caption is placed in the margin. Captions can even contain elements like a citation such as @R-base.\"\n#| cap-location: margin\nggplot(diamonds, aes(cut, price)) + geom_boxplot()\n```\n\n\n\n\n\nFigure 4: A figure with a longer caption. The figure appears in the main column, but the caption is placed in the margin. Captions can even contain elements like a citation such as R Core Team (2023)."
  },
  {
    "objectID": "HousePrices.html#missing-values",
    "href": "HousePrices.html#missing-values",
    "title": "Prediction of House Prices in Ames",
    "section": "Missing Values",
    "text": "Missing Values\nFirst things first, read the full data description1. The manifest is available here. There you’ll find that for several columns missing values NA means actually “None”. The physical absence of a determined feature in a house is a category exposing the lack of such quality that can have a significant impact on the Sale Price (response).1 Tip: print a copy, make notes and study it. This is the first step to get into domain knowledge for Feature Engineering in the modelling phase.\nTherefore, I will fill the empty NA fields of the indicated columns of both datasets with the string “None”.\n\n\nCode\n```{r}\n#| label: easy replace_na\n\ncols_NA_to_none &lt;- list(\n  Alley = \"None\",\n  BsmtQual = \"None\", BsmtCond = \"None\", BsmtExposure = \"None\", BsmtFinType1 = \"None\", BsmtFinType2 = \"None\", \n  FireplaceQu = \"None\", \n  GarageType = \"None\", GarageFinish = \"None\", GarageQual = \"None\", GarageCond = \"None\", \n  PoolQC = \"None\",\n  Fence = \"None\", \n  MiscFeature = \"None\")\n\ndf_all &lt;- df_all |&gt;\n  replace_na(cols_NA_to_none) \n  \ndf_sub &lt;- df_sub |&gt;\n  replace_na(cols_NA_to_none)\n```\n\n\nOne of the early and recurrent steps of the EDA is to check the completeness of data. Let’s search for missing values, after filling indicated fields2. For this case I wrote the function count_na() that generates the tables displayed on the right margin.2 Tip: write a function to quantify missing values depending on the language and labelling system used.\n\n```{r}\n#| label: Counting NAs\n#| code-fold: false\n#| column: margin\n\ndf_all |&gt; \n  count_na() |&gt;  \n  knitr::kable(caption = 'Training dataset')\n\ndf_sub |&gt; \n  count_na() |&gt;  \n  knitr::kable(caption = 'Submission dataset')  \n```\n\n\n\nTraining dataset\n\n\nVariable\nNA_count\nPercent\n\n\n\n\nLotFrontage\n259\n17.74\n\n\nGarageYrBlt\n81\n5.55\n\n\nMasVnrType\n8\n0.55\n\n\nMasVnrArea\n8\n0.55\n\n\nElectrical\n1\n0.07\n\n\n\n\nSubmission dataset\n\n\nVariable\nNA_count\nPercent\n\n\n\n\nLotFrontage\n227\n15.56\n\n\nGarageYrBlt\n78\n5.35\n\n\nMasVnrType\n16\n1.10\n\n\nMasVnrArea\n15\n1.03\n\n\nMSZoning\n4\n0.27\n\n\nUtilities\n2\n0.14\n\n\nBsmtFullBath\n2\n0.14\n\n\nBsmtHalfBath\n2\n0.14\n\n\nFunctional\n2\n0.14\n\n\nExterior1st\n1\n0.07\n\n\nExterior2nd\n1\n0.07\n\n\nBsmtFinSF1\n1\n0.07\n\n\nBsmtFinSF2\n1\n0.07\n\n\nBsmtUnfSF\n1\n0.07\n\n\nTotalBsmtSF\n1\n0.07\n\n\nKitchenQual\n1\n0.07\n\n\nGarageCars\n1\n0.07\n\n\nGarageArea\n1\n0.07\n\n\nSaleType\n1\n0.07\n\n\n\n\nI will fill the remaining missing values as follow:\n\nThe LotFrontage column refers to the linear feet of street connected to the house. This feature is not well documented and the missing percentage related to other variables makes it unreliable, therefore, I will delete the feature.\nEverytime there is no garage, i.e., GarageType = \"None\", there is the corresponding missing value in Garage Year Built GarageYrBlt = NA. I will engineer a new feature called GarageNew with three ordinal categories: None, No, Yes. This based on the delta of YearBuilt - GarageYrBlt; I expect that given the house price, the algorithm will learn that “None” is worse than “No” and so on. Then I will remove the GarageYrBlt predictor.\nFor the rest of variables with a 1 % or less NA, I’ll calculate the median (if numerical) and the mode (if categorical) in order to fill them with it.\n\nHere’s my pipeline for both datasets:\n\n\nCode\n```{r}\n#| label: terminating NAs\n\n# \"replace_na_with_median\" is a custom function\n\ndf_all &lt;- df_all |&gt;\n  mutate(LotFrontage = NULL) |&gt;\n  mutate(GarageNew = if_else(YearBuilt - GarageYrBlt &gt; 0, \"Yes\", \"No\")) |&gt;\n  replace_na(list(GarageNew = \"None\")) |&gt;\n  mutate(GarageNew = factor(GarageNew, levels = c(\"None\", \"No\", \"Yes\"))) |&gt;\n  mutate(GarageYrBlt = NULL) |&gt;\n  mutate_if(is.numeric, replace_na_with_median)\n\ndf_sub &lt;- df_sub |&gt;\n  mutate(LotFrontage = NULL) |&gt;\n  mutate(GarageNew = if_else(YearBuilt - GarageYrBlt &gt; 0, \"Yes\", \"No\")) |&gt;\n  replace_na(list(GarageNew = \"None\")) |&gt;\n  mutate(GarageNew = factor(GarageNew, levels = c(\"None\", \"No\", \"Yes\"))) |&gt;\n  mutate(GarageYrBlt = NULL) |&gt;\n  mutate_if(is.numeric, replace_na_with_median)\n```\n\n\nLet’s get a list with the mode of each remaining columns containing missing values NA.\n\n\nCode\n```{r}\n#| label: Mode for NAs\n\n# Get the mode for reamaining columns with NAs:\n\n# Good and old-fashion code: apply(df_all, 2, find_mode)\n\nls_na_mode_all &lt;- df_all |&gt;\n  select(MasVnrType, MasVnrArea, Electrical) |&gt;\n  map(find_mode)\n\nls_na_mode_sub &lt;- df_sub |&gt; \n  select(MasVnrType, MasVnrArea, MSZoning, Utilities,   BsmtFullBath,   BsmtHalfBath,   Functional, Exterior1st,    Exterior2nd,    BsmtFinSF1, BsmtFinSF2, BsmtUnfSF,  TotalBsmtSF,    KitchenQual,    GarageCars, GarageArea, SaleType) |&gt;\n  map(find_mode)\n\n# Replace with the created named-lists\ndf_all &lt;- df_all |&gt;\n  replace_na(ls_na_mode_all) \n  \ndf_sub &lt;- df_sub |&gt;\n  replace_na(ls_na_mode_sub)\n\n# Check for last time for missing values:\nprint(\"Training dataset\")\ndf_all |&gt;\n  count_na()\n\nprint(\"Submission dataset\")\ndf_sub |&gt;\n  count_na()\n```\n\n\n[1] \"Training dataset\"\n[1] \"No missing values (NA) found.\"\n[1] \"Submission dataset\"\n[1] \"No missing values (NA) found.\"\n\n\n“The data is complete!” Let’s think about predictors."
  },
  {
    "objectID": "HousePrices.html#extract-data",
    "href": "HousePrices.html#extract-data",
    "title": "Prediction of House Prices in Ames",
    "section": "Extract Data",
    "text": "Extract Data\nI checked beforehand that there are no missing values, here NA, in the target variable SalePrice. Therefore, I will write a pipeline to read and concatenate both datasets (bind rows), adding and extra column dataset to label as “train” and “test” for further easy splitting1 (subset or filter).1 The whole Feature Transformation pipeline most be always the same for all predictors in both datasets.\n\n\nCode\n```{r}\n#| label: Load Data\n#| warning: false\n\names_train_raw &lt;- read_csv(\"./data/raw/train.csv\") # Train, validattion and test dataset\nprint(\"Dimensions of training dataset\")\ndim(ames_train_raw)\n\names_test_raw &lt;- read_csv(\"./data/raw/test.csv\")  # Features for submission dataset\nprint(\"Dimensions of test dataset containing only Feats.\")\ndim(ames_test_raw) \n\n# Add Target column with NA so both DFs can be concatenated:.id\names_test_raw &lt;- \n  ames_test_raw |&gt; \n    mutate(SalePrice = NA)\n\n# Binding and adding identifier column \"dataset\" \names_all &lt;- \n  bind_rows(list(train = ames_train_raw, test = ames_test_raw), .id = \"dataset\")\n\nprint(\"Available variables:\")\nnames(ames_all)\n```\n\n\n[1] \"Dimensions of training dataset\"\n[1] 1460   81\n[1] \"Dimensions of test dataset containing only Feats.\"\n[1] 1459   80\n[1] \"Available variables:\"\n [1] \"dataset\"       \"Id\"            \"MSSubClass\"    \"MSZoning\"     \n [5] \"LotFrontage\"   \"LotArea\"       \"Street\"        \"Alley\"        \n [9] \"LotShape\"      \"LandContour\"   \"Utilities\"     \"LotConfig\"    \n[13] \"LandSlope\"     \"Neighborhood\"  \"Condition1\"    \"Condition2\"   \n[17] \"BldgType\"      \"HouseStyle\"    \"OverallQual\"   \"OverallCond\"  \n[21] \"YearBuilt\"     \"YearRemodAdd\"  \"RoofStyle\"     \"RoofMatl\"     \n[25] \"Exterior1st\"   \"Exterior2nd\"   \"MasVnrType\"    \"MasVnrArea\"   \n[29] \"ExterQual\"     \"ExterCond\"     \"Foundation\"    \"BsmtQual\"     \n[33] \"BsmtCond\"      \"BsmtExposure\"  \"BsmtFinType1\"  \"BsmtFinSF1\"   \n[37] \"BsmtFinType2\"  \"BsmtFinSF2\"    \"BsmtUnfSF\"     \"TotalBsmtSF\"  \n[41] \"Heating\"       \"HeatingQC\"     \"CentralAir\"    \"Electrical\"   \n[45] \"1stFlrSF\"      \"2ndFlrSF\"      \"LowQualFinSF\"  \"GrLivArea\"    \n[49] \"BsmtFullBath\"  \"BsmtHalfBath\"  \"FullBath\"      \"HalfBath\"     \n[53] \"BedroomAbvGr\"  \"KitchenAbvGr\"  \"KitchenQual\"   \"TotRmsAbvGrd\" \n[57] \"Functional\"    \"Fireplaces\"    \"FireplaceQu\"   \"GarageType\"   \n[61] \"GarageYrBlt\"   \"GarageFinish\"  \"GarageCars\"    \"GarageArea\"   \n[65] \"GarageQual\"    \"GarageCond\"    \"PavedDrive\"    \"WoodDeckSF\"   \n[69] \"OpenPorchSF\"   \"EnclosedPorch\" \"3SsnPorch\"     \"ScreenPorch\"  \n[73] \"PoolArea\"      \"PoolQC\"        \"Fence\"         \"MiscFeature\"  \n[77] \"MiscVal\"       \"MoSold\"        \"YrSold\"        \"SaleType\"     \n[81] \"SaleCondition\" \"SalePrice\"    \n\n\nIn order to organise my Data Budget I count on the train dataset with 1460 Observations. Note that Id and the just added dataset columns are not predictors. Hence, there are 78 features to play with. Also remember that the number of features for modelling will vary. Some will be discarded and some will be created along the analysis."
  },
  {
    "objectID": "HousePrices.html#feature-engineering",
    "href": "HousePrices.html#feature-engineering",
    "title": "Prediction of House Prices in Ames",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nI have already started with the creation of a new predictor in the previous step, this to substitute the problematic variable GarageYrBlt that had a lot of missing values.\nFeature creation and transformations are not sequential, i.e., this is not the only step where Feature Engineering is applied. Think about the next modelling phase, where you want to reduce features, e.g., where new Principal Components are the new aggregators of original features.\nIn this part I will establish which variables are categorical and numerical; this is not always as obvious as it seems4.4 Integer variables like calendar Years can be treated as categories for the analysis.\nThe first easy thing that comes to mind is to add the total area of the property as a new column named TotAreaSF; SF means Squared Feet.\nAfter that, I check the data documentation to see which variables can be thought as factors; this can be tedious if Real Estate is not your domain of expertise.\nAfter 1.5 hours of reading, googling, understanding other notebooks and deliberating (yes, it was tedious), I can summarise four types of ways to engineer new features depending on the nature of the variables:\n\nAggregation to a total of continuous-numerical surface area (Square Feet), e.g. new TotalIntArea Feat. summing basement and floors, etc. Same for a new TotalExtArea. Then deleting the addends variables.\nAggregation to a total of counts-numerical of an available installation of a determined type, e.g. new TotBaths. Then deleting the addends.\nCategorisation with ascending quality levels of the originally numerical variables OverallQual and OverallCond.\nCreation of new binary features that state if a determined installation exists or doesn’t.\n\nAfter my second Kaggle submission (Top 25%) I completed the Feature Engineering course that has a lot of tips for this projects. I included some new features and tricks from there\nHere’s the code that complies with the four previous steps:\n\n\nCode\n```{r}\n#| label: Feature Engineering\n\n# --- Total Aggregations ----\names_all &lt;- ames_all |&gt;\n\n  # Continuous numerical ----\n  # NEW TotalInteriorArea\n  mutate(TotalIntArea = BsmtFinSF1 + BsmtFinSF2 + FirstFlrSF + SecondFlrSF) |&gt;\n  select(-c(BsmtFinSF1, BsmtFinSF2))|&gt; # Bye bye...\n\n  # NEW HouseAge\n  # mutate(HouseAge = YrSold - YearBuilt) |&gt;  # Same as YearBuilt\n\n  # NEW TotalExteriorArea\n  mutate(TotalExtArea = WoodDeckSF + OpenPorchSF + EnclosedPorch + threessnporch + ScreenPorch) |&gt;\n  select(-c(WoodDeckSF, OpenPorchSF, EnclosedPorch, `threessnporch`, ScreenPorch)) |&gt; # Bye bye...\n\n  # Counting numerical ----\n  # NEW TotalBaths\n  mutate(TotBaths = BsmtFullBath + 0.5 * BsmtHalfBath + FullBath + 0.5 * HalfBath) |&gt; \n  select(-c(BsmtFullBath, BsmtHalfBath, FullBath, HalfBath)) # Bye bye...\n\n\n# ---- Ordered Categorical Features ----\n\n# Check which columns have \"None\"\n# sapply(ames_all, function(x) sum(x == \"None\"))\n\names_all &lt;- ames_all |&gt;\n  # 10 Levels:\n  mutate(OverallQual = factor(OverallQual)) |&gt;\n  mutate(OverallCond = factor(OverallCond)) |&gt; \n\n  # 5 Levels + None\n  mutate(ExterQual = factor(ExterQual, c(\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"))) |&gt;\n  mutate(ExterCond = factor(ExterCond, c(\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"))) |&gt;\n  mutate(BsmtQual = factor(BsmtQual, c(\"None\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"))) |&gt;\n  mutate(BsmtCond = factor(BsmtCond, c(\"None\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"))) |&gt;\n  mutate(HeatingQC = factor(HeatingQC, c(\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"))) |&gt;\n  mutate(KitchenQual = factor(KitchenQual, c(\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"))) |&gt;\n  mutate(FireplaceQu = factor(FireplaceQu, c(\"None\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"))) |&gt;\n  mutate(GarageQual = factor(GarageQual, c(\"None\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"))) |&gt;\n  mutate(GarageCond = factor(GarageCond, c(\"None\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"))) |&gt;\n  mutate(PoolQC = factor(PoolQC, c(\"None\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"))) |&gt;\n\n  #  Others:\n  mutate(LotShape = factor(LotShape, c(\"Reg\", \"IR1\", \"IR2\", \"IR3\"))) |&gt;\n  mutate(LandSlope = factor(LandSlope, c(\"Sev\", \"Mod\", \"Gtl\"))) |&gt;\n  mutate(BsmtExposure = factor(BsmtExposure, c(\"None\", \"No\", \"Mn\", \"Av\", \"Gd\"))) |&gt;\n  mutate(BsmtFinType1 = factor(BsmtFinType1, c(\"None\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"))) |&gt;\n  mutate(BsmtFinType2 = factor(BsmtFinType2, c(\"None\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"))) |&gt;\n  mutate(Functional = factor(Functional, c(\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"))) |&gt;\n  mutate(GarageFinish = factor(GarageFinish, c(\"None\", \"Unf\", \"RFn\", \"Fin\"))) |&gt;\n  mutate(PavedDrive = factor(PavedDrive, c(\"N\", \"P\", \"Y\"))) |&gt;\n  mutate(Utilities = factor(Utilities, c(\"NoSeWa\", \"NoSewr\", \"AllPub\"))) |&gt;  \n  mutate(Electrical = factor(Electrical, c(\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"))) |&gt;\n  mutate(Fence = factor(Fence, c(\"None\", \"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\")))\n\n\n# ---- New Binary Features ----\n\n# Does the installation or Feature exist? \names_all &lt;- ames_all |&gt;\n  # CentralAir\n  mutate(CentralAir = if_else(CentralAir == \"N\", \"No\", \"Yes\")) |&gt;\n  mutate(CentralAir = fct_relevel(CentralAir, \"No\", \"Yes\"))|&gt;\n\n  # NEW IsRemodelled (it will replace YearRemodAdd)\n  mutate(IsRemodelled = if_else(YearRemodAdd - YearBuilt &gt; 0, \"Yes\", \"No\")) |&gt;\n  mutate(IsRemodelled = factor(IsRemodelled, levels = c(\"No\", \"Yes\"))) |&gt;\n  select(-YearRemodAdd) |&gt; # Bye YearRemodAdd\n\n  # NEW ExistPool  \n  # mutate(ExistPool = if_else(PoolArea == 0, \"No\", \"Yes\")) |&gt;\n  # mutate(ExistPool = fct_relevel(ExistPool, \"No\", \"Yes\"))|&gt;\n  # a dummy of PoolQC will make this Feat. redundant!\n\n  # NEW ExistGarage\n  mutate(ExistGarage = if_else(GarageArea == 0, \"No\", \"Yes\")) |&gt;\n  mutate(ExistGarage = fct_relevel(ExistGarage, \"No\", \"Yes\"))|&gt;\n\n  # NEW ExistBsmt\n  mutate(ExistBsmt = if_else(TotalBsmtSF == 0, \"No\", \"Yes\")) |&gt;\n  mutate(ExistBsmt = fct_relevel(ExistBsmt, \"No\", \"Yes\"))|&gt;\n\n  # NEW Exist2ndFloor\n  mutate(Exist2ndFloor = if_else(`SecondFlrSF` == 0, \"No\", \"Yes\")) |&gt;  \n  mutate(Exist2ndFloor = fct_relevel(Exist2ndFloor, \"No\", \"Yes\")) |&gt;\n\n  # NEW ExistFireplace\n  mutate(ExistFireplace = if_else(Fireplaces == 0, \"No\", \"Yes\")) |&gt;\n  mutate(ExistFireplace = fct_relevel(ExistFireplace, \"No\", \"Yes\")) |&gt;\n\n  # NEW ExistMasVeneer: Masonry veneer area\n  mutate(ExistMasVeneer = if_else(MasVnrArea &gt; 0, \"Yes\", \"No\")) |&gt;\n  mutate(ExistMasVeneer = factor(ExistMasVeneer, levels = c(\"No\", \"Yes\"))) |&gt;\n\n  # From Kaggle's Feature Engineering course ---- \n\n  # New ratios\n  mutate(LivLotRatio = GrLivArea/LotArea) |&gt;\n  mutate(Spaciousness = (FirstFlrSF + SecondFlrSF) / TotRmsAbvGrd) |&gt;\n\n  # New interactions (by me!)\n  mutate(OverallQuCoInt = as.numeric(OverallQual) * as.numeric(OverallCond))\n\n\n \n \n\n# ---- Unordered factors ----\n\n# Nominative (unordered) categorical Feats\ncats_feats &lt;- c(\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"LotConfig\", \n                \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \n                \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \n                \"Foundation\", \"Heating\", \"CentralAir\", \"GarageType\", \"MiscFeature\", \n                \"SaleType\", \"SaleCondition\")\n\names_all &lt;- ames_all |&gt;\n  #mutate(across(where(is.character), factor)) |&gt;\n  mutate(across(all_of(cats_feats), factor))\n  \n\n\n# ---- Numerical Features ----\n\n# BedroomabvGr (count) left numerical \n# Kitchens (count) left numerical \n# TotRmsAbvGrd (count) left numerical\n# Fireplaces (count) left numerical\n# GarageCars (count) left numerical\n# GarageArea (continous) left numerical \n# MiscVal left numerical \n# MasVnrArea (continous) left numeric\n\n\n# ---- Remove 'LowQualFinSF' ----\n\n# Low quality finished square feet (all floors)\n# Most of the values are 0 -&gt; almost 0 variance. \n\names_all$LowQualFinSF &lt;- NULL\names_all$PoolArea &lt;- NULL\n```\n\n\nNote that I replaced YearRemodAdd to a simpler binary IsRemodelled variable, similar to what I did with the new GarageNew column.\n\nRecheck mutual info\n\n\nCode\n```{r}\n#| label: recheck_mi\n\n# \"calc_mi\" is a function I wrote.\n\nmi_y_X_2 &lt;- ames_all |&gt;\n  filter(dataset == \"train\") |&gt;\n  select(-c(Id, dataset)) |&gt;\n  calc_mi_score(target = \"SalePrice\")\n\nmi_y_X_2 |&gt; \n  head(40) |&gt;\n  knitr::kable(caption = \"Mutual Info\")\n```\n\n\n\nMutual Info\n\n\n\nx\n\n\n\n\nSalePrice\n2.6515433\n\n\nOverallQual\n0.6086855\n\n\nNeighborhood\n0.6084916\n\n\nGrLivArea\n0.5723431\n\n\nGarageArea\n0.5244219\n\n\nTotalIntArea\n0.5243177\n\n\nYearBuilt\n0.5195234\n\n\nOverallQuCoInt\n0.4865985\n\n\nTotalBsmtSF\n0.4473484\n\n\nFirstFlrSF\n0.4141527\n\n\nSpaciousness\n0.4133356\n\n\nTotBaths\n0.4108588\n\n\nGarageCars\n0.3805914\n\n\nSecondFlrSF\n0.3735544\n\n\nBsmtUnfSF\n0.3544288\n\n\nMSSubClass\n0.3458352\n\n\nBsmtQual\n0.3425788\n\n\nExterQual\n0.3394960\n\n\nKitchenQual\n0.3283328\n\n\nTotalExtArea\n0.3200495\n\n\nGarageFinish\n0.2784786\n\n\nTotRmsAbvGrd\n0.2704474\n\n\nMasVnrArea\n0.2657185\n\n\nGarageType\n0.2532877\n\n\nFireplaceQu\n0.2415506\n\n\nLivLotRatio\n0.2349274\n\n\nExterior2nd\n0.2215173\n\n\nFoundation\n0.2196649\n\n\nExterior1st\n0.2101311\n\n\nBsmtFinType1\n0.2022374\n\n\nFireplaces\n0.1891852\n\n\nExistFireplace\n0.1668687\n\n\nHeatingQC\n0.1617006\n\n\nLotArea\n0.1559991\n\n\nOverallCond\n0.1468085\n\n\nMSZoning\n0.1366905\n\n\nHouseStyle\n0.1343851\n\n\nMasVnrType\n0.1305975\n\n\nBsmtExposure\n0.1191639\n\n\nBedroomAbvGr\n0.1049602\n\n\n\n\n\nOK! Let’s move on.\n\n\nSearching for simple correlations\nAlthough nothing is linear, is good to have an idea of simple correlations between all variables; it gives me an idea of what kind of plots to make. After going through the previous steps, it’s obvious that there are correlations. Think about the number of cars that fit in a garage GarageCars and the area of the garage GarageArea.\n\n\nCode\n```{r}\n#| label: fig-num-corr\n#| fig-width: 8\n#| fig-height: 8\n#| fig-cap: The Pearson's correlation of numeric features and response shows high presence of multicollinearity.\n#| warning: false\n#| message: false\n\nlibrary(corrplot)\n\names_all |&gt;\n  filter(dataset == \"train\") |&gt;\n  select(-c(dataset, Id)) |&gt;\n  select(where(is.numeric)) |&gt;  \n  cor(method = \"pearson\") |&gt;\n  corrplot(type = \"lower\", method = \"circle\", insig = 'blank', order = \"hclust\", diag = TRUE,\n  tl.col=\"black\", tl.cex = 0.8, addCoef.col = 'black', number.cex = 0.6)\n```\n\n\n\n\n\nFigure 2: The Pearson’s correlation of numeric features and response shows high presence of multicollinearity.\n\n\n\n\nAccording to the matrix above, GrLivArea has the strongest linear relationship with the target SalePrice, followed by TotalIntArea, this given the collinearity with GrLivArea, and then GarageArea or GarageCars, both heavily correlated too, like commented before.\nI’m leaving blank, i.e., without a circle, the insignificant correlations for better visualisation and focus. Now I’m checking the categorical variables and the response.\n\n\nCode\n```{r}\n#| label: fig-cat-corr\n#| fig-width: 10\n#| fig-height: 10\n#| fig-cap: Pearson's correlation of categorical features and response.\n#| warning: false\n#| message: false\n\names_all |&gt;\n  filter(dataset == \"train\") |&gt;\n  select(-dataset) |&gt;\n  select(where(is.factor) | contains(\"Price\")) |&gt;  \n  mutate_if(is.factor, as.integer) |&gt; \n  cor(method = \"pearson\") |&gt;\n  corrplot(type = \"lower\", order=\"hclust\", diag = TRUE, insig = 'blank',\n  tl.col=\"black\", tl.cex = 0.8, number.cex = 0.6)\n```\n\n\n\n\n\nFigure 3: Pearson’s correlation of categorical features and response.\n\n\n\n\nThe best linear association with SalePrice is given by OverallQual factor, followed by everything else that has to do with quality ratings.\nHere, the highest collinearity is showed between PoolQC and the new ExistPool variables. This is because they share a majority ammount of “None” and “No” values respectively in the exact same positions, therefore making the information redundant. In case of performing one-hot encoding there would be two repeated columns.\nOverall, one can see clearly the multicollinearity clusters, which suggest the use of advanced Feature Selection techniques.\n\n\nVisual Exploration\nNow with the previous information in hand, I’m going to visualise separately the two most important quasi-linear relationships: GrLivArea and OverallQual vs. SalePrice.\n\n\nCode\n```{r}\n#| label: fig-cont_linear\n#| message: false\n#| warning: false\n#| column: body\n#| fig-width: 6\n#| fig-height: 6\n#| fig-cap: 'GrvLivArea Vs. SalePrice - The most \"linear\" realationship between continuous variables. One can clearly notice the increase of variance as both, Living Area and Price increase, i.e., heteroskedasticity.'\n#| cap-location: margin\n#| fig-subcap:\n#|   - \"Scatter Plot\"\n#|   - \"Hexbin Plot\"\n#| layout-ncol: 2\n\names_all |&gt;\n  filter(dataset == \"train\") |&gt;\n  ggplot(aes(x = GrLivArea, y = SalePrice/1000)) +  \n  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +  \n  labs(x = expression(\"Ground Living Area [\"~ft^2~\"]\"), \n       y = \"Sale\\nPrice\\n['000 $]\") +  \n  geom_point(aes(color = OverallQual), alpha = 0.5) + theme(axis.text.y = element_blank()) +\n  geom_smooth(method = \"lm\", formula =  y ~ splines::bs(x, 3), color = \"black\", size = 1.5, alpha = 0.5) +\n  theme(legend.position = \"bottom\")\n\n# install.packages(\"hexbin\")\names_all |&gt;\n  filter(dataset == \"train\") |&gt;\n  ggplot(aes(x = GrLivArea, y = SalePrice/1000)) +  \n  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +    \n  geom_hex(bins = 35, colour = \"seagreen\") + \n  labs(x = expression(\"Ground Living Area [\"~ft^2~\"]\")) +    \n  theme(axis.title.y = element_blank(), legend.position = \"bottom\")\n```\n\n\n\n\n\n\n\n\n(a) Scatter Plot\n\n\n\n\n\n\n\n(b) Hexbin Plot\n\n\n\n\nFigure 4: GrvLivArea Vs. SalePrice - The most “linear” realationship between continuous variables. One can clearly notice the increase of variance as both, Living Area and Price increase, i.e., heteroskedasticity.\n\n\n\nThe hexbin plot reveals three cells of price and size where houses are more common in the training dataset. There are more than 50 houses for each bin in the range of 130K, 140K and 180K USD approximately.\nOn the other hand, note how problematic are the outliers with the spline fit! This visual allows me to remove outliers, i.e., the biggest and cheap houses (right data points).\n\n\nCode\n```{r}\n#| label: fig-RemoveOutliers\n#| fig-cap: Removing two outliers from the training dataset part the spline fit is dramatically improved!\n#| column: margin\n\names_all |&gt;\n  filter(dataset == \"train\") |&gt;\n  select(c(SalePrice, GrLivArea, OverallQual, YrSold)) |&gt;\n  filter(GrLivArea &gt; 4500) |&gt;\n  glimpse()\n\nidx_remove &lt;- which(ames_all$dataset == \"train\" & ames_all$GrLivArea &gt; 4500)\n\names_all &lt;-\n  ames_all |&gt; \n    filter(!row_number() %in% idx_remove)\n\names_all |&gt;\n  filter(dataset == \"train\") |&gt;\n  ggplot(aes(x = GrLivArea, y = SalePrice/1000)) +  \n  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +  \n  labs(x = expression(\"Ground Living Area [\"~ft^2~\"]\"), \n       y = \"Sale Price ['000 $]\") +  \n  geom_point(aes(color = OverallQual), alpha = 0.5) + theme(axis.text.y = element_blank()) +\n  geom_smooth(method = \"lm\", \n              formula =  y ~ splines::bs(x, 3), \n              color = \"black\", \n              size = 1.5, \n              alpha = 0.5) +\n  theme(legend.position = \"none\", axis.title.y = element_text(angle = 90))\n```\n\n\nRows: 2\nColumns: 4\n$ SalePrice   &lt;dbl&gt; 184750, 160000\n$ GrLivArea   &lt;dbl&gt; 4676, 5642\n$ OverallQual &lt;fct&gt; 10, 10\n$ YrSold      &lt;dbl&gt; 2007, 2008\n\n\n\n\n\n\nFigure 5: Removing two outliers from the training dataset part the spline fit is dramatically improved!\n\n\n\nThe two big houses above were super offers! Or did the housing bubble exploded?\nIn the same fashion, I want to see the effect of the factor OverallQual.\n\n\nCode\n```{r}\n#| label: fig-cat_linear\n#| message: false\n#| warning: false\n#| fig-width: 7\n#| fig-height: 8\n#| fig-cap: 'OverallQual Vs. SalePrice - The most \"linear\" realationship with a categorical predictor. The raincloud plot shows how variances of factors are very different.'\n\n# install.packages(\"ggrain\")\nlibrary(ggrain)\n\names_all |&gt;\n  filter(dataset == \"train\") |&gt;\n  ggplot(aes(x = OverallQual, y = SalePrice/1000, colour = OverallQual, fill = OverallQual)) +     \n  geom_rain(alpha = 0.3) +  \n  scale_y_continuous(breaks = seq(0, 800, 100)) +  \n  labs(x = \"Overall\\nQuality\", y = \"Sale Price ['000 $]\") +\n  coord_flip() + \n  theme(legend.position = \"none\")\n```\n\n\n\n\n\nFigure 6: OverallQual Vs. SalePrice - The most “linear” realationship with a categorical predictor. The raincloud plot shows how variances of factors are very different.\n\n\n\n\n\nI’m wondering in which price range is the most popular quality?\n\n```{r}\n#| label: fig-count_overallQual\n#| cap-location: margin\n#| fig-cap: 'Houses with mid-quality levels are most frequent in the train dataset. I will reduce the categories to nine (9), grouping or lumping 1 and 2 as \"Other\".'\n#| column: margin\n#| code-fold: false\n\names_all |&gt;\n  filter(dataset == \"train\") |&gt;\n  count(OverallQual, name = \"count\") |&gt;\n  ggplot(aes(y = fct_reorder(OverallQual, count), x = count)) + \n  geom_col(aes(fill = OverallQual)) +\n  geom_text(aes(label = count), size = 10) +\n  labs(y = \"Overall Quality\") + \n  theme(legend.position = \"none\", axis.title.y = element_text(angle = 90))\n```\n\n\n\n\n\nFigure 7: Houses with mid-quality levels are most frequent in the train dataset. I will reduce the categories to nine (9), grouping or lumping 1 and 2 as “Other”.\n\n\n\nIt seems that most popular houses, with an overall quality of 5 (mid-quality level), are the ones between 120K and 145K USD. Of course, this was back in the years of 2006 - 2010.\nNow I’ll group or lump into one category OverallQual = 1 and OverallQual = 2.\n\n\nCode\n```{r}\n#| label: lumping\n\names_all &lt;- \n  ames_all |&gt;\n  mutate(OverallQual = fct_lump(OverallQual, n = 8)) |&gt;\n  mutate(OverallQual = fct_relevel(OverallQual, \"Other\"))  \n```\n\n\n\n\nHow sale prices vary with the living area and depending on house styles?\n\n\nCode\n```{r}\n#| label: fig-houseStyles\n#| fig-column: page-right\n#| fig-width: 8\n#| fig-height: 8\n#| fig-cap: \"One and two story houses are the most common within the train dataset.\"\n\names_all |&gt;\n  filter(dataset == \"train\") |&gt;\n  ggplot(aes(x = GrLivArea, y = SalePrice/1000)) +  \n  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +  \n  labs(x = expression(\"Ground Living Area [\"~ft^2~\"]\"), \n       y = \"Sale\\nPrice\\n['000 $]\") +  \n  geom_point(aes(color = OverallQual), alpha = 0.3) +\n  geom_smooth(method = \"lm\", \n              formula =  y ~ splines::bs(x, 3), \n              color = \"black\",\n              se = FALSE,\n              size = 0.7, \n              alpha = 0.5) +\n  facet_wrap(~HouseStyle, nrow = 2) +\n  theme(legend.position = \"bottom\")\n```\n\n\n\n\n\nFigure 8: One and two story houses are the most common within the train dataset.\n\n\n\n\n\n\nHow sale prices vary with the living area and depending on location?\nOne can hear often that location is everything. Let’s see it here:\n\n\nCode\n```{r}\n#| label: fig-houseLocations\n#| fig-column: page-right\n#| fig-cap: \"There's a lot of variation depending on location, as expected. Northridge (NoRidge), College Creek (CollgCr) and Northridge Heights (NridgHt) seem to be the most expensive neighbourhoods.\"\n\names_all |&gt;\n  filter(dataset == \"train\") |&gt;\n  ggplot(aes(x = GrLivArea, y = SalePrice/1000)) +  \n  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +  \n  labs(x = expression(\"Ground Living Area [\"~ft^2~\"]\"), \n       y = \"Sale\\nPrice\\n['000 $]\") +  \n  geom_point(aes(color = OverallQual), alpha = 0.3) +\n  geom_smooth(method = \"lm\", \n              formula =  y ~ splines::bs(x, 3), \n              color = \"black\",\n              se = FALSE,\n              size = 0.7, \n              alpha = 0.5) +\n  facet_wrap(~Neighborhood, nrow = 3) +\n  theme(legend.position = \"bottom\")\n```\n\n\nWarning in predict.lm(model, newdata = data_frame0(x = xseq), se.fit = se, :\nprediction from rank-deficient fit; attr(*, \"non-estim\") has doubtful cases\n\n\n\n\n\n\n\n\nAnalysis of response SalePrice\nHow well-behaved is the target?\n\n\nCode\n```{r}\n#| label: fig-Y\n#| message: false\n#| warning: false\n#| column: body\n#| fig-width: 6\n#| fig-height: 6\n#| fig-cap: \"Analysis of the target variable. All the assumptions of OLS and linear regression are broken, as usual.\"\n#| fig-subcap:\n#|   - \"Histogram\"\n#|   - \"Q-Q Plot\"\n#| layout-ncol: 2\n\names_all |&gt;\n  filter(dataset == \"train\") |&gt;\n  ggplot(aes(x = SalePrice/1000)) +  \n  geom_histogram(bins = 50, color = \"seagreen\", fill = \"seagreen\", alpha = 0.5) +\n  scale_x_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +  \n  labs(x = \"Sale Price ['000 $]\")\n\names_all |&gt;\n  filter(dataset == \"train\") |&gt; \n  ggplot(aes(sample = SalePrice/1000)) + \n  stat_qq(color = \"seagreen\", alpha = 0.5) + \n  stat_qq_line(color = \"seagreen\") +  \n  scale_y_continuous(breaks = seq(0, 800, 50), labels = scales::comma) +\n  labs(y = \"Sale\\nPrice\\n['000 $]\",\n       x = \"Theoretical Quantiles (Norm. Dist.)\")\n```\n\n\n\n\n\n\n\n\n(a) Histogram\n\n\n\n\n\n\n\n(b) Q-Q Plot\n\n\n\n\nFigure 9: Analysis of the target variable. All the assumptions of OLS and linear regression are broken, as usual.\n\n\n\nClearly, the target variable is not well-behaved. It shows a log-normal distribution heavily skewed, to right of course. In order to achieve better predictions, a transformation might be needed.\n\nTransformation of the response\nFor what I’ve seen, processes that have to do with money, e.g., sale prices, costs of products, salaries, they are most of the time Log-normal. Rarely there is something free, so you don’t have zeros or with negative value; think about the top 1% of reach people, or anything that becomes premium the higher the price. These are long and sparse right tails.\nThe logarithmic transformation works when there are few large values, like in this case, there are only a few expensive houses. The logarithm scale will shrink that right tail5, making the distribution more normal-like.5 Think of a Logarithmic transformation like pressing an accordeon only from your right arm.\nNevertheless, the moment the first data transformation is performed, we start to loose interpretability very quickly. Nowadays, the most accurate predictions need more than one transformation in order to work best6.6 Forget about linearity and interpretability if you want the most accurate results.\nBecause we have powerful computers and libraries with written functions to perform variable transformations, I will use the Box-Cox transformation, which is a family of functions that includes the logarithmic one depending on the value of a parameter lambda. This parameter is automatically assessed.\n\n\nFamily Box-Cox transformations:\n\n\\(y(\\lambda) = \\begin{cases} \\frac{y^{-1}-1}{\\lambda}, & \\text{if } \\lambda \\neq 0 \\\\ \\log y, & \\text{if } \\lambda = 0 \\end{cases}\\)\n\n```{r}\n#| label: fig-boxcox\n#| fig-cap: \"Best parameter lambda = -0.05050505. It is almost a pure logarithmic transformation.\"\n#| column: margin\n#| code-fold: false\n#| warning: false\n#| message: false\n\nboxcox_trans &lt;- MASS::boxcox(lm(SalePrice ~ 1, \n                                data = subset(ames_all, dataset == \"train\")), \n                             lambda = seq(-1, 1), \n                             plotit = TRUE)\n\nboxcox_lambda &lt;- as.numeric(boxcox_trans$x[which.max(boxcox_trans$y)])\n\n# New transformed response variable\names_all &lt;- ames_all |&gt;\n  mutate(SalePrice_bc = (SalePrice^boxcox_lambda - 1) / boxcox_lambda)\n```\n\n\n\n\n\nFigure 10: Best parameter lambda = -0.05050505. It is almost a pure logarithmic transformation.\n\n\n\nThe new mutated SalePrice should be better behaved…\n\n\nCode\n```{r}\n#| label: fig-Y-trans\n#| message: false\n#| warning: false\n#| column: body\n#| fig-width: 6\n#| fig-height: 6\n#| fig-cap: \"The transformed response shows a more bell-like shape.\"\n#| fig-subcap:\n#|   - \"Histogram\"\n#|   - \"Q-Q Plot\"\n#| layout-ncol: 2\n\names_all |&gt;\n  filter(dataset == \"train\") |&gt;  \n  ggplot(aes(x = SalePrice_bc)) +  \n  geom_histogram(bins = 50, color = \"seagreen\", fill = \"seagreen\", alpha = 0.5) +  \n  labs(x = \"Sale Price [Box-Cox]\")\n\names_all |&gt;\n  filter(dataset == \"train\") |&gt;  \n  ggplot(aes(sample = SalePrice_bc)) + \n  stat_qq(color = \"seagreen\", alpha = 0.5) + \n  stat_qq_line(color = \"seagreen\") +   \n  labs(y = \"Sale\\nPrice\\n[Box-Cox]\", x = \"Theoretical Quantiles (Norm. Dist.)\")\n```\n\n\n\n\n\n\n\n\n(a) Histogram\n\n\n\n\n\n\n\n(b) Q-Q Plot\n\n\n\n\nFigure 11: The transformed response shows a more bell-like shape.\n\n\n\n… and it is, indeed. Well, except for the tails, now the transformed values are more normal-like at the core.\nDo not include the transformation of the response as a step within the recipe. It will cause an error with the predict function trying to locate SalePrice in the test dataset."
  },
  {
    "objectID": "HousePrices.html#footnotes",
    "href": "HousePrices.html#footnotes",
    "title": "Prediction of House Prices in Ames",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTip: print a copy, make notes and study it. This is the first step to get into domain knowledge for Feature Engineering in the modelling phase.↩︎\nTip: write a function to quantify missing values depending on the language and labelling system used.↩︎\nTip: integer variables like calendar Years can be treated as categories for the analysis.↩︎\nTip: visualise a Logarithmic transformation like pressing an accordeon only from your right side.↩︎\nTip: visualise a Logarithmic transformation like pressing an accordeon only from your right side.↩︎"
  },
  {
    "objectID": "HousePrices.html#impute-missing-values",
    "href": "HousePrices.html#impute-missing-values",
    "title": "Prediction of House Prices in Ames",
    "section": "Impute Missing Values",
    "text": "Impute Missing Values\nFirst things first, read the full data description2. There you’ll find that for several columns, missing values NA means actually “None”. The physical absence of a determined feature in a house is a category exposing the lack of such quality that can have a significant impact on the response SalePrice. Later it is most probably that binary features indicating the existence of some installations should be created.2 Print a copy, make notes and study it. This is the first step to get into domain knowledge, in this case Real Estate, for later Feature Engineering.\nTherefore, I will fill the empty NA fields of the indicated columns of both datasets with the string “None”.\n\n\nCode\n```{r}\n#| label: easy replace_na\n\ncols_NA_to_none &lt;- list(\n  Alley = \"None\",\n  BsmtQual = \"None\", BsmtCond = \"None\", BsmtExposure = \"None\", BsmtFinType1 = \"None\", BsmtFinType2 = \"None\", \n  FireplaceQu = \"None\", \n  GarageType = \"None\", GarageFinish = \"None\", GarageQual = \"None\", GarageCond = \"None\", \n  PoolQC = \"None\",\n  Fence = \"None\", \n  MiscFeature = \"None\")\n\names_all &lt;- ames_all |&gt;\n  replace_na(cols_NA_to_none)   \n```\n\n\nOne of the early and recurrent steps of the EDA is to check the completeness of data. Let’s search for missing values, after filling indicated fields3. For this case I wrote the function count_na() that generates the table displayed on the right margin.3 Write a function to quantify missing values depending on the language and labelling system used.\n\n```{r}\n#| label: Counting NAs\n#| code-fold: false\n#| column: margin\n\n# Remaining NA count leaving out target SalePrice\names_all |&gt; \n  select(-SalePrice) |&gt;\n  count_na() |&gt;  \n  knitr::kable(caption = 'Ames dataset')\n```\n\n\n\nAmes dataset\n\n\nVariable\nNA_count\nPercent\n\n\n\n\nLotFrontage\n486\n16.65\n\n\nGarageYrBlt\n159\n5.45\n\n\nMasVnrType\n24\n0.82\n\n\nMasVnrArea\n23\n0.79\n\n\nMSZoning\n4\n0.14\n\n\nUtilities\n2\n0.07\n\n\nBsmtFullBath\n2\n0.07\n\n\nBsmtHalfBath\n2\n0.07\n\n\nFunctional\n2\n0.07\n\n\nExterior1st\n1\n0.03\n\n\nExterior2nd\n1\n0.03\n\n\nBsmtFinSF1\n1\n0.03\n\n\nBsmtFinSF2\n1\n0.03\n\n\nBsmtUnfSF\n1\n0.03\n\n\nTotalBsmtSF\n1\n0.03\n\n\nElectrical\n1\n0.03\n\n\nKitchenQual\n1\n0.03\n\n\nGarageCars\n1\n0.03\n\n\nGarageArea\n1\n0.03\n\n\nSaleType\n1\n0.03\n\n\n\n\nI’ll fill the remaining missing values as follow:\n\nThe LotFrontage column refers to the linear feet of street connected to the house. This feature is not well documented and the missing percentage related to other variables makes it not only unreliable, but very low in variance. Therefore, I will delete the feature.\nEverytime a Garage doesn’t exist, i.e., GarageType = \"None\", there is the corresponding missing value in Garage Year Built GarageYrBlt = NA. I will engineer a new feature called GarageNew with three ordinal categories: None, No, Yes. This based on the delta of YearBuilt - GarageYrBlt; I expect that given the house price, the algorithm will learn that “None” is worse than “No” and so on. Then I will remove the GarageYrBlt predictor.\nFor the rest of variables with a 1 % or less missing values NA, I’ll calculate the median (if numerical) and the mode (if string) in order to fill them with it.\n\nHere’s my pipeline for the whole dataset:\n\n\nCode\n```{r}\n#| label: terminating NAs\n\n# \"replace_na_with_median\" is a function I created\n\names_all &lt;- ames_all |&gt;\n  mutate(LotFrontage = NULL) |&gt; # Removing LotFrontage\n  mutate(GarageNew = if_else(YearBuilt - GarageYrBlt &gt; 0, \"Yes\", \"No\")) |&gt; # New Feat.\n  replace_na(list(GarageNew = \"None\")) |&gt;\n  mutate(GarageNew = factor(GarageNew, levels = c(\"None\", \"No\", \"Yes\"))) |&gt; # 3 levels\n  mutate(GarageYrBlt = NULL) |&gt; # Removing old Feat.\n  mutate_if(is.numeric, replace_na_with_median)\n```\n\n\nLet’s get a list with the mode of each remaining columns containing missing values NA.\n\n\nCode\n```{r}\n#| label: Mode for NAs\n\n# Get the mode for reamaining columns with NAs:\n# \"find_mode()\" is a custom function.\n\n# Good, old-fashioned code: apply(ames_all, 2, find_mode)\n\nlist_na_mode &lt;- ames_all |&gt; \n  select(MasVnrType, MasVnrArea, MSZoning, Electrical, Utilities,   BsmtFullBath,   BsmtHalfBath,   Functional, Exterior1st,    Exterior2nd,    BsmtFinSF1, BsmtFinSF2, BsmtUnfSF,  TotalBsmtSF,    KitchenQual,GarageCars, GarageArea, SaleType) |&gt;\n  map(find_mode)\n\n# map returns a list \n\n# Replace with the created named-lists\names_all &lt;- ames_all |&gt;\n  replace_na(list_na_mode) \n\n# Sanity check of missing values:\nprint(\"Full Ames dataset (train and test)\")\names_all |&gt;\n  select(-SalePrice) |&gt;\n  count_na()\n```\n\n\n[1] \"Full Ames dataset (train and test)\"\n[1] \"No missing values (NA) found.\"\n\n\n“The data is complete!” Let’s think about predictors.\nThere are variables (columns) starting with numbers? I’ll rename them:\n\n\nCode\n```{r}\n#| label: rename_cols\n\names_all &lt;-\n  ames_all |&gt;\n  rename(FirstFlrSF = \"1stFlrSF\") |&gt;\n  rename(SecondFlrSF = \"2ndFlrSF\") |&gt;\n  rename(threessnporch = \"3SsnPorch\")\n```\n\n\n\nScreening of variable importance\nUsing the Mutual Information criterion (MI), I’ll rank the extent to which knowledge of each original predictor reduces the uncertainty of SalePrice.\n\n```{r}\n#| label: mutual-info\n#| column: margin\n#| code-fold: false\n\nlibrary(infotheo)\n\n# \"calc_mi\" is a function I wrote.\n\nmi_y_X &lt;- ames_all |&gt;\n  filter(dataset == \"train\") |&gt;\n  select(-c(Id, dataset)) |&gt;\n  calc_mi_score(target = \"SalePrice\")\n\nmi_y_X |&gt; \n  head(20) |&gt;\n  knitr::kable(caption = \"Mutual Info\")\n```\n\n\n\nMutual Info\n\n\n\nx\n\n\n\n\nSalePrice\n2.6515433\n\n\nOverallQual\n0.6086855\n\n\nNeighborhood\n0.6084916\n\n\nGrLivArea\n0.5723431\n\n\nGarageArea\n0.5244219\n\n\nYearBuilt\n0.5195234\n\n\nYearRemodAdd\n0.4564483\n\n\nTotalBsmtSF\n0.4473484\n\n\nFirstFlrSF\n0.4141527\n\n\nGarageCars\n0.3805914\n\n\nSecondFlrSF\n0.3735544\n\n\nBsmtUnfSF\n0.3544288\n\n\nMSSubClass\n0.3458352\n\n\nBsmtQual\n0.3425788\n\n\nExterQual\n0.3394960\n\n\nKitchenQual\n0.3283328\n\n\nOpenPorchSF\n0.3051952\n\n\nFullBath\n0.2863069\n\n\nGarageFinish\n0.2784786\n\n\nWoodDeckSF\n0.2706731"
  },
  {
    "objectID": "HousePrices.html#simple-model-as-reference",
    "href": "HousePrices.html#simple-model-as-reference",
    "title": "Prediction of House Prices in Ames",
    "section": "Simple Model as Reference",
    "text": "Simple Model as Reference"
  },
  {
    "objectID": "HousePrices.html#model-selection",
    "href": "HousePrices.html#model-selection",
    "title": "Prediction of House Prices in Ames",
    "section": "Model Selection",
    "text": "Model Selection"
  },
  {
    "objectID": "HousePrices.html#reference-model",
    "href": "HousePrices.html#reference-model",
    "title": "Prediction of House Prices in Ames",
    "section": "Reference Model",
    "text": "Reference Model\n\n\nCode\n```{r}\n#| label: ReferenceModel\n\n# Model 1 Setup\nmodel_lm &lt;-  linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  set_mode(\"regression\")\n\n# Workflow\n\names_wf &lt;- workflow() |&gt;\n  add_model(model_lm) |&gt;\n  add_formula(SalePrice ~ GrLivArea)\n\n\n# Fit models to validation folds\n\ntrained_models &lt;- fit_resamples(object = ames_wf, resamples = ames_boots)\n\n# See estimated performance\ntrained_models |&gt; \n  collect_metrics(summarize = TRUE) # TRUE gets Avg. performance \n\n\n\n# Modern DS is a mixed territory. That's what make it so effective and with rapid advances\n# Every knowledge domain claiming the right terminology\n# Goodness of prediction (test )\n# Goodness of fit - seen data (train )  not realistic metric here \n```\n\n\n# A tibble: 2 × 6\n  .metric .estimator      mean     n   std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   54038.     1000 71.5      Preprocessor1_Model1\n2 rsq     standard       0.539  1000  0.000984 Preprocessor1_Model1"
  },
  {
    "objectID": "HousePrices.html#choosing-resampling-technique",
    "href": "HousePrices.html#choosing-resampling-technique",
    "title": "Prediction of House Prices in Ames",
    "section": "Choosing Resampling Technique",
    "text": "Choosing Resampling Technique\nDepending on the chosen resample technique to estimate the Goodness-of-Fit (test error) Take a look at the pristine explanation by (Zablotski 2022).\n\nSimple validation? 1 train fold and 1 test fold: NO, it’s old and innacurate.\nv-folds Cross-Validation? Analysis (training folds), Assessment (validation folds) and test: it’s small data, I can choose something even more powerful.\nBootstrap Sampling? YES.\n\nBy today’s standards this dataset is small, therefore I will choose Bootstraping to keep the chosen samples the same size as the training set. The probability of an observation of being sample at least once is 0.63, therefore the complementary training set (around 37%) will become the assessment set. Although performance metrics will be slightly conservative or pesimistic, I prefer it.\nI will generate 1000 bootstrapped samples.\n\nExpending Data Budget\nNormally with v-fold CV I expend my data budget, but here doing resampling with replacement is kind of printing money…\nHere’s the code to generate the bootstraps and therefore split the data budget:\n\n\nCode\n```{r}\n#| label: DataBudget\n#| warning: false\n#| fold: false\n\n# install.packages(\"tidymodels\")\nlibrary(tidymodels)\ntidymodels_prefer()\n# conflicted::conflicts_prefer(scales::discard)\n\n# Select the pre-processed training dataset\names_train &lt;- \n  ames_all |&gt; \n  filter(dataset == \"train\") |&gt;\n  select(-c(Id, dataset, SalePrice_bc))\n\n# v-folds CV repeated 10 times (v x 10 models)\n# ames_folds &lt;- vfold_cv(ames_train, v = 8, repeats = 10, strata = SalePrice) # v = 10 folds are default\n\n# Generate 1000 bootstrapped samples\nset.seed(1982)\names_boots &lt;- bootstraps(ames_train, times = 100)\names_boots  \n```\n\n\n# A tibble: 100 × 2\n   splits             id          \n   &lt;list&gt;             &lt;chr&gt;       \n 1 &lt;split [1458/547]&gt; Bootstrap001\n 2 &lt;split [1458/532]&gt; Bootstrap002\n 3 &lt;split [1458/552]&gt; Bootstrap003\n 4 &lt;split [1458/511]&gt; Bootstrap004\n 5 &lt;split [1458/506]&gt; Bootstrap005\n 6 &lt;split [1458/548]&gt; Bootstrap006\n 7 &lt;split [1458/537]&gt; Bootstrap007\n 8 &lt;split [1458/532]&gt; Bootstrap008\n 9 &lt;split [1458/527]&gt; Bootstrap009\n10 &lt;split [1458/539]&gt; Bootstrap010\n# ℹ 90 more rows"
  },
  {
    "objectID": "HousePrices.html#expending-the-data-budget",
    "href": "HousePrices.html#expending-the-data-budget",
    "title": "Prediction of House Prices in Ames",
    "section": "Expending the data budget",
    "text": "Expending the data budget\nNormally with v-fold CV I expend my data budget, but here doing resampling with replacement is kind of printing money… Here’s the code to generate the bootstraps and therefore split the data budget:\n\n\nCode\n```{r}\n#| label: DataBudget\n#| warning: false\n#| fold: false\n\nlibrary(tidymodels)\ntidymodels_prefer()\n# conflicted::conflicts_prefer(scales::discard)\n\n# Select the pre-processed training dataset\names_train &lt;- \n  ames_all |&gt; \n  filter(dataset == \"train\")\n \n\n# ---- v-folds Cross Validation ----\nset.seed(1982)\names_folds &lt;- vfold_cv(ames_train, v = 10, repeats = 10, strata = SalePrice_bc) # v = 10 folds are default\n\n# ---- The Bootstrap ----\nset.seed(1982)\names_boots &lt;- bootstraps(ames_train, times = 1000, strata = SalePrice_bc)\names_boots  \n```\n\n\n# A tibble: 1,000 × 2\n   splits             id           \n   &lt;list&gt;             &lt;chr&gt;        \n 1 &lt;split [1458/545]&gt; Bootstrap0001\n 2 &lt;split [1458/530]&gt; Bootstrap0002\n 3 &lt;split [1458/534]&gt; Bootstrap0003\n 4 &lt;split [1458/535]&gt; Bootstrap0004\n 5 &lt;split [1458/513]&gt; Bootstrap0005\n 6 &lt;split [1458/534]&gt; Bootstrap0006\n 7 &lt;split [1458/519]&gt; Bootstrap0007\n 8 &lt;split [1458/532]&gt; Bootstrap0008\n 9 &lt;split [1458/529]&gt; Bootstrap0009\n10 &lt;split [1458/555]&gt; Bootstrap0010\n# ℹ 990 more rows"
  },
  {
    "objectID": "HousePrices.html#expending-data-budget",
    "href": "HousePrices.html#expending-data-budget",
    "title": "Prediction of House Prices in Ames",
    "section": "Expending Data Budget",
    "text": "Expending Data Budget\n\n\nCode\n```{r}\n#| label: Data Budget\n#| warning: false\n\n# install.packages(\"tidymodels\")\nlibrary(tidymodels)\ntidymodels_prefer()\n# conflicted::conflicts_prefer(scales::discard)\n\n# Select the pre-processed training dataset\names_train &lt;- ames_all |&gt; filter(dataset == \"train\")\n\n# v-folds CV repeated 10 times (v x 10 models)\n# ames_folds &lt;- vfold_cv(ames_train, v = 8, repeats = 10, strata = SalePrice) # 10 folds are default\n\n# Generate 1000 bootstrapped samples\nset.seed(1982)\names_boots &lt;- bootstraps(ames_train, times = 1000)\names_boots\n```\n\n\n# A tibble: 1,000 × 2\n   splits             id           \n   &lt;list&gt;             &lt;chr&gt;        \n 1 &lt;split [1458/547]&gt; Bootstrap0001\n 2 &lt;split [1458/532]&gt; Bootstrap0002\n 3 &lt;split [1458/552]&gt; Bootstrap0003\n 4 &lt;split [1458/511]&gt; Bootstrap0004\n 5 &lt;split [1458/506]&gt; Bootstrap0005\n 6 &lt;split [1458/548]&gt; Bootstrap0006\n 7 &lt;split [1458/537]&gt; Bootstrap0007\n 8 &lt;split [1458/532]&gt; Bootstrap0008\n 9 &lt;split [1458/527]&gt; Bootstrap0009\n10 &lt;split [1458/539]&gt; Bootstrap0010\n# ℹ 990 more rows"
  },
  {
    "objectID": "HousePrices.html#reference-models",
    "href": "HousePrices.html#reference-models",
    "title": "Prediction of House Prices in Ames",
    "section": "Reference Models",
    "text": "Reference Models\nMy final objective is to stack (combine or ensemble) several models of different nature at the end.\nHowever, for a first model I’d like to keep interpretability and to involve all predictors in order to apply some regularisation given the problem of multicollinearity. Hence, I’ll go with a regularised (shrinkage) Generalised Linear Model, GLM. In this manner I can have a first look at feature importance.\nI’ll choose Elastic net regression. Features must be scaled!77 I don’t want to over-penalised non-scaled variables using shrinkage methods.\nThe following code shows preprocessing steps enclosed in a pipeline or recipe. The list of available step functions can be found here.\n\n\nCode\n```{r}\n#| label: Preprocessing\n\n# ---- Base recipe ----\nbase_recipe &lt;-\n  recipe(SalePrice ~ GrLivArea + OverallQual, data = ames_train)\n\nshrinkage_recipe &lt;-\n  recipe(SalePrice ~ ., data = ames_train) |&gt;\n  step_BoxCox(SalePrice, limits = c(-1, 1)) |&gt;  \n  step_scale(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;  \n  step_zv(all_predictors())\n  \n\n# Recipe for trees  \ndummy_recipe &lt;-\n  base_recipe |&gt;\n  step_dummy(all_nominal_predictors())\n\n# Recipe for splines\nspline_recipe &lt;-\n  dummy_recipe |&gt;\n  step_bs(GrLivArea)\n```\n\n\n\nSetting Models Specifications\n\n```{r}\n#| label: ElasticNetSpecs\n#| code-fold: false\n\n# install.packages(c(\"ranger\", \"ëarth\", \"glmnet\"))\n\n# Model 1: Elastic Net Regression\n\nelnet_spec &lt;-\n  linear_reg(penalty = tune(), mixture = tune()) |&gt; # penalty = lambda; mixture = alpha\n  set_mode(\"regression\") |&gt;\n  set_engine(\"glmnet\")\n\n\nelnet_grid &lt;- expand_grid(penalty = seq(0, 100, by = 10),\n                          mixture = seq(0, 1, by = 0.2) )\n\n\ndoParallel::registerDoParallel()\nset.seed(1982)\n\nelnet_tune_res &lt;- tune_grid(elnet_spec, \n                            preprocessor = shrinkage_recipe, \n                            grid = elnet_grid, \n                            resamples = ames_boots)\n\nelnet_tune_res |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n\nelnet_tune_res |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot(aes(penalty, mean, color = factor(mixture), group = factor(mixture))) + \n  geom_point() + \n  geom_line() + \n  labs(y = \"RMSE\")\n```\n\n# A tibble: 66 × 8\n   penalty mixture .metric .estimator   mean     n std_err .config              \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1       0     0   rmse    standard   0.0658   100 0.00542 Preprocessor1_Model01\n 2       0     0.2 rmse    standard   0.0702   100 0.00592 Preprocessor1_Model12\n 3       0     0.6 rmse    standard   0.0702   100 0.00592 Preprocessor1_Model34\n 4       0     0.4 rmse    standard   0.0703   100 0.00592 Preprocessor1_Model23\n 5       0     0.8 rmse    standard   0.0704   100 0.00594 Preprocessor1_Model45\n 6       0     1   rmse    standard   0.0705   100 0.00595 Preprocessor1_Model56\n 7      10     0   rmse    standard   0.151    100 0.0101  Preprocessor1_Model02\n 8      20     0   rmse    standard   0.169    100 0.0123  Preprocessor1_Model03\n 9      30     0   rmse    standard   0.178    100 0.0134  Preprocessor1_Model04\n10      40     0   rmse    standard   0.183    100 0.0141  Preprocessor1_Model05\n# ℹ 56 more rows\n\n\n\n\n\n\n\nCode\n```{r}\n#| label: OtherModelsSpecs\n\n# linear_reg(mixture = .1) %&gt;% \n#   set_engine(\"glmnet\") %&gt;% \n#   translate()\n\n\n# # Model 2 Setup\n# spec_lm &lt;- \n#   linear_reg() |&gt;\n#   set_engine(\"lm\") |&gt;\n#   set_mode(\"regression\")\n\n# # Model 3\n# spec_rf &lt;-\n#   rand_forest(trees = 1000) |&gt;\n#   set_engine(\"ranger\") |&gt;\n#   set_mode(\"regression\")\n  \n# # Model 4\n# spec_mars &lt;-\n#   mars() |&gt;\n#   set_engine(\"earth\") |&gt;\n#   set_mode(\"regression\")\n```\n\n\n\n\nDefining a workflow set\nWith a workflow set I can combine a list of pre-processors with a list of models:\n\n\nCode\n```{r}\n# Workflow set\n\n# Allows combine different pre-processors and models fit them all at once and compare results\n\n# list_of_recipes &lt;- list(base_recipe, dummy_recipe, spline_recipe)\n# list_of_models &lt;- list(spec_rf, spec_mars, spec_lm)\n\n# ames_set &lt;- \n#   workflow_set(\n#     list_of_recipes, \n#     list_of_models, \n#     cross = FALSE, \n#     case_weights = NULL)\n\n# ames_set\n```\n\n\n\n\nTraining with the Bootstrap samples\n\n\nCode\n```{r}\n#| label: GoF\n\n# install.packages(\"doParallel\")\n\n# doParallel::registerDoParallel()\n# set.seed(1982)\n\n# ames_results &lt;- workflow_map(ames_set, \"fit_resamples\", resamples = ames_boots)\n# ames_results\n\n# save(ames_results, file = \"./models/ames_results_wf.Rdata\")\n\n# ames_results &lt;- load(\"./models/ames_results_wf.Rdata\")\n```\n\n\n\n\nEvaluating the workflow\n\n\nCode\n```{r}\n#| label: EvaluateResults\n\n# plt_res &lt;- workflowsets::autoplot(ames_results)\n\n# plt_res\n```\n\n\n\n\nCode\n```{r}\n# ames_wf &lt;- workflow() |&gt;\n#   add_model(spec_lm) |&gt;\n#   add_formula(SalePrice ~ GrLivArea)\n\n# # Fit models to bootstrapped sets\n\n# trained_models &lt;- fit_resamples(object = ames_wf, resamples = ames_boots)\n\n# # See estimated performance\n# trained_models |&gt;  collect_metrics(summarize = TRUE) # TRUE gets Avg. performance \n```"
  },
  {
    "objectID": "HousePrices.html#resampling-technique-for-test-error-estimation",
    "href": "HousePrices.html#resampling-technique-for-test-error-estimation",
    "title": "Prediction of House Prices in Ames",
    "section": "Resampling Technique for Test Error Estimation",
    "text": "Resampling Technique for Test Error Estimation\nThis is a critical step for final model selection. Depending on the chosen resample technique to estimate the Goodness-of-Prediction or test error, in this case metrics like RMSE can be slightly optimistic or conservative. Take a look at the pristine explanation by (Zablotski 2022).\n\nSimple validation set? 1 train fold and 1 test fold: NO, it’s old and inaccurate.\nv-folds Cross-Validation? Analysis (training folds), Assessment (validation folds) and test: it’s small data, I can choose something even more powerful.\nThe Bootstrap? YES.\n\nBy today’s standards this dataset is small, therefore I will choose Bootstraping to keep the chosen samples the same size as the training set. The probability of an observation of being sample at least once is 0.63, therefore the complementary training set (around 37%) will become the assessment set. Although performance metrics will be slightly conservative or pessimistic, I prefer it.\nI will generate 1000 bootstrap-samples."
  },
  {
    "objectID": "HousePrices.html#base-models",
    "href": "HousePrices.html#base-models",
    "title": "Prediction of House Prices in Ames",
    "section": "Base Models",
    "text": "Base Models\nMy final objective is to stack (combine or ensemble) several models of different nature. However, for a first model I’d like to keep interpretability.\nWhere to start? In order to have a set of refence good models, I will proceed as follows:\n\nCherry-picking predictors with linear influence from correlograms Figure 2 and Figure 3.\nEstablish a workflow set of linear models:\n\nSimplest OLS univariate lm GrLivArea vs. SalePrice.\nCherry-pick predictors and fit a linear hyper-plane: multivariate lm.\nAdd some flexibility on top with a Spline GAM.\n\n\nFor the sake of RMSE metric comparison I will always scale numeric predictors and apply the Box-Cox transformation to the outcome SalePrice, this given the improvement shown in Figure 11.\nThe following code shows preprocessing steps enclosed in a pipeline or recipe. The list of available step functions can be found here.\n\n\nCode\n```{r}\n#| label: fig-cherry_picking_feats\n#| fig-cap: \"Correlated numerical predictors are skewed and contain zero values.\"\n\names_train |&gt;  \n  select(c(GrLivArea, TotBaths, GarageArea, TotalBsmtSF, YearBuilt)) |&gt; \n  pivot_longer(GrLivArea:YearBuilt) |&gt;\n  ggplot(aes(x = value)) +  \n  geom_histogram(bins = 50, color = \"lightgrey\", fill = \"lightgrey\") +  \n  labs(x = \"\") + \n  facet_wrap(~name, scales = \"free_x\")\n```\n\n\n\n\n\nFigure 12: Correlated numerical predictors are skewed and contain zero values.\n\n\n\n\nGiven the sensitivity to tails and outliers of linear models, remember Figure 4 (a), it will be convienent to try to normalise the distribution of these predictors. However, Box-Cox transformations work only for positive data. The alternative will be the more general Yeo-Johnson transformation, which works for both, positive and negative values.\nNote that SalePrice is already transformed. See here\n\n\nCode\n```{r}\n#| label: cherry-picking_lm\n\n# ---------- Preprocessing Pipelines ----------\n\n# Uni-lm (simplest reasonable model)\nsimplest_rec &lt;-\n  recipe(SalePrice ~ GrLivArea, data = ames_train) |&gt;  \n  step_YeoJohnson(GrLivArea) |&gt;\n  step_scale(all_numeric_predictors())\n\n# Sanity check for warnings:\n# simplest_rec |&gt; prep() |&gt;  bake(new_data = NULL)\n\n\n# Multi-lm \nmulti_lm_rec &lt;-\n  recipe(SalePrice ~ GrLivArea + TotBaths + GarageArea + TotalBsmtSF + YearBuilt + OverallQual + KitchenQual + Foundation + ExistFireplace + HeatingQC, data = ames_train) |&gt;  \n  step_YeoJohnson(all_numeric_predictors()) |&gt;\n  step_scale(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors())\n\n# Sanity check for warnings:\n# multi_lm_rec |&gt; prep() |&gt; bake(new_data = NULL)\n\n# GAM: Splines \nmulti_gam_rec &lt;-\n  multi_lm_rec |&gt;\n  step_bs(all_numeric_predictors())  # GAM basis fun expansion.\n\n\n# ---------- Model specification ----------\n\nlm_spec &lt;- \n  linear_reg() |&gt;\n  set_mode(\"regression\") |&gt;\n  set_engine(\"lm\")\n\n\n# ---------- Set of Workflows ----------  \n\nmy_lm_recipes &lt;- list(simplest_rec, multi_lm_rec, multi_gam_rec)\nmy_lms &lt;- list(lm_spec)\n\n# With a workflow set I can combine a list of pre-processors with a list of models:\names_wfset &lt;- workflow_set(my_lm_recipes, my_lms, cross = TRUE, case_weights = NULL)\names_wfset\n\n\n# ---------- Fit Bootstraps ----------\n\ndoParallel::registerDoParallel()\nset.seed(1982)\n\names_lms_res &lt;- workflow_map(ames_wfset, \"fit_resamples\", resamples = ames_boots)\n\names_lms_res |&gt;  \n  collect_metrics(summarize = TRUE) |&gt;\n  filter(.metric == \"rmse\")\n```\n\n\n# A tibble: 3 × 4\n  wflow_id            info             option    result    \n  &lt;chr&gt;               &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 recipe_1_linear_reg &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 recipe_2_linear_reg &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 recipe_3_linear_reg &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n# A tibble: 3 × 9\n  wflow_id         .config preproc model .metric .estimator   mean     n std_err\n  &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 recipe_1_linear… Prepro… recipe  line… rmse    standard   0.147   1000 1.40e-4\n2 recipe_2_linear… Prepro… recipe  line… rmse    standard   0.0814  1000 1.14e-4\n3 recipe_3_linear… Prepro… recipe  line… rmse    standard   0.0812  1000 1.14e-4\n\n\nOK! We can see the power of adding good predictors.\nMy reference is the linear GAM with Splines, showing a mean RMSE = 0.0806. The result was transformed in the pipeline, therefore no units are displayed.\nI got slighlty better results using one-hot encoded categoricals. Moreover, applying the Yeo-Johnson transformation to numeric Feats. helped too! They were skewed, see Figure 12.\nIn order to see the feature importance, I’ll train and extract the workflow of the simple, second model:\n\n\n\nCode\n```{r}\n#| label: lm_best_feats\n#| column: margin\n\names_lms_res |&gt;\n  extract_workflow(\"recipe_2_linear_reg\") |&gt;\n  fit(ames_train) |&gt;\n  tidy()\n```\n\n\n# A tibble: 27 × 5\n   term           estimate std.error statistic  p.value\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)      6.16     0.279       22.1  4.52e-93\n 2 GrLivArea        0.0597   0.00332     18.0  2.57e-65\n 3 TotBaths         0.0253   0.00305      8.31 2.26e-16\n 4 GarageArea       0.0270   0.00274      9.83 4.24e-22\n 5 TotalBsmtSF      0.0337   0.00289     11.7  4.55e-30\n 6 YearBuilt        0.0200   0.00405      4.94 8.78e- 7\n 7 OverallQual_X3   0.101    0.0411       2.46 1.42e- 2\n 8 OverallQual_X4   0.185    0.0380       4.87 1.23e- 6\n 9 OverallQual_X5   0.239    0.0381       6.28 4.39e-10\n10 OverallQual_X6   0.262    0.0383       6.84 1.16e-11\n# ℹ 17 more rows"
  },
  {
    "objectID": "HousePrices.html#predicting-for-first-kaggle-submission",
    "href": "HousePrices.html#predicting-for-first-kaggle-submission",
    "title": "Prediction of House Prices in Ames",
    "section": "Predicting for First Kaggle Submission",
    "text": "Predicting for First Kaggle Submission\n\n\nCode\n```{r}\n#| label: first_submission\n#| warning: false\n\n# Get best model and fit with whole training set\nfit_lm_bsplines &lt;- \n  extract_workflow(ames_lms_res, \"recipe_3_linear_reg\") |&gt;\n  fit(ames_train)\n\n# Predicting on the test set\names_test &lt;- \n  ames_all |&gt;\n  select(-SalePrice) |&gt;\n  filter(dataset == \"test\")\n\nres_1_bsplines &lt;- \n  predict(fit_lm_bsplines, ames_test) |&gt;\n  mutate(SalePrice_pred = inv_boxcox(.pred, boxcox_lambda))\n\n# .csv for submission\n\nsub_1 &lt;- data.frame(Id = ames_test$Id, SalePrice = res_1_bsplines$SalePrice_pred)\n\nwrite.csv(sub_1, \"./data/submissions/sub_1_splines.csv\", quote = FALSE, row.names = FALSE)\n```\n\n\nNow that I have some reference RMSEs, I proceed to involve all predictors. Having high multicollinearty it’s reasonable to think of some sort of penalisation or shrinkage method. Hence, I’ll go with a regularised (shrinkage) Generalised Linear Model, GLM. In this manner I can have a first look at feature importance.\nI’ll choose Elastic net regression. Features must be scaled! I don’t want to over-penalised non-scaled variables using shrinkage methods.\n\n\nCode\n```{r}\n#| label: Preprocessing\n\n# shrinkage_rec &lt;-\n#   recipe(SalePrice ~ ., data = ames_train) |&gt;\n#   update_role(Id, new_role = \"Id variabe\") |&gt;\n#   update_role(dataset, new_role = \"splitting variable\") |&gt;\n#   step_BoxCox(SalePrice, limits = c(-1, 1)) |&gt;  \n#   step_scale(all_numeric_predictors()) |&gt;\n#   step_dummy(all_nominal_predictors()) |&gt;  \n#   step_zv(all_predictors())\n\n# shrinkage_spline_rec &lt;-\n#   shrinkage_rec |&gt;\n#   step_bs(all_numeric_predictors())  \n\n\n\n# Recipe for trees  \n# dummy_recipe &lt;-\n#   base_recipe |&gt;\n#   step_dummy(all_nominal_predictors())\n\n# Recipe for splines\n# spline_recipe &lt;-\n#   dummy_recipe |&gt;\n#   step_bs(GrLivArea)\n```\n\n\n\n```{r}\n#| label: ElasticNetSpecs\n#| code-fold: false\n\n# install.packages(c(\"ranger\", \"earth\", \"glmnet\"))\n\n# Linear Model\n# lm_spec &lt;- \n#   linear_reg() |&gt;\n#   set_mode(\"regression\") |&gt;\n#   set_engine(\"lm\")\n\n\n# Linear Model penalised\n# elnet_spec &lt;-\n#   linear_reg(penalty = tune(), mixture = tune()) |&gt; # penalty = lambda; mixture = alpha\n#   set_mode(\"regression\") |&gt;\n#   set_engine(\"glmnet\")\n\n# elnet_grid &lt;- expand_grid(penalty = seq(0, 100, by = 10),\n#                           mixture = seq(0, 1, by = 0.2) )\n\n\n# doParallel::registerDoParallel()\n# set.seed(1982)\n\n# elnet_tune_res &lt;- tune_grid(elnet_spec, \n#                             preprocessor = shrinkage_spline_rec, \n#                             grid = elnet_grid, \n#                             resamples = ames_boots)\n\n# elnet_tune_res |&gt;\n#   collect_metrics() |&gt;\n#   filter(.metric == \"rmse\") |&gt;\n#   arrange(mean)\n\n\n# elnet_tune_res |&gt;\n#   collect_metrics() |&gt;\n#   filter(.metric == \"rmse\") |&gt;\n#   ggplot(aes(penalty, mean, color = factor(mixture), group = factor(mixture))) + \n#   geom_point() + \n#   geom_line() + \n#   labs(y = \"RMSE\")\n \n# elnet_spec &lt;-\n#   linear_reg(penalty = 10, mixture = 0) |&gt; # penalty = lambda; mixture = alpha\n#   set_mode(\"regression\") |&gt;\n#   set_engine(\"glmnet\")\n```\n\n\n\nCode\n```{r}\n#| label: OtherModelsSpecs\n\n# linear_reg(mixture = .1) %&gt;% \n#   set_engine(\"glmnet\") %&gt;% \n#   translate()\n\n\n# # Model 2 Setup\n# spec_lm &lt;- \n#   linear_reg() |&gt;\n#   set_engine(\"lm\") |&gt;\n#   set_mode(\"regression\")\n\n# # Model 3\n# spec_rf &lt;-\n#   rand_forest(trees = 1000) |&gt;\n#   set_engine(\"ranger\") |&gt;\n#   set_mode(\"regression\")\n  \n# # Model 4\n# spec_mars &lt;-\n#   mars() |&gt;\n#   set_engine(\"earth\") |&gt;\n#   set_mode(\"regression\")\n```\n\n\n\n\nCode\n```{r}\n# Workflow set\n\n# Allows combine different pre-processors and models fit them all at once and compare results\n\n# my_recipes &lt;- list(base_recipe, dummy_recipe, spline_recipe)\n# my_models &lt;- list(spec_rf, spec_mars, spec_lm)\n\n# my_recipes &lt;- list(num_uni_lm_rec, num_mul_lm_rec, num_mul_gam_rec)\n# my_models &lt;- list(lm_spec, lm_spec, lm_spec)\n\n# ames_wfset &lt;- \n#   workflow_set(\n#     my_recipes, \n#     my_models, \n#     cross = FALSE, \n#     case_weights = NULL)\n\n# ames_wfset\n```\n\n\n\n\nCode\n```{r}\n#| label: GoF\n\n# install.packages(\"doParallel\")\n\n# doParallel::registerDoParallel()\n# set.seed(1982)\n\n# ames_results &lt;- workflow_map(ames_wfset, \"fit_resamples\", resamples = ames_boots)\n\n# ames_results |&gt;  \n#   collect_metrics(summarize = TRUE) |&gt;\n#   filter(.metric == \"rmse\")\n\n\n\n# save(ames_results, file = \"./models/ames_results_wf.Rdata\")\n\n# ames_results &lt;- load(\"./models/ames_results_wf.Rdata\")\n```\n\n\n\n\nCode\n```{r}\n# ames_wf &lt;- workflow() |&gt;\n#   add_model(spec_lm) |&gt;\n#   add_formula(SalePrice ~ GrLivArea)\n\n# # Fit models to bootstrapped sets\n\n# trained_models &lt;- fit_resamples(object = ames_wf, resamples = ames_boots)\n\n# # See estimated performance\n# trained_models |&gt;  collect_metrics(summarize = TRUE) # TRUE gets Avg. performance \n```"
  },
  {
    "objectID": "HousePrices.html#base-worflows-set",
    "href": "HousePrices.html#base-worflows-set",
    "title": "Prediction of House Prices in Ames",
    "section": "Base Worflows Set",
    "text": "Base Worflows Set\nMy final objective is to stack (combine / ensemble) several models of different nature. However, for a first model I’d like to keep interpretability.\n\nLinear Models\nWhere to start? In order to have a set of reference models, I will proceed as follows:\n\nCherry-picking predictors with linear influence from correlograms Figure 2 and Figure 3.\nEstablish a workflow set of linear models:\n\nFit the simplest OLS univariate lm GrLivArea vs. SalePrice.\nCherry-pick predictors and fit a linear hyper-plane: multivariate lm.\nAdd some flexibility on top fitting a Spline GAM.\n\n\nFor the sake of RMSE metric comparison I will always scale numeric predictors and use the Box-Cox transformed response SalePrice_bc, this given the improvement shown in Figure 11.\nThe following code shows preprocessing steps enclosed in a pipeline or recipe. The list of available step functions can be found here.\n\n\nCode\n```{r}\n#| label: fig-cherry_picking_feats\n#| fig-cap: \"Correlated numerical predictors are skewed and contain zero values.\"\n\names_train |&gt;  \n  select(c(GrLivArea, TotBaths, GarageArea, TotalBsmtSF, YearBuilt)) |&gt; \n  pivot_longer(GrLivArea:YearBuilt) |&gt;\n  ggplot(aes(x = value)) +  \n  geom_histogram(bins = 50, color = \"lightgrey\", fill = \"lightgrey\") +  \n  labs(x = \"\") + \n  facet_wrap(~name, scales = \"free\")\n```\n\n\n\n\n\nFigure 12: Correlated numerical predictors are skewed and contain zero values.\n\n\n\n\nGiven the sensitivity to tails and outliers of linear models, remember Figure 4 (a), it will be convenient to try to normalise the distribution of these predictors. However, Box-Cox transformations work only for positive data. The alternative will be the more general Yeo-Johnson transformation, which works for both, positive and negative values.\nNote that SalePrice_bc is the transformed outcome. Click here for more info.\n\n\nCode\n```{r}\n#| label: cherry-picking_lm\n#| eval: false\n\n# ---------- Preprocessing Pipelines ----------\n\n# Uni-lm (SIMPLEST reasonable model)\nsimplest_rec &lt;-\n  recipe(SalePrice_bc ~ GrLivArea, data = ames_train) |&gt;  \n  step_YeoJohnson(GrLivArea) |&gt;\n  step_normalize(all_numeric_predictors())\n\n# simplest_rec |&gt; prep() |&gt;  bake(new_data = NULL) # Sanity check for warnings\n\n# Multi-lm \nmulti_lm_rec &lt;-\n  recipe(SalePrice_bc ~ GrLivArea + TotBaths + GarageArea + TotalBsmtSF + YearBuilt + OverallQual + KitchenQual + Foundation + ExistFireplace + HeatingQC, data = ames_train) |&gt;  \n  step_YeoJohnson(all_numeric_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors())\n\n# multi_lm_rec |&gt; prep() |&gt; bake(new_data = NULL) # Sanity check for warnings\n\n# GAM: Splines \nmulti_gam_rec &lt;-\n  multi_lm_rec |&gt;\n  step_bs(all_numeric_predictors())  # GAM basis fun expansion.\n\n\n# ---------- Model specification ----------\nlm_spec &lt;- \n  linear_reg() |&gt;\n  set_mode(\"regression\") |&gt;\n  set_engine(\"lm\")\n\n\n# ---------- Set of Workflows ----------  \nmy_lm_recipes &lt;- list(simplest_rec, multi_lm_rec, multi_gam_rec)\nmy_lms_specs &lt;- list(lm_spec)\n\n# Combine a list of pre-processors with a list of models:\names_wfset &lt;- workflow_set(my_lm_recipes, my_lms_specs, \n                           cross = TRUE, \n                           case_weights = NULL)\names_wfset\n\n\n# ---------- Fit Bootstraps ----------\ndoParallel::registerDoParallel()\nset.seed(1982)\n\names_lms_res &lt;- workflow_map(ames_wfset, \"fit_resamples\", resamples = ames_boots)\n\nsave(ames_lms_res, file = \"./models/trained_resamples/ames_lms_res.Rdata\")\n\n# Load fitted Bootstraps\n\n# ames_lms_res &lt;- load(file = \"./models/trained_resamples/ames_lms_res.Rdata\")\n\names_lms_res |&gt;  \n  collect_metrics(summarize = TRUE) |&gt;\n  select(-c(.estimator, .config)) |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  knitr::kable(caption = \"Three Base Linear Models\")\n```\n\n\nOK! We can see the power of adding good predictors in the test RMSE estimation\nMy reference is the linear GAM with Splines, showing a mean RMSE = 0.0812. The result was transformed in the pipeline, therefore no units are displayed.\nI got slightly better results using dummy encoded factors. Moreover, applying the Yeo-Johnson transformation to numeric Feats. helped too! They were skewed, see Figure 12.\nIn order to see the feature importance, I’ll train and extract the workflow of the simple, second model, i.e., the multivariate lm:\n\n\n\nCode\n```{r}\n#| label: lm_best_feats\n#| column: margin\n#| eval: false\n\names_lms_res |&gt;\n  extract_workflow(\"recipe_2_linear_reg\") |&gt;\n  fit(ames_train) |&gt;  \n  tidy() |&gt; \n  select(-statistic) |&gt;\n  arrange(-abs(estimate)) |&gt;\n  slice_head(n = 15) |&gt; \n  knitr::kable(caption = 'Multi-lm')  \n```\n\nHere is the code to extract the best model workflow, retrain with the whole training set, predicting and writing the submission .csv file:\n\n\nCode\n```{r}\n#| label: first_submission\n#| warning: false\n#| eval: false\n\n# Get best model and fit with whole training set\nfit_lm_bsplines &lt;- \n  extract_workflow(ames_lms_res, \"recipe_3_linear_reg\") |&gt;\n  fit(ames_train)\n\n# Predicting on the test set\names_test &lt;- \n  ames_all |&gt;\n  select(-SalePrice) |&gt;\n  filter(dataset == \"test\")\n\nres_1_bsplines &lt;- \n  predict(fit_lm_bsplines, ames_test) |&gt;\n  mutate(SalePrice_pred = inv_boxcox(.pred, boxcox_lambda))  # orignal values in USD\n\n# .csv for submission\nsub_1 &lt;- data.frame(Id = ames_test$Id, SalePrice = res_1_bsplines$SalePrice_pred)\n# write.csv(sub_1, \"./data/submissions/sub_1_splines.csv\", quote = FALSE, row.names = FALSE)\n```\n\n\n\nFirst Kaggle Submission and Result\nOk! I got on the top 59% (2825/4819) cherry picking predictors and a GAM of splines. Not great, as expected, but it’s an important reference.\nNow that I have some reference, I’ll proceed with more complexity and involving all predictors in order to extract more signal."
  },
  {
    "objectID": "HousePrices.html#advanced-workflows",
    "href": "HousePrices.html#advanced-workflows",
    "title": "Prediction of House Prices in Ames",
    "section": "Advanced Workflows",
    "text": "Advanced Workflows\nHaving high multicollinearty it’s reasonable to think of some sort of penalisation or shrinkage method. Hence, I’ll go with a Generalised Linear Model, GLM. In this manner I can have a first look at feature importance.\nI’ll choose Elastic Net regression, combining the best of Lasso and Ridge. I must remember to myself that predictors must be scaled! I don’t want to over-penalised non-scaled variables using shrinkage methods.\n\n\nCode\n```{r}\n#| label: Preprocessing\n\n# Elastic Net\nglmnet_base_rec &lt;-\n  recipe(SalePrice_bc ~ ., data = ames_train) |&gt;\n  update_role(Id, new_role = \"Id variable\") |&gt;\n  update_role(SalePrice, new_role = \"original outcome\") |&gt;\n  update_role(dataset, new_role = \"splitting variable\") |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;    \n  step_YeoJohnson(all_numeric_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_zv(all_predictors())\n\n# glmnet_base_rec |&gt; prep() |&gt; bake(new_data = NULL) \n\n# lm_pca_rec &lt;- \n#   glmnet_base_rec |&gt;\n#   # step_pca(all_numeric_predictors(), num_comp = tune()) |&gt;\n#   step_pca(all_numeric_predictors(), threshold = 0.90) |&gt;\n#   step_normalize(all_numeric_predictors())\n\n# lm_pca_rec |&gt; prep(verbose = TRUE) |&gt; bake(new_data = NULL) \n```\n\n\n\nAssessment lm with PCA\n\nSpecifications and Tuning\n\n\nCode\n```{r}\n#| label: lm-pca\n#| eval: false\n\npca_wf &lt;- \n  workflow() |&gt;\n  add_model(lm_spec)|&gt;\n  add_recipe(lm_pca_rec)\n\nlm_pca_res &lt;- pca_wf |&gt;\n  fit_resamples(ames_boots)\n\nlm_pca_res |&gt;\n  collect_metrics(summarize = TRUE)\n```\n\n\n\n\n\nAssessment with Elastic-Net\n\nSpecifications and Tuning\n\n\nCode\n```{r}\n#| label: Elastic_net-set-tune\n\n# library(usemodels)  # CheatSheets\n\n# Model: Elastic Net -------------\nglmnet_spec &lt;-\n  linear_reg(penalty = tune(), mixture = tune()) |&gt; \n  set_engine(\"glmnet\") |&gt;\n  set_mode(\"regression\")\n  # translate()\n\nglmnet_wf &lt;-\n  workflow() |&gt;\n  add_recipe(glmnet_base_rec) |&gt;\n  add_model(glmnet_spec)\n\n# penalty = lambda; mixture = alpha\nglmnet_grid &lt;- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20), \n                               mixture = c(0.05, 0.2, 0.4, 0.6, 0.8, 1))\n\n# Tune with Folds\nset.seed(1982)\ndoParallel::registerDoParallel()\n\nglmnet_tune &lt;- tune_grid(glmnet_wf, resamples = ames_folds, grid = glmnet_grid)\n\n# Explore Results \nshow_best(glmnet_tune, metric = \"rmse\")\n\n# Set best parameters\nfinal_glmnet &lt;- glmnet_wf |&gt;\n  finalize_workflow(select_best(glmnet_tune, metric = \"rmse\"))\n\nfinal_glmnet\n\n# save(final_glmnet, file = \"./models/final_models/final_glmnet.Rdata\")\n```\n\n\n# A tibble: 5 × 8\n  penalty mixture .metric .estimator   mean     n  std_err .config              \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                \n1 0.00264     0.6 rmse    standard   0.0621   100 0.000887 Preprocessor1_Model0…\n2 0.00144     1   rmse    standard   0.0621   100 0.000888 Preprocessor1_Model1…\n3 0.00264     0.8 rmse    standard   0.0621   100 0.000886 Preprocessor1_Model0…\n4 0.00483     0.4 rmse    standard   0.0621   100 0.000885 Preprocessor1_Model0…\n5 0.00886     0.2 rmse    standard   0.0622   100 0.000884 Preprocessor1_Model0…\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_dummy()\n• step_YeoJohnson()\n• step_normalize()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0.00263665089873036\n  mixture = 0.6\n\nComputational engine: glmnet \n\n\nLoad Elastic-Net workflow with best parameters:\n\n```{r}\n#| label: final_glmnet\n#| code-fold: false\n\n# final_glmnet &lt;- load(\"./models/final_models/final_glmnet.Rdata\")\n# final_glmnet\n\nglmnet_boot_fit &lt;- \n  final_glmnet |&gt; \n  fit_resamples(resamples = ames_boots)\n  \n# glmnet_boot_fit |&gt;\n#   collect_metrics(summarize = TRUE)\n```\n\nA very low penalty of \\(\\lambda = 0.00264\\) with a mixture of \\(\\alpha = 0.6\\) were the best parameters.\nFor curiosity, I fitted the 1000 bootstraps to the elastic net model (code above) obtaining a mean RMSE = 0.0652. Best so far…\nNew fit with new feature engineering, mean RMSE = 0.0641. Kaggle’s Top 15%.\n\n\nCode\n```{r}\n#| label: Elastic-Net_submission\n#| eval: false\n\n# Get best model and fit with whole training set\nfit_glm_elnet &lt;- \n  final_glmnet |&gt;\n  fit(ames_train)\n\n# Predicting on the test set\names_test &lt;- \n  ames_all |&gt;\n  # select(-SalePrice) |&gt;\n  filter(dataset == \"test\")\n\nres_3_elasticnet &lt;- \n  predict(fit_glm_elnet, ames_test) |&gt;\n  mutate(SalePrice_pred = inv_boxcox(.pred, boxcox_lambda))  # orignal values in USD\n\n# .csv for submission\nsub_3_elasticnet &lt;- data.frame(Id = ames_test$Id, SalePrice = res_3_elasticnet$SalePrice_pred)\nwrite.csv(sub_3_elasticnet, \"./data/submissions/jthom_sub_3_elasticnet.csv\", quote = FALSE, row.names = FALSE)\n```\n\n\n\n\n\nAssessment with Random Forest\n\nSpecifications and Tuning\n\n\nCode\n```{r}\n#| label: Random_Forest-set-tune\n#| eval: false\n\ntree_rec &lt;- \n  recipe(SalePrice_bc ~ ., data = ames_train) |&gt;\n  update_role(Id, new_role = \"Id variabe\") |&gt;\n  update_role(SalePrice, new_role = \"original outcome\") |&gt;\n  update_role(dataset, new_role = \"splitting variable\") |&gt;\n  step_dummy(all_nominal_predictors()) \n\n\nrf_spec &lt;-\n  rand_forest(mtry = tune(), trees = 1000, min_n = tune()) |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() |&gt;\n  add_recipe(tree_rec) |&gt;\n  add_model(rf_spec)\n\n# Hyperparameters tuning ---\nset.seed(1982)\ndoParallel::registerDoParallel()\n\nrf_tune &lt;- tune_grid(rf_wf,                     \n                    resamples = ames_folds, \n                    grid = 10)\n\n# Explore Results\nshow_best(rf_tune, metric = \"rmse\")\n\n#  26 minutes Aprox. ---\n\n# Set best parameters\nfinal_rf &lt;- rf_wf |&gt;\n  finalize_workflow(select_best(rf_tune, metric = \"rmse\"))\n\nfinal_rf\n\nsave(final_rf, file = \"./models/final_models/final_rf.Rdata\")\n# final_rf &lt;- load(\"./models/final_models/final_rf.Rdata\")\n```\n\n\nLoad Random Forest workflow with best parameters:\n\n```{r}\n#| label: final_rf\n#| code-fold: false\n\nfinal_rf &lt;- load(\"./models/final_models/final_rf.Rdata\")\nfinal_rf\n\n# Test error estimation using the bootstraps\n\n# doParallel::registerDoParallel()\n# set.seed(1982)\n\n# final_rf |&gt; \n#   fit_resamples(resamples = ames_boots) |&gt;\n#   collect_metrics(summarize = TRUE)\n```\n\n[1] \"final_rf\"\n\n\nFor curiosity, I fitted the 1000 bootstraps to the best rf candidate (code above) obtaining a mean RMSE = 0.0772"
  },
  {
    "objectID": "HousePrices.html#drop-no-info-columns",
    "href": "HousePrices.html#drop-no-info-columns",
    "title": "Prediction of House Prices in Ames",
    "section": "Drop no info columns",
    "text": "Drop no info columns\n\n```{r}\n#| label: noinfo_drop\n#| code-fold: false\n\nnoinfo_cols &lt;- names(which(mi_y_X_2 &lt; 0.06444))\n\names_all &lt;-\n  ames_all |&gt;\n  select(-all_of(noinfo_cols)) \n```"
  }
]